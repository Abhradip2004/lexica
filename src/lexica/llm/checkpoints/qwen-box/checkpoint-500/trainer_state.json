{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.45454545454545453,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 3.844104766845703,
      "learning_rate": 0.0002,
      "loss": 2.6499,
      "step": 1
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 1.1307287216186523,
      "learning_rate": 0.00019981818181818184,
      "loss": 2.5737,
      "step": 2
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 1.0758846998214722,
      "learning_rate": 0.00019963636363636364,
      "loss": 2.5252,
      "step": 3
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.9520041942596436,
      "learning_rate": 0.00019945454545454547,
      "loss": 2.4344,
      "step": 4
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.720777690410614,
      "learning_rate": 0.00019927272727272727,
      "loss": 2.3914,
      "step": 5
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.7198169827461243,
      "learning_rate": 0.0001990909090909091,
      "loss": 2.3367,
      "step": 6
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.8427526354789734,
      "learning_rate": 0.0001989090909090909,
      "loss": 2.4615,
      "step": 7
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.7055410742759705,
      "learning_rate": 0.00019872727272727273,
      "loss": 2.2591,
      "step": 8
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.8072471022605896,
      "learning_rate": 0.00019854545454545456,
      "loss": 2.3417,
      "step": 9
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.8699756264686584,
      "learning_rate": 0.00019836363636363639,
      "loss": 2.2645,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.7976265549659729,
      "learning_rate": 0.00019818181818181821,
      "loss": 2.1378,
      "step": 11
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.8496799468994141,
      "learning_rate": 0.00019800000000000002,
      "loss": 2.0291,
      "step": 12
    },
    {
      "epoch": 0.011818181818181818,
      "grad_norm": 0.8838374018669128,
      "learning_rate": 0.00019781818181818184,
      "loss": 1.9871,
      "step": 13
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 0.9850407838821411,
      "learning_rate": 0.00019763636363636365,
      "loss": 1.9227,
      "step": 14
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 1.1282647848129272,
      "learning_rate": 0.00019745454545454547,
      "loss": 1.8363,
      "step": 15
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 1.3780126571655273,
      "learning_rate": 0.00019727272727272728,
      "loss": 1.7651,
      "step": 16
    },
    {
      "epoch": 0.015454545454545455,
      "grad_norm": 1.6845067739486694,
      "learning_rate": 0.0001970909090909091,
      "loss": 1.7389,
      "step": 17
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 3.1404452323913574,
      "learning_rate": 0.0001969090909090909,
      "loss": 1.5993,
      "step": 18
    },
    {
      "epoch": 0.017272727272727273,
      "grad_norm": 3.6539554595947266,
      "learning_rate": 0.00019672727272727273,
      "loss": 1.5742,
      "step": 19
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 1.521996021270752,
      "learning_rate": 0.00019654545454545456,
      "loss": 1.4357,
      "step": 20
    },
    {
      "epoch": 0.019090909090909092,
      "grad_norm": 2.171576738357544,
      "learning_rate": 0.00019636363636363636,
      "loss": 1.4181,
      "step": 21
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.9122514724731445,
      "learning_rate": 0.0001961818181818182,
      "loss": 1.3394,
      "step": 22
    },
    {
      "epoch": 0.02090909090909091,
      "grad_norm": 2.175624132156372,
      "learning_rate": 0.000196,
      "loss": 1.2544,
      "step": 23
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 1.9323697090148926,
      "learning_rate": 0.00019581818181818182,
      "loss": 1.139,
      "step": 24
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 2.6747820377349854,
      "learning_rate": 0.00019563636363636365,
      "loss": 1.0514,
      "step": 25
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 2.799640417098999,
      "learning_rate": 0.00019545454545454548,
      "loss": 0.9755,
      "step": 26
    },
    {
      "epoch": 0.024545454545454544,
      "grad_norm": 2.184305191040039,
      "learning_rate": 0.00019527272727272728,
      "loss": 0.8765,
      "step": 27
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 2.066234588623047,
      "learning_rate": 0.0001950909090909091,
      "loss": 0.7469,
      "step": 28
    },
    {
      "epoch": 0.026363636363636363,
      "grad_norm": 1.7848398685455322,
      "learning_rate": 0.0001949090909090909,
      "loss": 0.7317,
      "step": 29
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 1.5407531261444092,
      "learning_rate": 0.00019472727272727274,
      "loss": 0.6598,
      "step": 30
    },
    {
      "epoch": 0.028181818181818183,
      "grad_norm": 1.714348316192627,
      "learning_rate": 0.00019454545454545457,
      "loss": 0.6,
      "step": 31
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 1.5763064622879028,
      "learning_rate": 0.00019436363636363637,
      "loss": 0.4513,
      "step": 32
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.376051425933838,
      "learning_rate": 0.0001941818181818182,
      "loss": 0.4534,
      "step": 33
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 1.1843162775039673,
      "learning_rate": 0.000194,
      "loss": 0.4395,
      "step": 34
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 1.0660947561264038,
      "learning_rate": 0.00019381818181818183,
      "loss": 0.4007,
      "step": 35
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.8651633262634277,
      "learning_rate": 0.00019363636363636363,
      "loss": 0.2278,
      "step": 36
    },
    {
      "epoch": 0.03363636363636364,
      "grad_norm": 0.711283266544342,
      "learning_rate": 0.00019345454545454546,
      "loss": 0.2118,
      "step": 37
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.633023738861084,
      "learning_rate": 0.00019327272727272726,
      "loss": 0.3023,
      "step": 38
    },
    {
      "epoch": 0.035454545454545454,
      "grad_norm": 0.5552642345428467,
      "learning_rate": 0.0001930909090909091,
      "loss": 0.1378,
      "step": 39
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.503321647644043,
      "learning_rate": 0.00019290909090909092,
      "loss": 0.2099,
      "step": 40
    },
    {
      "epoch": 0.03727272727272727,
      "grad_norm": 0.5409606099128723,
      "learning_rate": 0.00019272727272727274,
      "loss": 0.2944,
      "step": 41
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.5227068066596985,
      "learning_rate": 0.00019254545454545457,
      "loss": 0.2527,
      "step": 42
    },
    {
      "epoch": 0.03909090909090909,
      "grad_norm": 0.4945509135723114,
      "learning_rate": 0.00019236363636363637,
      "loss": 0.2897,
      "step": 43
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.5977767705917358,
      "learning_rate": 0.0001921818181818182,
      "loss": 0.2994,
      "step": 44
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 0.5275575518608093,
      "learning_rate": 0.000192,
      "loss": 0.2685,
      "step": 45
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.29506799578666687,
      "learning_rate": 0.00019181818181818183,
      "loss": 0.1322,
      "step": 46
    },
    {
      "epoch": 0.042727272727272725,
      "grad_norm": 0.44444817304611206,
      "learning_rate": 0.00019163636363636363,
      "loss": 0.2423,
      "step": 47
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.39444103837013245,
      "learning_rate": 0.00019145454545454546,
      "loss": 0.2451,
      "step": 48
    },
    {
      "epoch": 0.04454545454545455,
      "grad_norm": 0.5036789774894714,
      "learning_rate": 0.0001912727272727273,
      "loss": 0.2325,
      "step": 49
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.42586374282836914,
      "learning_rate": 0.0001910909090909091,
      "loss": 0.1545,
      "step": 50
    },
    {
      "epoch": 0.046363636363636364,
      "grad_norm": 0.4589099586009979,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.1502,
      "step": 51
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.24508163332939148,
      "learning_rate": 0.00019072727272727272,
      "loss": 0.1277,
      "step": 52
    },
    {
      "epoch": 0.04818181818181818,
      "grad_norm": 0.3400561213493347,
      "learning_rate": 0.00019054545454545455,
      "loss": 0.1385,
      "step": 53
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.36088642477989197,
      "learning_rate": 0.00019036363636363635,
      "loss": 0.137,
      "step": 54
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.5668820738792419,
      "learning_rate": 0.00019018181818181818,
      "loss": 0.1796,
      "step": 55
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.43633806705474854,
      "learning_rate": 0.00019,
      "loss": 0.1285,
      "step": 56
    },
    {
      "epoch": 0.05181818181818182,
      "grad_norm": 0.5673348903656006,
      "learning_rate": 0.00018981818181818184,
      "loss": 0.2164,
      "step": 57
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.4976480007171631,
      "learning_rate": 0.00018963636363636367,
      "loss": 0.1507,
      "step": 58
    },
    {
      "epoch": 0.053636363636363635,
      "grad_norm": 0.6397409439086914,
      "learning_rate": 0.00018945454545454547,
      "loss": 0.228,
      "step": 59
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.6459603905677795,
      "learning_rate": 0.0001892727272727273,
      "loss": 0.1934,
      "step": 60
    },
    {
      "epoch": 0.05545454545454546,
      "grad_norm": 2.0986852645874023,
      "learning_rate": 0.0001890909090909091,
      "loss": 0.1321,
      "step": 61
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.8827850818634033,
      "learning_rate": 0.00018890909090909093,
      "loss": 0.1259,
      "step": 62
    },
    {
      "epoch": 0.057272727272727274,
      "grad_norm": 1.6709438562393188,
      "learning_rate": 0.00018872727272727273,
      "loss": 0.1562,
      "step": 63
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.40136706829071045,
      "learning_rate": 0.00018854545454545456,
      "loss": 0.1058,
      "step": 64
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.43806692957878113,
      "learning_rate": 0.00018836363636363636,
      "loss": 0.1378,
      "step": 65
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.5711289644241333,
      "learning_rate": 0.0001881818181818182,
      "loss": 0.193,
      "step": 66
    },
    {
      "epoch": 0.060909090909090906,
      "grad_norm": 0.6346274018287659,
      "learning_rate": 0.000188,
      "loss": 0.2189,
      "step": 67
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.4270968437194824,
      "learning_rate": 0.00018781818181818182,
      "loss": 0.1101,
      "step": 68
    },
    {
      "epoch": 0.06272727272727273,
      "grad_norm": 0.5597787499427795,
      "learning_rate": 0.00018763636363636365,
      "loss": 0.1396,
      "step": 69
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.31080517172813416,
      "learning_rate": 0.00018745454545454545,
      "loss": 0.1141,
      "step": 70
    },
    {
      "epoch": 0.06454545454545454,
      "grad_norm": 0.5419543385505676,
      "learning_rate": 0.00018727272727272728,
      "loss": 0.1706,
      "step": 71
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.6489369869232178,
      "learning_rate": 0.0001870909090909091,
      "loss": 0.1278,
      "step": 72
    },
    {
      "epoch": 0.06636363636363636,
      "grad_norm": 0.3447032868862152,
      "learning_rate": 0.00018690909090909093,
      "loss": 0.0855,
      "step": 73
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.42414870858192444,
      "learning_rate": 0.00018672727272727273,
      "loss": 0.1133,
      "step": 74
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.4410507380962372,
      "learning_rate": 0.00018654545454545456,
      "loss": 0.0936,
      "step": 75
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.7216399312019348,
      "learning_rate": 0.00018636363636363636,
      "loss": 0.1405,
      "step": 76
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.5273827910423279,
      "learning_rate": 0.0001861818181818182,
      "loss": 0.076,
      "step": 77
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.5395326018333435,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.1164,
      "step": 78
    },
    {
      "epoch": 0.07181818181818182,
      "grad_norm": 0.35664859414100647,
      "learning_rate": 0.00018581818181818182,
      "loss": 0.1121,
      "step": 79
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.5577934384346008,
      "learning_rate": 0.00018563636363636365,
      "loss": 0.1287,
      "step": 80
    },
    {
      "epoch": 0.07363636363636364,
      "grad_norm": 0.9976377487182617,
      "learning_rate": 0.00018545454545454545,
      "loss": 0.1214,
      "step": 81
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.4558241367340088,
      "learning_rate": 0.00018527272727272728,
      "loss": 0.1099,
      "step": 82
    },
    {
      "epoch": 0.07545454545454545,
      "grad_norm": 0.5938529968261719,
      "learning_rate": 0.00018509090909090908,
      "loss": 0.1256,
      "step": 83
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.468515545129776,
      "learning_rate": 0.0001849090909090909,
      "loss": 0.1543,
      "step": 84
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.49928829073905945,
      "learning_rate": 0.0001847272727272727,
      "loss": 0.1522,
      "step": 85
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.4244269132614136,
      "learning_rate": 0.00018454545454545454,
      "loss": 0.1005,
      "step": 86
    },
    {
      "epoch": 0.07909090909090909,
      "grad_norm": 0.936093807220459,
      "learning_rate": 0.00018436363636363637,
      "loss": 0.1119,
      "step": 87
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.5401094555854797,
      "learning_rate": 0.0001841818181818182,
      "loss": 0.1007,
      "step": 88
    },
    {
      "epoch": 0.0809090909090909,
      "grad_norm": 0.5497111678123474,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.1278,
      "step": 89
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.4034898281097412,
      "learning_rate": 0.00018381818181818183,
      "loss": 0.0631,
      "step": 90
    },
    {
      "epoch": 0.08272727272727273,
      "grad_norm": 0.41505053639411926,
      "learning_rate": 0.00018363636363636366,
      "loss": 0.1213,
      "step": 91
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.43224844336509705,
      "learning_rate": 0.00018345454545454546,
      "loss": 0.1336,
      "step": 92
    },
    {
      "epoch": 0.08454545454545455,
      "grad_norm": 0.40726834535598755,
      "learning_rate": 0.0001832727272727273,
      "loss": 0.1092,
      "step": 93
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.3503759503364563,
      "learning_rate": 0.0001830909090909091,
      "loss": 0.0456,
      "step": 94
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.3604673147201538,
      "learning_rate": 0.00018290909090909092,
      "loss": 0.0739,
      "step": 95
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.37602782249450684,
      "learning_rate": 0.00018272727272727275,
      "loss": 0.0616,
      "step": 96
    },
    {
      "epoch": 0.08818181818181818,
      "grad_norm": 0.7534922361373901,
      "learning_rate": 0.00018254545454545455,
      "loss": 0.093,
      "step": 97
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.3380594849586487,
      "learning_rate": 0.00018236363636363638,
      "loss": 0.0668,
      "step": 98
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.30590587854385376,
      "learning_rate": 0.00018218181818181818,
      "loss": 0.0657,
      "step": 99
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.3250317871570587,
      "learning_rate": 0.000182,
      "loss": 0.0875,
      "step": 100
    },
    {
      "epoch": 0.09181818181818181,
      "grad_norm": 0.3628494143486023,
      "learning_rate": 0.00018181818181818183,
      "loss": 0.0853,
      "step": 101
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.35652098059654236,
      "learning_rate": 0.00018163636363636364,
      "loss": 0.0843,
      "step": 102
    },
    {
      "epoch": 0.09363636363636364,
      "grad_norm": 0.354947030544281,
      "learning_rate": 0.00018145454545454546,
      "loss": 0.0762,
      "step": 103
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.43706026673316956,
      "learning_rate": 0.0001812727272727273,
      "loss": 0.0837,
      "step": 104
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.41350850462913513,
      "learning_rate": 0.0001810909090909091,
      "loss": 0.1173,
      "step": 105
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.5691813230514526,
      "learning_rate": 0.00018090909090909092,
      "loss": 0.0479,
      "step": 106
    },
    {
      "epoch": 0.09727272727272727,
      "grad_norm": 0.3962486684322357,
      "learning_rate": 0.00018072727272727275,
      "loss": 0.0712,
      "step": 107
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 0.810084879398346,
      "learning_rate": 0.00018054545454545455,
      "loss": 0.1253,
      "step": 108
    },
    {
      "epoch": 0.09909090909090909,
      "grad_norm": 0.4956572353839874,
      "learning_rate": 0.00018036363636363638,
      "loss": 0.0501,
      "step": 109
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5629420280456543,
      "learning_rate": 0.00018018181818181818,
      "loss": 0.0893,
      "step": 110
    },
    {
      "epoch": 0.1009090909090909,
      "grad_norm": 0.5411421656608582,
      "learning_rate": 0.00018,
      "loss": 0.0761,
      "step": 111
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.35724589228630066,
      "learning_rate": 0.0001798181818181818,
      "loss": 0.0882,
      "step": 112
    },
    {
      "epoch": 0.10272727272727272,
      "grad_norm": 0.3808988928794861,
      "learning_rate": 0.00017963636363636364,
      "loss": 0.0813,
      "step": 113
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.5714194774627686,
      "learning_rate": 0.00017945454545454544,
      "loss": 0.0639,
      "step": 114
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.460254967212677,
      "learning_rate": 0.00017927272727272727,
      "loss": 0.0823,
      "step": 115
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.4014129042625427,
      "learning_rate": 0.0001790909090909091,
      "loss": 0.0662,
      "step": 116
    },
    {
      "epoch": 0.10636363636363637,
      "grad_norm": 0.48428401350975037,
      "learning_rate": 0.00017890909090909093,
      "loss": 0.0829,
      "step": 117
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.39502206444740295,
      "learning_rate": 0.00017872727272727273,
      "loss": 0.1005,
      "step": 118
    },
    {
      "epoch": 0.10818181818181818,
      "grad_norm": 0.3241663873195648,
      "learning_rate": 0.00017854545454545456,
      "loss": 0.067,
      "step": 119
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.42227038741111755,
      "learning_rate": 0.0001783636363636364,
      "loss": 0.0652,
      "step": 120
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.9497225880622864,
      "learning_rate": 0.0001781818181818182,
      "loss": 0.0986,
      "step": 121
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.31568074226379395,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.0747,
      "step": 122
    },
    {
      "epoch": 0.11181818181818182,
      "grad_norm": 0.3764244019985199,
      "learning_rate": 0.00017781818181818182,
      "loss": 0.0478,
      "step": 123
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.3317689895629883,
      "learning_rate": 0.00017763636363636365,
      "loss": 0.0625,
      "step": 124
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.5252065062522888,
      "learning_rate": 0.00017745454545454548,
      "loss": 0.096,
      "step": 125
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.32530727982521057,
      "learning_rate": 0.00017727272727272728,
      "loss": 0.0354,
      "step": 126
    },
    {
      "epoch": 0.11545454545454545,
      "grad_norm": 0.44210246205329895,
      "learning_rate": 0.0001770909090909091,
      "loss": 0.0847,
      "step": 127
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.460885226726532,
      "learning_rate": 0.0001769090909090909,
      "loss": 0.0559,
      "step": 128
    },
    {
      "epoch": 0.11727272727272728,
      "grad_norm": 0.3610718250274658,
      "learning_rate": 0.00017672727272727274,
      "loss": 0.0567,
      "step": 129
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.4787140488624573,
      "learning_rate": 0.00017654545454545454,
      "loss": 0.0868,
      "step": 130
    },
    {
      "epoch": 0.1190909090909091,
      "grad_norm": 0.3033028841018677,
      "learning_rate": 0.00017636363636363637,
      "loss": 0.0552,
      "step": 131
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.165903091430664,
      "learning_rate": 0.0001761818181818182,
      "loss": 0.0834,
      "step": 132
    },
    {
      "epoch": 0.12090909090909091,
      "grad_norm": 0.5295712947845459,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0797,
      "step": 133
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.47136053442955017,
      "learning_rate": 0.00017581818181818182,
      "loss": 0.0301,
      "step": 134
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.8169677257537842,
      "learning_rate": 0.00017563636363636365,
      "loss": 0.0577,
      "step": 135
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.6883771419525146,
      "learning_rate": 0.00017545454545454548,
      "loss": 0.1038,
      "step": 136
    },
    {
      "epoch": 0.12454545454545454,
      "grad_norm": 0.7295814156532288,
      "learning_rate": 0.00017527272727272728,
      "loss": 0.0723,
      "step": 137
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 1.5295498371124268,
      "learning_rate": 0.0001750909090909091,
      "loss": 0.0871,
      "step": 138
    },
    {
      "epoch": 0.12636363636363637,
      "grad_norm": 0.43849867582321167,
      "learning_rate": 0.0001749090909090909,
      "loss": 0.0467,
      "step": 139
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 2.056004047393799,
      "learning_rate": 0.00017472727272727274,
      "loss": 0.2679,
      "step": 140
    },
    {
      "epoch": 0.12818181818181817,
      "grad_norm": 0.43350860476493835,
      "learning_rate": 0.00017454545454545454,
      "loss": 0.0758,
      "step": 141
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.45173409581184387,
      "learning_rate": 0.00017436363636363637,
      "loss": 0.0687,
      "step": 142
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.3220479190349579,
      "learning_rate": 0.00017418181818181817,
      "loss": 0.0951,
      "step": 143
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.33805662393569946,
      "learning_rate": 0.000174,
      "loss": 0.0844,
      "step": 144
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.4200410544872284,
      "learning_rate": 0.00017381818181818183,
      "loss": 0.0764,
      "step": 145
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.33691897988319397,
      "learning_rate": 0.00017363636363636363,
      "loss": 0.0806,
      "step": 146
    },
    {
      "epoch": 0.13363636363636364,
      "grad_norm": 0.26621606945991516,
      "learning_rate": 0.00017345454545454546,
      "loss": 0.0464,
      "step": 147
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.2577629089355469,
      "learning_rate": 0.0001732727272727273,
      "loss": 0.0405,
      "step": 148
    },
    {
      "epoch": 0.13545454545454547,
      "grad_norm": 0.26968035101890564,
      "learning_rate": 0.00017309090909090912,
      "loss": 0.0458,
      "step": 149
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.3730718493461609,
      "learning_rate": 0.00017290909090909092,
      "loss": 0.1065,
      "step": 150
    },
    {
      "epoch": 0.13727272727272727,
      "grad_norm": 0.3224756121635437,
      "learning_rate": 0.00017272727272727275,
      "loss": 0.0897,
      "step": 151
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.3172087073326111,
      "learning_rate": 0.00017254545454545455,
      "loss": 0.0725,
      "step": 152
    },
    {
      "epoch": 0.1390909090909091,
      "grad_norm": 0.48595497012138367,
      "learning_rate": 0.00017236363636363638,
      "loss": 0.0785,
      "step": 153
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.4119967222213745,
      "learning_rate": 0.0001721818181818182,
      "loss": 0.0702,
      "step": 154
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.725997269153595,
      "learning_rate": 0.000172,
      "loss": 0.0665,
      "step": 155
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.4398091733455658,
      "learning_rate": 0.00017181818181818184,
      "loss": 0.046,
      "step": 156
    },
    {
      "epoch": 0.14272727272727273,
      "grad_norm": 0.5407041311264038,
      "learning_rate": 0.00017163636363636364,
      "loss": 0.0611,
      "step": 157
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.4772058129310608,
      "learning_rate": 0.00017145454545454547,
      "loss": 0.0674,
      "step": 158
    },
    {
      "epoch": 0.14454545454545453,
      "grad_norm": 0.4515410363674164,
      "learning_rate": 0.00017127272727272727,
      "loss": 0.0936,
      "step": 159
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.284618079662323,
      "learning_rate": 0.0001710909090909091,
      "loss": 0.0581,
      "step": 160
    },
    {
      "epoch": 0.14636363636363636,
      "grad_norm": 0.2853127717971802,
      "learning_rate": 0.0001709090909090909,
      "loss": 0.0602,
      "step": 161
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.7650349736213684,
      "learning_rate": 0.00017072727272727273,
      "loss": 0.087,
      "step": 162
    },
    {
      "epoch": 0.1481818181818182,
      "grad_norm": 0.47186723351478577,
      "learning_rate": 0.00017054545454545455,
      "loss": 0.0612,
      "step": 163
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.5100069642066956,
      "learning_rate": 0.00017036363636363638,
      "loss": 0.0743,
      "step": 164
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.42298009991645813,
      "learning_rate": 0.0001701818181818182,
      "loss": 0.0683,
      "step": 165
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.39526915550231934,
      "learning_rate": 0.00017,
      "loss": 0.0608,
      "step": 166
    },
    {
      "epoch": 0.15181818181818182,
      "grad_norm": 0.5403361320495605,
      "learning_rate": 0.00016981818181818184,
      "loss": 0.0868,
      "step": 167
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.34632667899131775,
      "learning_rate": 0.00016963636363636364,
      "loss": 0.0565,
      "step": 168
    },
    {
      "epoch": 0.15363636363636363,
      "grad_norm": 1.0513156652450562,
      "learning_rate": 0.00016945454545454547,
      "loss": 0.0916,
      "step": 169
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.5142855644226074,
      "learning_rate": 0.00016927272727272727,
      "loss": 0.0751,
      "step": 170
    },
    {
      "epoch": 0.15545454545454546,
      "grad_norm": 0.8188840746879578,
      "learning_rate": 0.0001690909090909091,
      "loss": 0.2156,
      "step": 171
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.3634525537490845,
      "learning_rate": 0.00016890909090909093,
      "loss": 0.067,
      "step": 172
    },
    {
      "epoch": 0.1572727272727273,
      "grad_norm": 0.37589511275291443,
      "learning_rate": 0.00016872727272727273,
      "loss": 0.0609,
      "step": 173
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.22695940732955933,
      "learning_rate": 0.00016854545454545456,
      "loss": 0.0341,
      "step": 174
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.35485586524009705,
      "learning_rate": 0.00016836363636363636,
      "loss": 0.0625,
      "step": 175
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.3815512955188751,
      "learning_rate": 0.0001681818181818182,
      "loss": 0.0633,
      "step": 176
    },
    {
      "epoch": 0.16090909090909092,
      "grad_norm": 0.38654011487960815,
      "learning_rate": 0.000168,
      "loss": 0.0671,
      "step": 177
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.37335512042045593,
      "learning_rate": 0.00016781818181818182,
      "loss": 0.0684,
      "step": 178
    },
    {
      "epoch": 0.16272727272727272,
      "grad_norm": 0.26454266905784607,
      "learning_rate": 0.00016763636363636365,
      "loss": 0.046,
      "step": 179
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.32721492648124695,
      "learning_rate": 0.00016745454545454548,
      "loss": 0.0348,
      "step": 180
    },
    {
      "epoch": 0.16454545454545455,
      "grad_norm": 0.26956799626350403,
      "learning_rate": 0.00016727272727272728,
      "loss": 0.0542,
      "step": 181
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.34550362825393677,
      "learning_rate": 0.0001670909090909091,
      "loss": 0.054,
      "step": 182
    },
    {
      "epoch": 0.16636363636363635,
      "grad_norm": 0.4350583255290985,
      "learning_rate": 0.00016690909090909093,
      "loss": 0.0634,
      "step": 183
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.3344632387161255,
      "learning_rate": 0.00016672727272727274,
      "loss": 0.0578,
      "step": 184
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.658018171787262,
      "learning_rate": 0.00016654545454545456,
      "loss": 0.0716,
      "step": 185
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.3725290894508362,
      "learning_rate": 0.00016636363636363637,
      "loss": 0.067,
      "step": 186
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.5518211722373962,
      "learning_rate": 0.0001661818181818182,
      "loss": 0.0583,
      "step": 187
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.5504435896873474,
      "learning_rate": 0.000166,
      "loss": 0.0632,
      "step": 188
    },
    {
      "epoch": 0.17181818181818181,
      "grad_norm": 0.3524181544780731,
      "learning_rate": 0.00016581818181818182,
      "loss": 0.062,
      "step": 189
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 1.21303129196167,
      "learning_rate": 0.00016563636363636363,
      "loss": 0.0761,
      "step": 190
    },
    {
      "epoch": 0.17363636363636364,
      "grad_norm": 0.5341414213180542,
      "learning_rate": 0.00016545454545454545,
      "loss": 0.0623,
      "step": 191
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.41886356472969055,
      "learning_rate": 0.00016527272727272728,
      "loss": 0.067,
      "step": 192
    },
    {
      "epoch": 0.17545454545454545,
      "grad_norm": 0.2814624309539795,
      "learning_rate": 0.00016509090909090908,
      "loss": 0.0486,
      "step": 193
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.8648954033851624,
      "learning_rate": 0.0001649090909090909,
      "loss": 0.0655,
      "step": 194
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.6673822999000549,
      "learning_rate": 0.00016472727272727274,
      "loss": 0.1761,
      "step": 195
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.36489492654800415,
      "learning_rate": 0.00016454545454545457,
      "loss": 0.054,
      "step": 196
    },
    {
      "epoch": 0.17909090909090908,
      "grad_norm": 1.3858158588409424,
      "learning_rate": 0.00016436363636363637,
      "loss": 0.1114,
      "step": 197
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.31344178318977356,
      "learning_rate": 0.0001641818181818182,
      "loss": 0.074,
      "step": 198
    },
    {
      "epoch": 0.1809090909090909,
      "grad_norm": 0.39883172512054443,
      "learning_rate": 0.000164,
      "loss": 0.091,
      "step": 199
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.2853845953941345,
      "learning_rate": 0.00016381818181818183,
      "loss": 0.0736,
      "step": 200
    },
    {
      "epoch": 0.18272727272727274,
      "grad_norm": 0.32599788904190063,
      "learning_rate": 0.00016363636363636366,
      "loss": 0.0701,
      "step": 201
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.30363285541534424,
      "learning_rate": 0.00016345454545454546,
      "loss": 0.068,
      "step": 202
    },
    {
      "epoch": 0.18454545454545454,
      "grad_norm": 0.23653580248355865,
      "learning_rate": 0.0001632727272727273,
      "loss": 0.055,
      "step": 203
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.23997002840042114,
      "learning_rate": 0.0001630909090909091,
      "loss": 0.0434,
      "step": 204
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.4529974162578583,
      "learning_rate": 0.00016290909090909092,
      "loss": 0.0794,
      "step": 205
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.28779682517051697,
      "learning_rate": 0.00016272727272727272,
      "loss": 0.0676,
      "step": 206
    },
    {
      "epoch": 0.18818181818181817,
      "grad_norm": 0.3656376302242279,
      "learning_rate": 0.00016254545454545455,
      "loss": 0.0726,
      "step": 207
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.32229742407798767,
      "learning_rate": 0.00016236363636363635,
      "loss": 0.0748,
      "step": 208
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.3518376350402832,
      "learning_rate": 0.00016218181818181818,
      "loss": 0.0807,
      "step": 209
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.3145832419395447,
      "learning_rate": 0.000162,
      "loss": 0.0451,
      "step": 210
    },
    {
      "epoch": 0.1918181818181818,
      "grad_norm": 0.2687370181083679,
      "learning_rate": 0.00016181818181818184,
      "loss": 0.056,
      "step": 211
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.4849449098110199,
      "learning_rate": 0.00016163636363636366,
      "loss": 0.1671,
      "step": 212
    },
    {
      "epoch": 0.19363636363636363,
      "grad_norm": 0.2831375300884247,
      "learning_rate": 0.00016145454545454547,
      "loss": 0.0707,
      "step": 213
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.3781835734844208,
      "learning_rate": 0.0001612727272727273,
      "loss": 0.0658,
      "step": 214
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.39633432030677795,
      "learning_rate": 0.0001610909090909091,
      "loss": 0.076,
      "step": 215
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.43652641773223877,
      "learning_rate": 0.00016090909090909092,
      "loss": 0.0507,
      "step": 216
    },
    {
      "epoch": 0.19727272727272727,
      "grad_norm": 0.34391725063323975,
      "learning_rate": 0.00016072727272727273,
      "loss": 0.064,
      "step": 217
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.35748812556266785,
      "learning_rate": 0.00016054545454545455,
      "loss": 0.0677,
      "step": 218
    },
    {
      "epoch": 0.1990909090909091,
      "grad_norm": 0.28287839889526367,
      "learning_rate": 0.00016036363636363636,
      "loss": 0.0527,
      "step": 219
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.3084470331668854,
      "learning_rate": 0.00016018181818181818,
      "loss": 0.0535,
      "step": 220
    },
    {
      "epoch": 0.2009090909090909,
      "grad_norm": 0.7639946937561035,
      "learning_rate": 0.00016,
      "loss": 0.0736,
      "step": 221
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.4157367944717407,
      "learning_rate": 0.00015981818181818181,
      "loss": 0.038,
      "step": 222
    },
    {
      "epoch": 0.20272727272727273,
      "grad_norm": 1.066529393196106,
      "learning_rate": 0.00015963636363636364,
      "loss": 0.0723,
      "step": 223
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.2961093783378601,
      "learning_rate": 0.00015945454545454544,
      "loss": 0.0358,
      "step": 224
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.4235130846500397,
      "learning_rate": 0.00015927272727272727,
      "loss": 0.0348,
      "step": 225
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.3501872420310974,
      "learning_rate": 0.0001590909090909091,
      "loss": 0.0337,
      "step": 226
    },
    {
      "epoch": 0.20636363636363636,
      "grad_norm": 0.49333199858665466,
      "learning_rate": 0.00015890909090909093,
      "loss": 0.0667,
      "step": 227
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.4144633412361145,
      "learning_rate": 0.00015872727272727273,
      "loss": 0.0669,
      "step": 228
    },
    {
      "epoch": 0.2081818181818182,
      "grad_norm": 0.3668809235095978,
      "learning_rate": 0.00015854545454545456,
      "loss": 0.0684,
      "step": 229
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.29837721586227417,
      "learning_rate": 0.0001583636363636364,
      "loss": 0.0314,
      "step": 230
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.41129645705223083,
      "learning_rate": 0.0001581818181818182,
      "loss": 0.0572,
      "step": 231
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.30699533224105835,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.0545,
      "step": 232
    },
    {
      "epoch": 0.21181818181818182,
      "grad_norm": 0.40545928478240967,
      "learning_rate": 0.00015781818181818182,
      "loss": 0.0382,
      "step": 233
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.38194581866264343,
      "learning_rate": 0.00015763636363636365,
      "loss": 0.0662,
      "step": 234
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 0.2148771733045578,
      "learning_rate": 0.00015745454545454545,
      "loss": 0.0493,
      "step": 235
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.5970410108566284,
      "learning_rate": 0.00015727272727272728,
      "loss": 0.0994,
      "step": 236
    },
    {
      "epoch": 0.21545454545454545,
      "grad_norm": 0.3390374779701233,
      "learning_rate": 0.00015709090909090908,
      "loss": 0.0712,
      "step": 237
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.2767135798931122,
      "learning_rate": 0.0001569090909090909,
      "loss": 0.0621,
      "step": 238
    },
    {
      "epoch": 0.21727272727272728,
      "grad_norm": 0.28535711765289307,
      "learning_rate": 0.00015672727272727274,
      "loss": 0.0568,
      "step": 239
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.22101570665836334,
      "learning_rate": 0.00015654545454545454,
      "loss": 0.0553,
      "step": 240
    },
    {
      "epoch": 0.2190909090909091,
      "grad_norm": 0.30055683851242065,
      "learning_rate": 0.00015636363636363637,
      "loss": 0.056,
      "step": 241
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.9120745062828064,
      "learning_rate": 0.0001561818181818182,
      "loss": 0.1478,
      "step": 242
    },
    {
      "epoch": 0.22090909090909092,
      "grad_norm": 0.3014279007911682,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0642,
      "step": 243
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.28367742896080017,
      "learning_rate": 0.00015581818181818183,
      "loss": 0.0623,
      "step": 244
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.5188584923744202,
      "learning_rate": 0.00015563636363636365,
      "loss": 0.064,
      "step": 245
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.3491906225681305,
      "learning_rate": 0.00015545454545454546,
      "loss": 0.0644,
      "step": 246
    },
    {
      "epoch": 0.22454545454545455,
      "grad_norm": 0.4080652892589569,
      "learning_rate": 0.00015527272727272728,
      "loss": 0.0345,
      "step": 247
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.26522231101989746,
      "learning_rate": 0.0001550909090909091,
      "loss": 0.0511,
      "step": 248
    },
    {
      "epoch": 0.22636363636363635,
      "grad_norm": 0.26065099239349365,
      "learning_rate": 0.00015490909090909091,
      "loss": 0.0511,
      "step": 249
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.0964210033416748,
      "learning_rate": 0.00015472727272727274,
      "loss": 0.0779,
      "step": 250
    },
    {
      "epoch": 0.22818181818181818,
      "grad_norm": 0.33859649300575256,
      "learning_rate": 0.00015454545454545454,
      "loss": 0.0546,
      "step": 251
    },
    {
      "epoch": 0.2290909090909091,
      "grad_norm": 0.6657006740570068,
      "learning_rate": 0.00015436363636363637,
      "loss": 0.1359,
      "step": 252
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.45145413279533386,
      "learning_rate": 0.00015418181818181817,
      "loss": 0.0596,
      "step": 253
    },
    {
      "epoch": 0.2309090909090909,
      "grad_norm": 0.4086307883262634,
      "learning_rate": 0.000154,
      "loss": 0.0366,
      "step": 254
    },
    {
      "epoch": 0.2318181818181818,
      "grad_norm": 0.44712793827056885,
      "learning_rate": 0.0001538181818181818,
      "loss": 0.0326,
      "step": 255
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.4363364577293396,
      "learning_rate": 0.00015363636363636363,
      "loss": 0.0603,
      "step": 256
    },
    {
      "epoch": 0.23363636363636364,
      "grad_norm": 0.33348706364631653,
      "learning_rate": 0.00015345454545454546,
      "loss": 0.0492,
      "step": 257
    },
    {
      "epoch": 0.23454545454545456,
      "grad_norm": 0.4831458628177643,
      "learning_rate": 0.0001532727272727273,
      "loss": 0.0662,
      "step": 258
    },
    {
      "epoch": 0.23545454545454544,
      "grad_norm": 0.27985963225364685,
      "learning_rate": 0.00015309090909090912,
      "loss": 0.0526,
      "step": 259
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.3676934540271759,
      "learning_rate": 0.00015290909090909092,
      "loss": 0.0567,
      "step": 260
    },
    {
      "epoch": 0.23727272727272727,
      "grad_norm": 0.8116787672042847,
      "learning_rate": 0.00015272727272727275,
      "loss": 0.0781,
      "step": 261
    },
    {
      "epoch": 0.2381818181818182,
      "grad_norm": 0.5857946872711182,
      "learning_rate": 0.00015254545454545455,
      "loss": 0.0635,
      "step": 262
    },
    {
      "epoch": 0.2390909090909091,
      "grad_norm": 0.24523243308067322,
      "learning_rate": 0.00015236363636363638,
      "loss": 0.0518,
      "step": 263
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5746257901191711,
      "learning_rate": 0.00015218181818181818,
      "loss": 0.0744,
      "step": 264
    },
    {
      "epoch": 0.2409090909090909,
      "grad_norm": 0.389180988073349,
      "learning_rate": 0.000152,
      "loss": 0.0676,
      "step": 265
    },
    {
      "epoch": 0.24181818181818182,
      "grad_norm": 0.7754030823707581,
      "learning_rate": 0.0001518181818181818,
      "loss": 0.0865,
      "step": 266
    },
    {
      "epoch": 0.24272727272727274,
      "grad_norm": 0.5362390875816345,
      "learning_rate": 0.00015163636363636364,
      "loss": 0.0788,
      "step": 267
    },
    {
      "epoch": 0.24363636363636362,
      "grad_norm": 0.28874656558036804,
      "learning_rate": 0.00015145454545454547,
      "loss": 0.0517,
      "step": 268
    },
    {
      "epoch": 0.24454545454545454,
      "grad_norm": 0.3100777864456177,
      "learning_rate": 0.00015127272727272727,
      "loss": 0.0638,
      "step": 269
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.3305991590023041,
      "learning_rate": 0.0001510909090909091,
      "loss": 0.0789,
      "step": 270
    },
    {
      "epoch": 0.24636363636363637,
      "grad_norm": 0.476613849401474,
      "learning_rate": 0.0001509090909090909,
      "loss": 0.041,
      "step": 271
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.3935636281967163,
      "learning_rate": 0.00015072727272727273,
      "loss": 0.0651,
      "step": 272
    },
    {
      "epoch": 0.24818181818181817,
      "grad_norm": 0.35811352729797363,
      "learning_rate": 0.00015054545454545456,
      "loss": 0.0418,
      "step": 273
    },
    {
      "epoch": 0.24909090909090909,
      "grad_norm": 0.2877602279186249,
      "learning_rate": 0.00015036363636363638,
      "loss": 0.0357,
      "step": 274
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.4203980565071106,
      "learning_rate": 0.00015018181818181819,
      "loss": 0.0582,
      "step": 275
    },
    {
      "epoch": 0.2509090909090909,
      "grad_norm": 0.695185661315918,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.1121,
      "step": 276
    },
    {
      "epoch": 0.25181818181818183,
      "grad_norm": 0.27368593215942383,
      "learning_rate": 0.00014981818181818184,
      "loss": 0.0534,
      "step": 277
    },
    {
      "epoch": 0.25272727272727274,
      "grad_norm": 0.3566378057003021,
      "learning_rate": 0.00014963636363636364,
      "loss": 0.0606,
      "step": 278
    },
    {
      "epoch": 0.25363636363636366,
      "grad_norm": 0.4263373911380768,
      "learning_rate": 0.00014945454545454547,
      "loss": 0.0962,
      "step": 279
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.5588882565498352,
      "learning_rate": 0.00014927272727272727,
      "loss": 0.0628,
      "step": 280
    },
    {
      "epoch": 0.25545454545454543,
      "grad_norm": 0.3418556749820709,
      "learning_rate": 0.0001490909090909091,
      "loss": 0.0553,
      "step": 281
    },
    {
      "epoch": 0.25636363636363635,
      "grad_norm": 0.4757777154445648,
      "learning_rate": 0.0001489090909090909,
      "loss": 0.0518,
      "step": 282
    },
    {
      "epoch": 0.25727272727272726,
      "grad_norm": 0.320576012134552,
      "learning_rate": 0.00014872727272727273,
      "loss": 0.0578,
      "step": 283
    },
    {
      "epoch": 0.2581818181818182,
      "grad_norm": 0.8968614935874939,
      "learning_rate": 0.00014854545454545453,
      "loss": 0.0701,
      "step": 284
    },
    {
      "epoch": 0.2590909090909091,
      "grad_norm": 0.30290350317955017,
      "learning_rate": 0.00014836363636363636,
      "loss": 0.0595,
      "step": 285
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.6205119490623474,
      "learning_rate": 0.0001481818181818182,
      "loss": 0.0588,
      "step": 286
    },
    {
      "epoch": 0.2609090909090909,
      "grad_norm": 0.38472452759742737,
      "learning_rate": 0.000148,
      "loss": 0.0674,
      "step": 287
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.9134518504142761,
      "learning_rate": 0.00014781818181818182,
      "loss": 0.0797,
      "step": 288
    },
    {
      "epoch": 0.26272727272727275,
      "grad_norm": 0.41153064370155334,
      "learning_rate": 0.00014763636363636365,
      "loss": 0.0256,
      "step": 289
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.3434330224990845,
      "learning_rate": 0.00014745454545454548,
      "loss": 0.0608,
      "step": 290
    },
    {
      "epoch": 0.26454545454545453,
      "grad_norm": 0.6904555559158325,
      "learning_rate": 0.00014727272727272728,
      "loss": 0.0603,
      "step": 291
    },
    {
      "epoch": 0.26545454545454544,
      "grad_norm": 0.5986083745956421,
      "learning_rate": 0.0001470909090909091,
      "loss": 0.0581,
      "step": 292
    },
    {
      "epoch": 0.26636363636363636,
      "grad_norm": 0.3535238206386566,
      "learning_rate": 0.0001469090909090909,
      "loss": 0.0567,
      "step": 293
    },
    {
      "epoch": 0.2672727272727273,
      "grad_norm": 0.3912685513496399,
      "learning_rate": 0.00014672727272727274,
      "loss": 0.0526,
      "step": 294
    },
    {
      "epoch": 0.2681818181818182,
      "grad_norm": 0.27785733342170715,
      "learning_rate": 0.00014654545454545457,
      "loss": 0.0527,
      "step": 295
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.35518336296081543,
      "learning_rate": 0.00014636363636363637,
      "loss": 0.0608,
      "step": 296
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.17473004758358002,
      "learning_rate": 0.0001461818181818182,
      "loss": 0.0502,
      "step": 297
    },
    {
      "epoch": 0.27090909090909093,
      "grad_norm": 0.32768329977989197,
      "learning_rate": 0.000146,
      "loss": 0.0383,
      "step": 298
    },
    {
      "epoch": 0.2718181818181818,
      "grad_norm": 0.2942841053009033,
      "learning_rate": 0.00014581818181818183,
      "loss": 0.0435,
      "step": 299
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.2813413441181183,
      "learning_rate": 0.00014563636363636363,
      "loss": 0.0613,
      "step": 300
    },
    {
      "epoch": 0.2736363636363636,
      "grad_norm": 0.49997127056121826,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.0713,
      "step": 301
    },
    {
      "epoch": 0.27454545454545454,
      "grad_norm": 0.4218082129955292,
      "learning_rate": 0.00014527272727272726,
      "loss": 0.0772,
      "step": 302
    },
    {
      "epoch": 0.27545454545454545,
      "grad_norm": 0.3178226053714752,
      "learning_rate": 0.0001450909090909091,
      "loss": 0.0631,
      "step": 303
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.37111949920654297,
      "learning_rate": 0.00014490909090909092,
      "loss": 0.0611,
      "step": 304
    },
    {
      "epoch": 0.2772727272727273,
      "grad_norm": 0.331106573343277,
      "learning_rate": 0.00014472727272727274,
      "loss": 0.0724,
      "step": 305
    },
    {
      "epoch": 0.2781818181818182,
      "grad_norm": 0.6825160384178162,
      "learning_rate": 0.00014454545454545457,
      "loss": 0.0592,
      "step": 306
    },
    {
      "epoch": 0.2790909090909091,
      "grad_norm": 0.2132847160100937,
      "learning_rate": 0.00014436363636363637,
      "loss": 0.0529,
      "step": 307
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.32321757078170776,
      "learning_rate": 0.0001441818181818182,
      "loss": 0.054,
      "step": 308
    },
    {
      "epoch": 0.2809090909090909,
      "grad_norm": 0.22994203865528107,
      "learning_rate": 0.000144,
      "loss": 0.0574,
      "step": 309
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.4163561165332794,
      "learning_rate": 0.00014381818181818183,
      "loss": 0.0628,
      "step": 310
    },
    {
      "epoch": 0.2827272727272727,
      "grad_norm": 0.2362978756427765,
      "learning_rate": 0.00014363636363636363,
      "loss": 0.0349,
      "step": 311
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.26621732115745544,
      "learning_rate": 0.00014345454545454546,
      "loss": 0.0594,
      "step": 312
    },
    {
      "epoch": 0.28454545454545455,
      "grad_norm": 0.4258481562137604,
      "learning_rate": 0.00014327272727272726,
      "loss": 0.0649,
      "step": 313
    },
    {
      "epoch": 0.28545454545454546,
      "grad_norm": 0.23366166651248932,
      "learning_rate": 0.0001430909090909091,
      "loss": 0.0565,
      "step": 314
    },
    {
      "epoch": 0.2863636363636364,
      "grad_norm": 0.1733178347349167,
      "learning_rate": 0.00014290909090909092,
      "loss": 0.0463,
      "step": 315
    },
    {
      "epoch": 0.2872727272727273,
      "grad_norm": 0.33140629529953003,
      "learning_rate": 0.00014272727272727272,
      "loss": 0.0537,
      "step": 316
    },
    {
      "epoch": 0.2881818181818182,
      "grad_norm": 0.24063198268413544,
      "learning_rate": 0.00014254545454545455,
      "loss": 0.0524,
      "step": 317
    },
    {
      "epoch": 0.28909090909090907,
      "grad_norm": 0.374969482421875,
      "learning_rate": 0.00014236363636363635,
      "loss": 0.0587,
      "step": 318
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.7078608274459839,
      "learning_rate": 0.00014218181818181818,
      "loss": 0.0669,
      "step": 319
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.585820734500885,
      "learning_rate": 0.000142,
      "loss": 0.0692,
      "step": 320
    },
    {
      "epoch": 0.2918181818181818,
      "grad_norm": 0.39992237091064453,
      "learning_rate": 0.00014181818181818184,
      "loss": 0.0343,
      "step": 321
    },
    {
      "epoch": 0.2927272727272727,
      "grad_norm": 0.6490345001220703,
      "learning_rate": 0.00014163636363636364,
      "loss": 0.0622,
      "step": 322
    },
    {
      "epoch": 0.29363636363636364,
      "grad_norm": 1.13790762424469,
      "learning_rate": 0.00014145454545454547,
      "loss": 0.1416,
      "step": 323
    },
    {
      "epoch": 0.29454545454545455,
      "grad_norm": 0.24870239198207855,
      "learning_rate": 0.0001412727272727273,
      "loss": 0.0496,
      "step": 324
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.29779812693595886,
      "learning_rate": 0.0001410909090909091,
      "loss": 0.0554,
      "step": 325
    },
    {
      "epoch": 0.2963636363636364,
      "grad_norm": 0.24062931537628174,
      "learning_rate": 0.00014090909090909093,
      "loss": 0.0555,
      "step": 326
    },
    {
      "epoch": 0.2972727272727273,
      "grad_norm": 0.9518408179283142,
      "learning_rate": 0.00014072727272727273,
      "loss": 0.0709,
      "step": 327
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.2986001670360565,
      "learning_rate": 0.00014054545454545456,
      "loss": 0.0338,
      "step": 328
    },
    {
      "epoch": 0.2990909090909091,
      "grad_norm": 0.2352650761604309,
      "learning_rate": 0.00014036363636363636,
      "loss": 0.0315,
      "step": 329
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.40472808480262756,
      "learning_rate": 0.00014018181818181819,
      "loss": 0.0895,
      "step": 330
    },
    {
      "epoch": 0.3009090909090909,
      "grad_norm": 0.18986326456069946,
      "learning_rate": 0.00014,
      "loss": 0.0533,
      "step": 331
    },
    {
      "epoch": 0.3018181818181818,
      "grad_norm": 0.3095628619194031,
      "learning_rate": 0.00013981818181818182,
      "loss": 0.0648,
      "step": 332
    },
    {
      "epoch": 0.30272727272727273,
      "grad_norm": 0.2555875778198242,
      "learning_rate": 0.00013963636363636364,
      "loss": 0.0334,
      "step": 333
    },
    {
      "epoch": 0.30363636363636365,
      "grad_norm": 0.2453082650899887,
      "learning_rate": 0.00013945454545454547,
      "loss": 0.0632,
      "step": 334
    },
    {
      "epoch": 0.30454545454545456,
      "grad_norm": 0.2123185247182846,
      "learning_rate": 0.00013927272727272727,
      "loss": 0.0303,
      "step": 335
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.25283560156822205,
      "learning_rate": 0.0001390909090909091,
      "loss": 0.0598,
      "step": 336
    },
    {
      "epoch": 0.30636363636363634,
      "grad_norm": 0.2467118501663208,
      "learning_rate": 0.00013890909090909093,
      "loss": 0.0335,
      "step": 337
    },
    {
      "epoch": 0.30727272727272725,
      "grad_norm": 0.2974618375301361,
      "learning_rate": 0.00013872727272727273,
      "loss": 0.0651,
      "step": 338
    },
    {
      "epoch": 0.30818181818181817,
      "grad_norm": 0.349877268075943,
      "learning_rate": 0.00013854545454545456,
      "loss": 0.0654,
      "step": 339
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.2828841507434845,
      "learning_rate": 0.00013836363636363636,
      "loss": 0.0711,
      "step": 340
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.25441470742225647,
      "learning_rate": 0.0001381818181818182,
      "loss": 0.0623,
      "step": 341
    },
    {
      "epoch": 0.3109090909090909,
      "grad_norm": 0.28517380356788635,
      "learning_rate": 0.000138,
      "loss": 0.032,
      "step": 342
    },
    {
      "epoch": 0.3118181818181818,
      "grad_norm": 0.407034695148468,
      "learning_rate": 0.00013781818181818182,
      "loss": 0.0563,
      "step": 343
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.5762506127357483,
      "learning_rate": 0.00013763636363636365,
      "loss": 0.0911,
      "step": 344
    },
    {
      "epoch": 0.31363636363636366,
      "grad_norm": 0.3475022614002228,
      "learning_rate": 0.00013745454545454545,
      "loss": 0.0244,
      "step": 345
    },
    {
      "epoch": 0.3145454545454546,
      "grad_norm": 0.21855224668979645,
      "learning_rate": 0.00013727272727272728,
      "loss": 0.059,
      "step": 346
    },
    {
      "epoch": 0.31545454545454543,
      "grad_norm": 0.2575247585773468,
      "learning_rate": 0.00013709090909090908,
      "loss": 0.0293,
      "step": 347
    },
    {
      "epoch": 0.31636363636363635,
      "grad_norm": 0.25404801964759827,
      "learning_rate": 0.0001369090909090909,
      "loss": 0.0539,
      "step": 348
    },
    {
      "epoch": 0.31727272727272726,
      "grad_norm": 0.21709829568862915,
      "learning_rate": 0.00013672727272727274,
      "loss": 0.0262,
      "step": 349
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.7160714268684387,
      "learning_rate": 0.00013654545454545457,
      "loss": 0.0708,
      "step": 350
    },
    {
      "epoch": 0.3190909090909091,
      "grad_norm": 0.2654300928115845,
      "learning_rate": 0.00013636363636363637,
      "loss": 0.0517,
      "step": 351
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.30640852451324463,
      "learning_rate": 0.0001361818181818182,
      "loss": 0.0568,
      "step": 352
    },
    {
      "epoch": 0.3209090909090909,
      "grad_norm": 0.5211756825447083,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.0376,
      "step": 353
    },
    {
      "epoch": 0.32181818181818184,
      "grad_norm": 0.30252358317375183,
      "learning_rate": 0.00013581818181818183,
      "loss": 0.0547,
      "step": 354
    },
    {
      "epoch": 0.32272727272727275,
      "grad_norm": 0.22614049911499023,
      "learning_rate": 0.00013563636363636366,
      "loss": 0.027,
      "step": 355
    },
    {
      "epoch": 0.3236363636363636,
      "grad_norm": 0.22392538189888,
      "learning_rate": 0.00013545454545454546,
      "loss": 0.0256,
      "step": 356
    },
    {
      "epoch": 0.3245454545454545,
      "grad_norm": 0.526628851890564,
      "learning_rate": 0.00013527272727272729,
      "loss": 0.0696,
      "step": 357
    },
    {
      "epoch": 0.32545454545454544,
      "grad_norm": 0.5137444734573364,
      "learning_rate": 0.0001350909090909091,
      "loss": 0.0662,
      "step": 358
    },
    {
      "epoch": 0.32636363636363636,
      "grad_norm": 0.401333212852478,
      "learning_rate": 0.00013490909090909092,
      "loss": 0.0637,
      "step": 359
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.48179176449775696,
      "learning_rate": 0.00013472727272727272,
      "loss": 0.0662,
      "step": 360
    },
    {
      "epoch": 0.3281818181818182,
      "grad_norm": 0.23825472593307495,
      "learning_rate": 0.00013454545454545455,
      "loss": 0.0538,
      "step": 361
    },
    {
      "epoch": 0.3290909090909091,
      "grad_norm": 0.2028726190328598,
      "learning_rate": 0.00013436363636363637,
      "loss": 0.0528,
      "step": 362
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.5386766195297241,
      "learning_rate": 0.00013418181818181818,
      "loss": 0.0641,
      "step": 363
    },
    {
      "epoch": 0.33090909090909093,
      "grad_norm": 0.879684329032898,
      "learning_rate": 0.000134,
      "loss": 0.1,
      "step": 364
    },
    {
      "epoch": 0.33181818181818185,
      "grad_norm": 0.5751354694366455,
      "learning_rate": 0.00013381818181818183,
      "loss": 0.0697,
      "step": 365
    },
    {
      "epoch": 0.3327272727272727,
      "grad_norm": 0.2810046374797821,
      "learning_rate": 0.00013363636363636366,
      "loss": 0.0258,
      "step": 366
    },
    {
      "epoch": 0.3336363636363636,
      "grad_norm": 0.17471550405025482,
      "learning_rate": 0.00013345454545454546,
      "loss": 0.0521,
      "step": 367
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 0.535923421382904,
      "learning_rate": 0.0001332727272727273,
      "loss": 0.0566,
      "step": 368
    },
    {
      "epoch": 0.33545454545454545,
      "grad_norm": 0.27498024702072144,
      "learning_rate": 0.0001330909090909091,
      "loss": 0.0263,
      "step": 369
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.26790621876716614,
      "learning_rate": 0.00013290909090909092,
      "loss": 0.0538,
      "step": 370
    },
    {
      "epoch": 0.3372727272727273,
      "grad_norm": 0.4243675768375397,
      "learning_rate": 0.00013272727272727275,
      "loss": 0.0578,
      "step": 371
    },
    {
      "epoch": 0.3381818181818182,
      "grad_norm": 1.8800444602966309,
      "learning_rate": 0.00013254545454545455,
      "loss": 0.0609,
      "step": 372
    },
    {
      "epoch": 0.3390909090909091,
      "grad_norm": 0.30719539523124695,
      "learning_rate": 0.00013236363636363638,
      "loss": 0.0487,
      "step": 373
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.4191758632659912,
      "learning_rate": 0.00013218181818181818,
      "loss": 0.0609,
      "step": 374
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.38152873516082764,
      "learning_rate": 0.000132,
      "loss": 0.0566,
      "step": 375
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 0.42054736614227295,
      "learning_rate": 0.0001318181818181818,
      "loss": 0.0539,
      "step": 376
    },
    {
      "epoch": 0.3427272727272727,
      "grad_norm": 1.2096164226531982,
      "learning_rate": 0.00013163636363636364,
      "loss": 0.078,
      "step": 377
    },
    {
      "epoch": 0.34363636363636363,
      "grad_norm": 0.27969950437545776,
      "learning_rate": 0.00013145454545454544,
      "loss": 0.0519,
      "step": 378
    },
    {
      "epoch": 0.34454545454545454,
      "grad_norm": 0.28441449999809265,
      "learning_rate": 0.00013127272727272727,
      "loss": 0.0275,
      "step": 379
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.2087356597185135,
      "learning_rate": 0.0001310909090909091,
      "loss": 0.0244,
      "step": 380
    },
    {
      "epoch": 0.3463636363636364,
      "grad_norm": 0.2210444211959839,
      "learning_rate": 0.00013090909090909093,
      "loss": 0.0461,
      "step": 381
    },
    {
      "epoch": 0.3472727272727273,
      "grad_norm": 0.5052229762077332,
      "learning_rate": 0.00013072727272727276,
      "loss": 0.0542,
      "step": 382
    },
    {
      "epoch": 0.3481818181818182,
      "grad_norm": 0.31664231419563293,
      "learning_rate": 0.00013054545454545456,
      "loss": 0.0566,
      "step": 383
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.37068459391593933,
      "learning_rate": 0.00013036363636363639,
      "loss": 0.062,
      "step": 384
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.6041725277900696,
      "learning_rate": 0.0001301818181818182,
      "loss": 0.0559,
      "step": 385
    },
    {
      "epoch": 0.3509090909090909,
      "grad_norm": 0.4123896360397339,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.0714,
      "step": 386
    },
    {
      "epoch": 0.3518181818181818,
      "grad_norm": 0.3000895380973816,
      "learning_rate": 0.00012981818181818182,
      "loss": 0.054,
      "step": 387
    },
    {
      "epoch": 0.3527272727272727,
      "grad_norm": 0.28894808888435364,
      "learning_rate": 0.00012963636363636365,
      "loss": 0.0586,
      "step": 388
    },
    {
      "epoch": 0.35363636363636364,
      "grad_norm": 0.41421765089035034,
      "learning_rate": 0.00012945454545454545,
      "loss": 0.0656,
      "step": 389
    },
    {
      "epoch": 0.35454545454545455,
      "grad_norm": 0.5089485049247742,
      "learning_rate": 0.00012927272727272728,
      "loss": 0.0794,
      "step": 390
    },
    {
      "epoch": 0.35545454545454547,
      "grad_norm": 0.33198341727256775,
      "learning_rate": 0.0001290909090909091,
      "loss": 0.0546,
      "step": 391
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 0.24477115273475647,
      "learning_rate": 0.0001289090909090909,
      "loss": 0.0548,
      "step": 392
    },
    {
      "epoch": 0.3572727272727273,
      "grad_norm": 0.32516542077064514,
      "learning_rate": 0.00012872727272727273,
      "loss": 0.0643,
      "step": 393
    },
    {
      "epoch": 0.35818181818181816,
      "grad_norm": 0.5522053837776184,
      "learning_rate": 0.00012854545454545454,
      "loss": 0.0657,
      "step": 394
    },
    {
      "epoch": 0.35909090909090907,
      "grad_norm": 0.4417881667613983,
      "learning_rate": 0.00012836363636363636,
      "loss": 0.0667,
      "step": 395
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.3131585717201233,
      "learning_rate": 0.0001281818181818182,
      "loss": 0.0336,
      "step": 396
    },
    {
      "epoch": 0.3609090909090909,
      "grad_norm": 1.796571969985962,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.0735,
      "step": 397
    },
    {
      "epoch": 0.3618181818181818,
      "grad_norm": 0.21185645461082458,
      "learning_rate": 0.00012781818181818182,
      "loss": 0.0544,
      "step": 398
    },
    {
      "epoch": 0.36272727272727273,
      "grad_norm": 0.34661370515823364,
      "learning_rate": 0.00012763636363636365,
      "loss": 0.0568,
      "step": 399
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.2482728362083435,
      "learning_rate": 0.00012745454545454548,
      "loss": 0.0539,
      "step": 400
    },
    {
      "epoch": 0.36454545454545456,
      "grad_norm": 0.37459617853164673,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.0385,
      "step": 401
    },
    {
      "epoch": 0.3654545454545455,
      "grad_norm": 0.33130964636802673,
      "learning_rate": 0.0001270909090909091,
      "loss": 0.0561,
      "step": 402
    },
    {
      "epoch": 0.3663636363636364,
      "grad_norm": 0.3169567286968231,
      "learning_rate": 0.0001269090909090909,
      "loss": 0.0654,
      "step": 403
    },
    {
      "epoch": 0.36727272727272725,
      "grad_norm": 0.46138474345207214,
      "learning_rate": 0.00012672727272727274,
      "loss": 0.0569,
      "step": 404
    },
    {
      "epoch": 0.36818181818181817,
      "grad_norm": 0.2633850872516632,
      "learning_rate": 0.00012654545454545454,
      "loss": 0.0514,
      "step": 405
    },
    {
      "epoch": 0.3690909090909091,
      "grad_norm": 0.4556572735309601,
      "learning_rate": 0.00012636363636363637,
      "loss": 0.058,
      "step": 406
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.26810529828071594,
      "learning_rate": 0.00012618181818181817,
      "loss": 0.0506,
      "step": 407
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.6776942610740662,
      "learning_rate": 0.000126,
      "loss": 0.0681,
      "step": 408
    },
    {
      "epoch": 0.3718181818181818,
      "grad_norm": 0.17854923009872437,
      "learning_rate": 0.00012581818181818183,
      "loss": 0.0478,
      "step": 409
    },
    {
      "epoch": 0.37272727272727274,
      "grad_norm": 0.3468070328235626,
      "learning_rate": 0.00012563636363636363,
      "loss": 0.0529,
      "step": 410
    },
    {
      "epoch": 0.37363636363636366,
      "grad_norm": 0.30567243695259094,
      "learning_rate": 0.00012545454545454546,
      "loss": 0.0523,
      "step": 411
    },
    {
      "epoch": 0.37454545454545457,
      "grad_norm": 0.28140345215797424,
      "learning_rate": 0.0001252727272727273,
      "loss": 0.0298,
      "step": 412
    },
    {
      "epoch": 0.37545454545454543,
      "grad_norm": 0.2122993767261505,
      "learning_rate": 0.00012509090909090912,
      "loss": 0.025,
      "step": 413
    },
    {
      "epoch": 0.37636363636363634,
      "grad_norm": 0.20685145258903503,
      "learning_rate": 0.00012490909090909092,
      "loss": 0.052,
      "step": 414
    },
    {
      "epoch": 0.37727272727272726,
      "grad_norm": 0.21590258181095123,
      "learning_rate": 0.00012472727272727275,
      "loss": 0.0547,
      "step": 415
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 0.3307115137577057,
      "learning_rate": 0.00012454545454545455,
      "loss": 0.0646,
      "step": 416
    },
    {
      "epoch": 0.3790909090909091,
      "grad_norm": 0.17455750703811646,
      "learning_rate": 0.00012436363636363638,
      "loss": 0.0497,
      "step": 417
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.5359787940979004,
      "learning_rate": 0.00012418181818181818,
      "loss": 0.063,
      "step": 418
    },
    {
      "epoch": 0.3809090909090909,
      "grad_norm": 0.21668583154678345,
      "learning_rate": 0.000124,
      "loss": 0.052,
      "step": 419
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.44090867042541504,
      "learning_rate": 0.00012381818181818183,
      "loss": 0.0607,
      "step": 420
    },
    {
      "epoch": 0.38272727272727275,
      "grad_norm": 0.37858569622039795,
      "learning_rate": 0.00012363636363636364,
      "loss": 0.0634,
      "step": 421
    },
    {
      "epoch": 0.3836363636363636,
      "grad_norm": 0.2864830195903778,
      "learning_rate": 0.00012345454545454546,
      "loss": 0.0633,
      "step": 422
    },
    {
      "epoch": 0.3845454545454545,
      "grad_norm": 0.29256516695022583,
      "learning_rate": 0.00012327272727272727,
      "loss": 0.0575,
      "step": 423
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.15663467347621918,
      "learning_rate": 0.0001230909090909091,
      "loss": 0.0494,
      "step": 424
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.373901903629303,
      "learning_rate": 0.0001229090909090909,
      "loss": 0.0598,
      "step": 425
    },
    {
      "epoch": 0.38727272727272727,
      "grad_norm": 0.29351624846458435,
      "learning_rate": 0.00012272727272727272,
      "loss": 0.0618,
      "step": 426
    },
    {
      "epoch": 0.3881818181818182,
      "grad_norm": 0.3674153685569763,
      "learning_rate": 0.00012254545454545455,
      "loss": 0.0674,
      "step": 427
    },
    {
      "epoch": 0.3890909090909091,
      "grad_norm": 0.224331796169281,
      "learning_rate": 0.00012236363636363638,
      "loss": 0.025,
      "step": 428
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.26831763982772827,
      "learning_rate": 0.0001221818181818182,
      "loss": 0.0619,
      "step": 429
    },
    {
      "epoch": 0.39090909090909093,
      "grad_norm": 0.41530120372772217,
      "learning_rate": 0.000122,
      "loss": 0.0589,
      "step": 430
    },
    {
      "epoch": 0.39181818181818184,
      "grad_norm": 0.16580913960933685,
      "learning_rate": 0.00012181818181818183,
      "loss": 0.0578,
      "step": 431
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.2596126198768616,
      "learning_rate": 0.00012163636363636364,
      "loss": 0.0615,
      "step": 432
    },
    {
      "epoch": 0.3936363636363636,
      "grad_norm": 0.3120420575141907,
      "learning_rate": 0.00012145454545454547,
      "loss": 0.0601,
      "step": 433
    },
    {
      "epoch": 0.39454545454545453,
      "grad_norm": 0.16304001212120056,
      "learning_rate": 0.00012127272727272727,
      "loss": 0.0474,
      "step": 434
    },
    {
      "epoch": 0.39545454545454545,
      "grad_norm": 0.29967305064201355,
      "learning_rate": 0.0001210909090909091,
      "loss": 0.056,
      "step": 435
    },
    {
      "epoch": 0.39636363636363636,
      "grad_norm": 0.31106922030448914,
      "learning_rate": 0.0001209090909090909,
      "loss": 0.0574,
      "step": 436
    },
    {
      "epoch": 0.3972727272727273,
      "grad_norm": 0.16946370899677277,
      "learning_rate": 0.00012072727272727273,
      "loss": 0.0487,
      "step": 437
    },
    {
      "epoch": 0.3981818181818182,
      "grad_norm": 0.3095954954624176,
      "learning_rate": 0.00012054545454545456,
      "loss": 0.0553,
      "step": 438
    },
    {
      "epoch": 0.3990909090909091,
      "grad_norm": 0.25037890672683716,
      "learning_rate": 0.00012036363636363637,
      "loss": 0.051,
      "step": 439
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2038840502500534,
      "learning_rate": 0.0001201818181818182,
      "loss": 0.0535,
      "step": 440
    },
    {
      "epoch": 0.4009090909090909,
      "grad_norm": 0.21250835061073303,
      "learning_rate": 0.00012,
      "loss": 0.0482,
      "step": 441
    },
    {
      "epoch": 0.4018181818181818,
      "grad_norm": 0.4324744641780853,
      "learning_rate": 0.00011981818181818183,
      "loss": 0.065,
      "step": 442
    },
    {
      "epoch": 0.4027272727272727,
      "grad_norm": 0.2809184789657593,
      "learning_rate": 0.00011963636363636363,
      "loss": 0.0333,
      "step": 443
    },
    {
      "epoch": 0.4036363636363636,
      "grad_norm": 0.39474454522132874,
      "learning_rate": 0.00011945454545454546,
      "loss": 0.0623,
      "step": 444
    },
    {
      "epoch": 0.40454545454545454,
      "grad_norm": 0.18584288656711578,
      "learning_rate": 0.00011927272727272726,
      "loss": 0.0479,
      "step": 445
    },
    {
      "epoch": 0.40545454545454546,
      "grad_norm": 0.23968562483787537,
      "learning_rate": 0.00011909090909090909,
      "loss": 0.0617,
      "step": 446
    },
    {
      "epoch": 0.40636363636363637,
      "grad_norm": 0.1965046226978302,
      "learning_rate": 0.00011890909090909092,
      "loss": 0.0478,
      "step": 447
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.235986590385437,
      "learning_rate": 0.00011872727272727274,
      "loss": 0.052,
      "step": 448
    },
    {
      "epoch": 0.4081818181818182,
      "grad_norm": 0.269978404045105,
      "learning_rate": 0.00011854545454545456,
      "loss": 0.0607,
      "step": 449
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.5871107578277588,
      "learning_rate": 0.00011836363636363637,
      "loss": 0.0561,
      "step": 450
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.5020459890365601,
      "learning_rate": 0.0001181818181818182,
      "loss": 0.0611,
      "step": 451
    },
    {
      "epoch": 0.4109090909090909,
      "grad_norm": 0.3803790211677551,
      "learning_rate": 0.000118,
      "loss": 0.0366,
      "step": 452
    },
    {
      "epoch": 0.4118181818181818,
      "grad_norm": 0.302725613117218,
      "learning_rate": 0.00011781818181818182,
      "loss": 0.0585,
      "step": 453
    },
    {
      "epoch": 0.4127272727272727,
      "grad_norm": 0.4407569169998169,
      "learning_rate": 0.00011763636363636364,
      "loss": 0.0653,
      "step": 454
    },
    {
      "epoch": 0.41363636363636364,
      "grad_norm": 0.3270758092403412,
      "learning_rate": 0.00011745454545454547,
      "loss": 0.0409,
      "step": 455
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 0.24664907157421112,
      "learning_rate": 0.00011727272727272727,
      "loss": 0.0554,
      "step": 456
    },
    {
      "epoch": 0.41545454545454547,
      "grad_norm": 0.38030359148979187,
      "learning_rate": 0.0001170909090909091,
      "loss": 0.0531,
      "step": 457
    },
    {
      "epoch": 0.4163636363636364,
      "grad_norm": 0.2152968794107437,
      "learning_rate": 0.00011690909090909093,
      "loss": 0.0551,
      "step": 458
    },
    {
      "epoch": 0.4172727272727273,
      "grad_norm": 0.45130792260169983,
      "learning_rate": 0.00011672727272727273,
      "loss": 0.0368,
      "step": 459
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.356636643409729,
      "learning_rate": 0.00011654545454545456,
      "loss": 0.0642,
      "step": 460
    },
    {
      "epoch": 0.41909090909090907,
      "grad_norm": 0.17512737214565277,
      "learning_rate": 0.00011636363636363636,
      "loss": 0.0482,
      "step": 461
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.1930370181798935,
      "learning_rate": 0.00011618181818181819,
      "loss": 0.0248,
      "step": 462
    },
    {
      "epoch": 0.4209090909090909,
      "grad_norm": 0.21924149990081787,
      "learning_rate": 0.000116,
      "loss": 0.0583,
      "step": 463
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.41108423471450806,
      "learning_rate": 0.00011581818181818183,
      "loss": 0.067,
      "step": 464
    },
    {
      "epoch": 0.42272727272727273,
      "grad_norm": 0.5114759802818298,
      "learning_rate": 0.00011563636363636363,
      "loss": 0.0567,
      "step": 465
    },
    {
      "epoch": 0.42363636363636364,
      "grad_norm": 0.24411222338676453,
      "learning_rate": 0.00011545454545454546,
      "loss": 0.0317,
      "step": 466
    },
    {
      "epoch": 0.42454545454545456,
      "grad_norm": 0.25589290261268616,
      "learning_rate": 0.00011527272727272729,
      "loss": 0.055,
      "step": 467
    },
    {
      "epoch": 0.4254545454545455,
      "grad_norm": 0.19092974066734314,
      "learning_rate": 0.00011509090909090909,
      "loss": 0.0543,
      "step": 468
    },
    {
      "epoch": 0.4263636363636364,
      "grad_norm": 0.19766953587532043,
      "learning_rate": 0.00011490909090909092,
      "loss": 0.0578,
      "step": 469
    },
    {
      "epoch": 0.42727272727272725,
      "grad_norm": 0.2944876551628113,
      "learning_rate": 0.00011472727272727273,
      "loss": 0.0302,
      "step": 470
    },
    {
      "epoch": 0.42818181818181816,
      "grad_norm": 0.2261492758989334,
      "learning_rate": 0.00011454545454545456,
      "loss": 0.0548,
      "step": 471
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.23333518207073212,
      "learning_rate": 0.00011436363636363636,
      "loss": 0.0601,
      "step": 472
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.3678867518901825,
      "learning_rate": 0.00011418181818181819,
      "loss": 0.0684,
      "step": 473
    },
    {
      "epoch": 0.4309090909090909,
      "grad_norm": 0.20013348758220673,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.0279,
      "step": 474
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.23921912908554077,
      "learning_rate": 0.00011381818181818182,
      "loss": 0.0501,
      "step": 475
    },
    {
      "epoch": 0.43272727272727274,
      "grad_norm": 0.1860620081424713,
      "learning_rate": 0.00011363636363636365,
      "loss": 0.0533,
      "step": 476
    },
    {
      "epoch": 0.43363636363636365,
      "grad_norm": 0.3705650866031647,
      "learning_rate": 0.00011345454545454545,
      "loss": 0.0618,
      "step": 477
    },
    {
      "epoch": 0.43454545454545457,
      "grad_norm": 0.16604411602020264,
      "learning_rate": 0.00011327272727272728,
      "loss": 0.0498,
      "step": 478
    },
    {
      "epoch": 0.4354545454545454,
      "grad_norm": 1.017686128616333,
      "learning_rate": 0.0001130909090909091,
      "loss": 0.0625,
      "step": 479
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.1998215764760971,
      "learning_rate": 0.00011290909090909092,
      "loss": 0.0486,
      "step": 480
    },
    {
      "epoch": 0.43727272727272726,
      "grad_norm": 0.20952782034873962,
      "learning_rate": 0.00011272727272727272,
      "loss": 0.0517,
      "step": 481
    },
    {
      "epoch": 0.4381818181818182,
      "grad_norm": 0.3302755057811737,
      "learning_rate": 0.00011254545454545455,
      "loss": 0.0608,
      "step": 482
    },
    {
      "epoch": 0.4390909090909091,
      "grad_norm": 0.2577175796031952,
      "learning_rate": 0.00011236363636363635,
      "loss": 0.0545,
      "step": 483
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.36605533957481384,
      "learning_rate": 0.00011218181818181818,
      "loss": 0.0646,
      "step": 484
    },
    {
      "epoch": 0.4409090909090909,
      "grad_norm": 0.46119019389152527,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.0546,
      "step": 485
    },
    {
      "epoch": 0.44181818181818183,
      "grad_norm": 0.2114674299955368,
      "learning_rate": 0.00011181818181818183,
      "loss": 0.0255,
      "step": 486
    },
    {
      "epoch": 0.44272727272727275,
      "grad_norm": 0.44168820977211,
      "learning_rate": 0.00011163636363636366,
      "loss": 0.0625,
      "step": 487
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.45915475487709045,
      "learning_rate": 0.00011145454545454546,
      "loss": 0.0306,
      "step": 488
    },
    {
      "epoch": 0.4445454545454545,
      "grad_norm": 0.604668140411377,
      "learning_rate": 0.00011127272727272729,
      "loss": 0.033,
      "step": 489
    },
    {
      "epoch": 0.44545454545454544,
      "grad_norm": 0.40914684534072876,
      "learning_rate": 0.00011109090909090909,
      "loss": 0.0593,
      "step": 490
    },
    {
      "epoch": 0.44636363636363635,
      "grad_norm": 0.23462608456611633,
      "learning_rate": 0.00011090909090909092,
      "loss": 0.0569,
      "step": 491
    },
    {
      "epoch": 0.44727272727272727,
      "grad_norm": 0.38203924894332886,
      "learning_rate": 0.00011072727272727273,
      "loss": 0.0618,
      "step": 492
    },
    {
      "epoch": 0.4481818181818182,
      "grad_norm": 0.2028045654296875,
      "learning_rate": 0.00011054545454545455,
      "loss": 0.0641,
      "step": 493
    },
    {
      "epoch": 0.4490909090909091,
      "grad_norm": 0.24982093274593353,
      "learning_rate": 0.00011036363636363636,
      "loss": 0.0253,
      "step": 494
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.28545159101486206,
      "learning_rate": 0.00011018181818181819,
      "loss": 0.0577,
      "step": 495
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 0.2881263196468353,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.0533,
      "step": 496
    },
    {
      "epoch": 0.45181818181818184,
      "grad_norm": 0.32865965366363525,
      "learning_rate": 0.00010981818181818182,
      "loss": 0.0526,
      "step": 497
    },
    {
      "epoch": 0.4527272727272727,
      "grad_norm": 0.36418604850769043,
      "learning_rate": 0.00010963636363636365,
      "loss": 0.0652,
      "step": 498
    },
    {
      "epoch": 0.4536363636363636,
      "grad_norm": 0.456924706697464,
      "learning_rate": 0.00010945454545454545,
      "loss": 0.0574,
      "step": 499
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.7064059376716614,
      "learning_rate": 0.00010927272727272728,
      "loss": 0.059,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1378350417070080.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
