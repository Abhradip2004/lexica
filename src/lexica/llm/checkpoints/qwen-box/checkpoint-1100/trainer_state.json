{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 3.8475234508514404,
      "learning_rate": 0.0002,
      "loss": 2.6499,
      "step": 1
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 1.1122139692306519,
      "learning_rate": 0.00019981818181818184,
      "loss": 2.5718,
      "step": 2
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 0.9808070659637451,
      "learning_rate": 0.00019963636363636364,
      "loss": 2.5235,
      "step": 3
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.9163109660148621,
      "learning_rate": 0.00019945454545454547,
      "loss": 2.4331,
      "step": 4
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.7343728542327881,
      "learning_rate": 0.00019927272727272727,
      "loss": 2.3882,
      "step": 5
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.7262378931045532,
      "learning_rate": 0.0001990909090909091,
      "loss": 2.3331,
      "step": 6
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.8618015646934509,
      "learning_rate": 0.0001989090909090909,
      "loss": 2.4559,
      "step": 7
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.724307119846344,
      "learning_rate": 0.00019872727272727273,
      "loss": 2.2546,
      "step": 8
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.8314208388328552,
      "learning_rate": 0.00019854545454545456,
      "loss": 2.3343,
      "step": 9
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.9023773074150085,
      "learning_rate": 0.00019836363636363639,
      "loss": 2.257,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.8332047462463379,
      "learning_rate": 0.00019818181818181821,
      "loss": 2.1302,
      "step": 11
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.871664822101593,
      "learning_rate": 0.00019800000000000002,
      "loss": 2.0186,
      "step": 12
    },
    {
      "epoch": 0.011818181818181818,
      "grad_norm": 0.9215967059135437,
      "learning_rate": 0.00019781818181818184,
      "loss": 1.9742,
      "step": 13
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 1.0580787658691406,
      "learning_rate": 0.00019763636363636365,
      "loss": 1.9072,
      "step": 14
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 1.2257686853408813,
      "learning_rate": 0.00019745454545454547,
      "loss": 1.8173,
      "step": 15
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 1.431105136871338,
      "learning_rate": 0.00019727272727272728,
      "loss": 1.7441,
      "step": 16
    },
    {
      "epoch": 0.015454545454545455,
      "grad_norm": 2.884584903717041,
      "learning_rate": 0.0001970909090909091,
      "loss": 1.728,
      "step": 17
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 4.233071804046631,
      "learning_rate": 0.0001969090909090909,
      "loss": 1.5958,
      "step": 18
    },
    {
      "epoch": 0.017272727272727273,
      "grad_norm": 3.459383964538574,
      "learning_rate": 0.00019672727272727273,
      "loss": 1.5548,
      "step": 19
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 1.6147829294204712,
      "learning_rate": 0.00019654545454545456,
      "loss": 1.4275,
      "step": 20
    },
    {
      "epoch": 0.019090909090909092,
      "grad_norm": 2.7011287212371826,
      "learning_rate": 0.00019636363636363636,
      "loss": 1.4134,
      "step": 21
    },
    {
      "epoch": 0.02,
      "grad_norm": 3.182844877243042,
      "learning_rate": 0.0001961818181818182,
      "loss": 1.3308,
      "step": 22
    },
    {
      "epoch": 0.02090909090909091,
      "grad_norm": 2.0103561878204346,
      "learning_rate": 0.000196,
      "loss": 1.2408,
      "step": 23
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 1.9280016422271729,
      "learning_rate": 0.00019581818181818182,
      "loss": 1.1251,
      "step": 24
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 2.746408224105835,
      "learning_rate": 0.00019563636363636365,
      "loss": 1.0274,
      "step": 25
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 2.244034767150879,
      "learning_rate": 0.00019545454545454548,
      "loss": 0.9494,
      "step": 26
    },
    {
      "epoch": 0.024545454545454544,
      "grad_norm": 2.1545846462249756,
      "learning_rate": 0.00019527272727272728,
      "loss": 0.8412,
      "step": 27
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 2.0328893661499023,
      "learning_rate": 0.0001950909090909091,
      "loss": 0.7052,
      "step": 28
    },
    {
      "epoch": 0.026363636363636363,
      "grad_norm": 1.5851980447769165,
      "learning_rate": 0.0001949090909090909,
      "loss": 0.6942,
      "step": 29
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 1.648553490638733,
      "learning_rate": 0.00019472727272727274,
      "loss": 0.6251,
      "step": 30
    },
    {
      "epoch": 0.028181818181818183,
      "grad_norm": 1.4385238885879517,
      "learning_rate": 0.00019454545454545457,
      "loss": 0.56,
      "step": 31
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 1.5685687065124512,
      "learning_rate": 0.00019436363636363637,
      "loss": 0.4129,
      "step": 32
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.2941879034042358,
      "learning_rate": 0.0001941818181818182,
      "loss": 0.4315,
      "step": 33
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 1.1808931827545166,
      "learning_rate": 0.000194,
      "loss": 0.4151,
      "step": 34
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 1.1275943517684937,
      "learning_rate": 0.00019381818181818183,
      "loss": 0.3793,
      "step": 35
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.857045590877533,
      "learning_rate": 0.00019363636363636363,
      "loss": 0.2153,
      "step": 36
    },
    {
      "epoch": 0.03363636363636364,
      "grad_norm": 0.7470954656600952,
      "learning_rate": 0.00019345454545454546,
      "loss": 0.2028,
      "step": 37
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.5908777713775635,
      "learning_rate": 0.00019327272727272726,
      "loss": 0.2899,
      "step": 38
    },
    {
      "epoch": 0.035454545454545454,
      "grad_norm": 0.5947657227516174,
      "learning_rate": 0.0001930909090909091,
      "loss": 0.1349,
      "step": 39
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.5091672539710999,
      "learning_rate": 0.00019290909090909092,
      "loss": 0.2064,
      "step": 40
    },
    {
      "epoch": 0.03727272727272727,
      "grad_norm": 0.5625275373458862,
      "learning_rate": 0.00019272727272727274,
      "loss": 0.2906,
      "step": 41
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.5606116652488708,
      "learning_rate": 0.00019254545454545457,
      "loss": 0.2524,
      "step": 42
    },
    {
      "epoch": 0.03909090909090909,
      "grad_norm": 0.5210984945297241,
      "learning_rate": 0.00019236363636363637,
      "loss": 0.2853,
      "step": 43
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.6541027426719666,
      "learning_rate": 0.0001921818181818182,
      "loss": 0.2969,
      "step": 44
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 0.5495396852493286,
      "learning_rate": 0.000192,
      "loss": 0.2646,
      "step": 45
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.26841095089912415,
      "learning_rate": 0.00019181818181818183,
      "loss": 0.1298,
      "step": 46
    },
    {
      "epoch": 0.042727272727272725,
      "grad_norm": 0.47711697220802307,
      "learning_rate": 0.00019163636363636363,
      "loss": 0.2366,
      "step": 47
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.47867244482040405,
      "learning_rate": 0.00019145454545454546,
      "loss": 0.2396,
      "step": 48
    },
    {
      "epoch": 0.04454545454545455,
      "grad_norm": 0.505203902721405,
      "learning_rate": 0.0001912727272727273,
      "loss": 0.2304,
      "step": 49
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.4256228506565094,
      "learning_rate": 0.0001910909090909091,
      "loss": 0.1538,
      "step": 50
    },
    {
      "epoch": 0.046363636363636364,
      "grad_norm": 0.5116578936576843,
      "learning_rate": 0.00019090909090909092,
      "loss": 0.1437,
      "step": 51
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.2542370557785034,
      "learning_rate": 0.00019072727272727272,
      "loss": 0.1273,
      "step": 52
    },
    {
      "epoch": 0.04818181818181818,
      "grad_norm": 0.36036983132362366,
      "learning_rate": 0.00019054545454545455,
      "loss": 0.1389,
      "step": 53
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.3746963441371918,
      "learning_rate": 0.00019036363636363635,
      "loss": 0.1361,
      "step": 54
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.6412153244018555,
      "learning_rate": 0.00019018181818181818,
      "loss": 0.1721,
      "step": 55
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.46318620443344116,
      "learning_rate": 0.00019,
      "loss": 0.1191,
      "step": 56
    },
    {
      "epoch": 0.05181818181818182,
      "grad_norm": 0.9299911856651306,
      "learning_rate": 0.00018981818181818184,
      "loss": 0.2163,
      "step": 57
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.6176466941833496,
      "learning_rate": 0.00018963636363636367,
      "loss": 0.1439,
      "step": 58
    },
    {
      "epoch": 0.053636363636363635,
      "grad_norm": 0.7123691439628601,
      "learning_rate": 0.00018945454545454547,
      "loss": 0.2209,
      "step": 59
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.7242896556854248,
      "learning_rate": 0.0001892727272727273,
      "loss": 0.1798,
      "step": 60
    },
    {
      "epoch": 0.05545454545454546,
      "grad_norm": 0.5178562998771667,
      "learning_rate": 0.0001890909090909091,
      "loss": 0.1144,
      "step": 61
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.5213590264320374,
      "learning_rate": 0.00018890909090909093,
      "loss": 0.1208,
      "step": 62
    },
    {
      "epoch": 0.057272727272727274,
      "grad_norm": 2.1917402744293213,
      "learning_rate": 0.00018872727272727273,
      "loss": 0.165,
      "step": 63
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.3921738266944885,
      "learning_rate": 0.00018854545454545456,
      "loss": 0.0978,
      "step": 64
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.5919716954231262,
      "learning_rate": 0.00018836363636363636,
      "loss": 0.1393,
      "step": 65
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.837393045425415,
      "learning_rate": 0.0001881818181818182,
      "loss": 0.201,
      "step": 66
    },
    {
      "epoch": 0.060909090909090906,
      "grad_norm": 0.8871742486953735,
      "learning_rate": 0.000188,
      "loss": 0.2126,
      "step": 67
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.5538226366043091,
      "learning_rate": 0.00018781818181818182,
      "loss": 0.1119,
      "step": 68
    },
    {
      "epoch": 0.06272727272727273,
      "grad_norm": 0.5651689767837524,
      "learning_rate": 0.00018763636363636365,
      "loss": 0.1282,
      "step": 69
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.46789148449897766,
      "learning_rate": 0.00018745454545454545,
      "loss": 0.1145,
      "step": 70
    },
    {
      "epoch": 0.06454545454545454,
      "grad_norm": 0.6433213949203491,
      "learning_rate": 0.00018727272727272728,
      "loss": 0.1654,
      "step": 71
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.5863708257675171,
      "learning_rate": 0.0001870909090909091,
      "loss": 0.1197,
      "step": 72
    },
    {
      "epoch": 0.06636363636363636,
      "grad_norm": 1.0642094612121582,
      "learning_rate": 0.00018690909090909093,
      "loss": 0.0911,
      "step": 73
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.7503703832626343,
      "learning_rate": 0.00018672727272727273,
      "loss": 0.1059,
      "step": 74
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.5599052906036377,
      "learning_rate": 0.00018654545454545456,
      "loss": 0.0894,
      "step": 75
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.651985228061676,
      "learning_rate": 0.00018636363636363636,
      "loss": 0.1377,
      "step": 76
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.39972618222236633,
      "learning_rate": 0.0001861818181818182,
      "loss": 0.0733,
      "step": 77
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.45193052291870117,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.1108,
      "step": 78
    },
    {
      "epoch": 0.07181818181818182,
      "grad_norm": 0.3457213044166565,
      "learning_rate": 0.00018581818181818182,
      "loss": 0.1105,
      "step": 79
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.5565018057823181,
      "learning_rate": 0.00018563636363636365,
      "loss": 0.132,
      "step": 80
    },
    {
      "epoch": 0.07363636363636364,
      "grad_norm": 0.6090856194496155,
      "learning_rate": 0.00018545454545454545,
      "loss": 0.1288,
      "step": 81
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.5270015597343445,
      "learning_rate": 0.00018527272727272728,
      "loss": 0.1058,
      "step": 82
    },
    {
      "epoch": 0.07545454545454545,
      "grad_norm": 0.6306567788124084,
      "learning_rate": 0.00018509090909090908,
      "loss": 0.1216,
      "step": 83
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.5428218245506287,
      "learning_rate": 0.0001849090909090909,
      "loss": 0.1459,
      "step": 84
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.5766558647155762,
      "learning_rate": 0.0001847272727272727,
      "loss": 0.1424,
      "step": 85
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.5183713436126709,
      "learning_rate": 0.00018454545454545454,
      "loss": 0.0909,
      "step": 86
    },
    {
      "epoch": 0.07909090909090909,
      "grad_norm": 1.0369123220443726,
      "learning_rate": 0.00018436363636363637,
      "loss": 0.1141,
      "step": 87
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.5091747045516968,
      "learning_rate": 0.0001841818181818182,
      "loss": 0.0981,
      "step": 88
    },
    {
      "epoch": 0.0809090909090909,
      "grad_norm": 0.7579784393310547,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.1301,
      "step": 89
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.4203048050403595,
      "learning_rate": 0.00018381818181818183,
      "loss": 0.0566,
      "step": 90
    },
    {
      "epoch": 0.08272727272727273,
      "grad_norm": 0.5601348876953125,
      "learning_rate": 0.00018363636363636366,
      "loss": 0.1207,
      "step": 91
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.6357861757278442,
      "learning_rate": 0.00018345454545454546,
      "loss": 0.1343,
      "step": 92
    },
    {
      "epoch": 0.08454545454545455,
      "grad_norm": 0.48749780654907227,
      "learning_rate": 0.0001832727272727273,
      "loss": 0.11,
      "step": 93
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.36760175228118896,
      "learning_rate": 0.0001830909090909091,
      "loss": 0.0434,
      "step": 94
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.3860127925872803,
      "learning_rate": 0.00018290909090909092,
      "loss": 0.0707,
      "step": 95
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.352789044380188,
      "learning_rate": 0.00018272727272727275,
      "loss": 0.0599,
      "step": 96
    },
    {
      "epoch": 0.08818181818181818,
      "grad_norm": 0.541006863117218,
      "learning_rate": 0.00018254545454545455,
      "loss": 0.0857,
      "step": 97
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.3463971018791199,
      "learning_rate": 0.00018236363636363638,
      "loss": 0.0634,
      "step": 98
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.3582240343093872,
      "learning_rate": 0.00018218181818181818,
      "loss": 0.0631,
      "step": 99
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.5624595880508423,
      "learning_rate": 0.000182,
      "loss": 0.0887,
      "step": 100
    },
    {
      "epoch": 0.09181818181818181,
      "grad_norm": 0.3853033483028412,
      "learning_rate": 0.00018181818181818183,
      "loss": 0.0822,
      "step": 101
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.4156612753868103,
      "learning_rate": 0.00018163636363636364,
      "loss": 0.078,
      "step": 102
    },
    {
      "epoch": 0.09363636363636364,
      "grad_norm": 0.38248634338378906,
      "learning_rate": 0.00018145454545454546,
      "loss": 0.067,
      "step": 103
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.5106273889541626,
      "learning_rate": 0.0001812727272727273,
      "loss": 0.0776,
      "step": 104
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.6367241144180298,
      "learning_rate": 0.0001810909090909091,
      "loss": 0.1153,
      "step": 105
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.5572682023048401,
      "learning_rate": 0.00018090909090909092,
      "loss": 0.0466,
      "step": 106
    },
    {
      "epoch": 0.09727272727272727,
      "grad_norm": 0.7476198673248291,
      "learning_rate": 0.00018072727272727275,
      "loss": 0.0721,
      "step": 107
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 1.247708797454834,
      "learning_rate": 0.00018054545454545455,
      "loss": 0.1252,
      "step": 108
    },
    {
      "epoch": 0.09909090909090909,
      "grad_norm": 0.7379880547523499,
      "learning_rate": 0.00018036363636363638,
      "loss": 0.051,
      "step": 109
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.738685667514801,
      "learning_rate": 0.00018018181818181818,
      "loss": 0.0881,
      "step": 110
    },
    {
      "epoch": 0.1009090909090909,
      "grad_norm": 0.884497880935669,
      "learning_rate": 0.00018,
      "loss": 0.0755,
      "step": 111
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.3297708332538605,
      "learning_rate": 0.0001798181818181818,
      "loss": 0.0917,
      "step": 112
    },
    {
      "epoch": 0.10272727272727272,
      "grad_norm": 0.32667309045791626,
      "learning_rate": 0.00017963636363636364,
      "loss": 0.078,
      "step": 113
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.4086042046546936,
      "learning_rate": 0.00017945454545454544,
      "loss": 0.0652,
      "step": 114
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.5783510208129883,
      "learning_rate": 0.00017927272727272727,
      "loss": 0.0912,
      "step": 115
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.370318204164505,
      "learning_rate": 0.0001790909090909091,
      "loss": 0.0687,
      "step": 116
    },
    {
      "epoch": 0.10636363636363637,
      "grad_norm": 0.46975985169410706,
      "learning_rate": 0.00017890909090909093,
      "loss": 0.0865,
      "step": 117
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.373563677072525,
      "learning_rate": 0.00017872727272727273,
      "loss": 0.0951,
      "step": 118
    },
    {
      "epoch": 0.10818181818181818,
      "grad_norm": 0.2745921015739441,
      "learning_rate": 0.00017854545454545456,
      "loss": 0.068,
      "step": 119
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.4455898106098175,
      "learning_rate": 0.0001783636363636364,
      "loss": 0.0679,
      "step": 120
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.753769040107727,
      "learning_rate": 0.0001781818181818182,
      "loss": 0.0887,
      "step": 121
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.28498703241348267,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.0742,
      "step": 122
    },
    {
      "epoch": 0.11181818181818182,
      "grad_norm": 0.38414520025253296,
      "learning_rate": 0.00017781818181818182,
      "loss": 0.0533,
      "step": 123
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.3663877248764038,
      "learning_rate": 0.00017763636363636365,
      "loss": 0.0668,
      "step": 124
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.4195910692214966,
      "learning_rate": 0.00017745454545454548,
      "loss": 0.0941,
      "step": 125
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.3575162887573242,
      "learning_rate": 0.00017727272727272728,
      "loss": 0.0411,
      "step": 126
    },
    {
      "epoch": 0.11545454545454545,
      "grad_norm": 1.2457098960876465,
      "learning_rate": 0.0001770909090909091,
      "loss": 0.0895,
      "step": 127
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.5257028937339783,
      "learning_rate": 0.0001769090909090909,
      "loss": 0.0524,
      "step": 128
    },
    {
      "epoch": 0.11727272727272728,
      "grad_norm": 0.47162601351737976,
      "learning_rate": 0.00017672727272727274,
      "loss": 0.0548,
      "step": 129
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.6216538548469543,
      "learning_rate": 0.00017654545454545454,
      "loss": 0.0858,
      "step": 130
    },
    {
      "epoch": 0.1190909090909091,
      "grad_norm": 0.3741771876811981,
      "learning_rate": 0.00017636363636363637,
      "loss": 0.0534,
      "step": 131
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9085583686828613,
      "learning_rate": 0.0001761818181818182,
      "loss": 0.0816,
      "step": 132
    },
    {
      "epoch": 0.12090909090909091,
      "grad_norm": 0.6373740434646606,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0864,
      "step": 133
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.4097341299057007,
      "learning_rate": 0.00017581818181818182,
      "loss": 0.0321,
      "step": 134
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.526960015296936,
      "learning_rate": 0.00017563636363636365,
      "loss": 0.0545,
      "step": 135
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.6773700714111328,
      "learning_rate": 0.00017545454545454548,
      "loss": 0.101,
      "step": 136
    },
    {
      "epoch": 0.12454545454545454,
      "grad_norm": 0.6933131814002991,
      "learning_rate": 0.00017527272727272728,
      "loss": 0.0745,
      "step": 137
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 1.6824766397476196,
      "learning_rate": 0.0001750909090909091,
      "loss": 0.0918,
      "step": 138
    },
    {
      "epoch": 0.12636363636363637,
      "grad_norm": 0.5269562602043152,
      "learning_rate": 0.0001749090909090909,
      "loss": 0.0492,
      "step": 139
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 1.2305870056152344,
      "learning_rate": 0.00017472727272727274,
      "loss": 0.2499,
      "step": 140
    },
    {
      "epoch": 0.12818181818181817,
      "grad_norm": 0.4778180420398712,
      "learning_rate": 0.00017454545454545454,
      "loss": 0.0775,
      "step": 141
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.5714892148971558,
      "learning_rate": 0.00017436363636363637,
      "loss": 0.0791,
      "step": 142
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.479902982711792,
      "learning_rate": 0.00017418181818181817,
      "loss": 0.0973,
      "step": 143
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.3410756289958954,
      "learning_rate": 0.000174,
      "loss": 0.084,
      "step": 144
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.4154212772846222,
      "learning_rate": 0.00017381818181818183,
      "loss": 0.0797,
      "step": 145
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.301802396774292,
      "learning_rate": 0.00017363636363636363,
      "loss": 0.0828,
      "step": 146
    },
    {
      "epoch": 0.13363636363636364,
      "grad_norm": 0.28796127438545227,
      "learning_rate": 0.00017345454545454546,
      "loss": 0.0477,
      "step": 147
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.33684250712394714,
      "learning_rate": 0.0001732727272727273,
      "loss": 0.0416,
      "step": 148
    },
    {
      "epoch": 0.13545454545454547,
      "grad_norm": 0.2741227149963379,
      "learning_rate": 0.00017309090909090912,
      "loss": 0.0455,
      "step": 149
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.36194121837615967,
      "learning_rate": 0.00017290909090909092,
      "loss": 0.1072,
      "step": 150
    },
    {
      "epoch": 0.13727272727272727,
      "grad_norm": 0.4145103693008423,
      "learning_rate": 0.00017272727272727275,
      "loss": 0.088,
      "step": 151
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.2760033905506134,
      "learning_rate": 0.00017254545454545455,
      "loss": 0.07,
      "step": 152
    },
    {
      "epoch": 0.1390909090909091,
      "grad_norm": 0.6060256361961365,
      "learning_rate": 0.00017236363636363638,
      "loss": 0.0845,
      "step": 153
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.37726476788520813,
      "learning_rate": 0.0001721818181818182,
      "loss": 0.0725,
      "step": 154
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.7992291450500488,
      "learning_rate": 0.000172,
      "loss": 0.0693,
      "step": 155
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.28957056999206543,
      "learning_rate": 0.00017181818181818184,
      "loss": 0.0401,
      "step": 156
    },
    {
      "epoch": 0.14272727272727273,
      "grad_norm": 0.49206358194351196,
      "learning_rate": 0.00017163636363636364,
      "loss": 0.0616,
      "step": 157
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.34178856015205383,
      "learning_rate": 0.00017145454545454547,
      "loss": 0.0629,
      "step": 158
    },
    {
      "epoch": 0.14454545454545453,
      "grad_norm": 0.3810032308101654,
      "learning_rate": 0.00017127272727272727,
      "loss": 0.0952,
      "step": 159
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.40222328901290894,
      "learning_rate": 0.0001710909090909091,
      "loss": 0.0631,
      "step": 160
    },
    {
      "epoch": 0.14636363636363636,
      "grad_norm": 0.2680341303348541,
      "learning_rate": 0.0001709090909090909,
      "loss": 0.0592,
      "step": 161
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.6202617883682251,
      "learning_rate": 0.00017072727272727273,
      "loss": 0.0798,
      "step": 162
    },
    {
      "epoch": 0.1481818181818182,
      "grad_norm": 0.44885116815567017,
      "learning_rate": 0.00017054545454545455,
      "loss": 0.0592,
      "step": 163
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.5136794447898865,
      "learning_rate": 0.00017036363636363638,
      "loss": 0.0756,
      "step": 164
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.5321502089500427,
      "learning_rate": 0.0001701818181818182,
      "loss": 0.0712,
      "step": 165
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.47125715017318726,
      "learning_rate": 0.00017,
      "loss": 0.0638,
      "step": 166
    },
    {
      "epoch": 0.15181818181818182,
      "grad_norm": 0.7143350839614868,
      "learning_rate": 0.00016981818181818184,
      "loss": 0.0877,
      "step": 167
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.4361687898635864,
      "learning_rate": 0.00016963636363636364,
      "loss": 0.0544,
      "step": 168
    },
    {
      "epoch": 0.15363636363636363,
      "grad_norm": 1.066275715827942,
      "learning_rate": 0.00016945454545454547,
      "loss": 0.0826,
      "step": 169
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.5841320157051086,
      "learning_rate": 0.00016927272727272727,
      "loss": 0.0719,
      "step": 170
    },
    {
      "epoch": 0.15545454545454546,
      "grad_norm": 1.122616171836853,
      "learning_rate": 0.0001690909090909091,
      "loss": 0.2054,
      "step": 171
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.32751601934432983,
      "learning_rate": 0.00016890909090909093,
      "loss": 0.0627,
      "step": 172
    },
    {
      "epoch": 0.1572727272727273,
      "grad_norm": 0.27208754420280457,
      "learning_rate": 0.00016872727272727273,
      "loss": 0.0559,
      "step": 173
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.37237298488616943,
      "learning_rate": 0.00016854545454545456,
      "loss": 0.0386,
      "step": 174
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.34511059522628784,
      "learning_rate": 0.00016836363636363636,
      "loss": 0.0589,
      "step": 175
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.4910053610801697,
      "learning_rate": 0.0001681818181818182,
      "loss": 0.0645,
      "step": 176
    },
    {
      "epoch": 0.16090909090909092,
      "grad_norm": 0.4837384819984436,
      "learning_rate": 0.000168,
      "loss": 0.0635,
      "step": 177
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.5366002917289734,
      "learning_rate": 0.00016781818181818182,
      "loss": 0.0651,
      "step": 178
    },
    {
      "epoch": 0.16272727272727272,
      "grad_norm": 0.3176431655883789,
      "learning_rate": 0.00016763636363636365,
      "loss": 0.0491,
      "step": 179
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.33452340960502625,
      "learning_rate": 0.00016745454545454548,
      "loss": 0.0357,
      "step": 180
    },
    {
      "epoch": 0.16454545454545455,
      "grad_norm": 0.4510149359703064,
      "learning_rate": 0.00016727272727272728,
      "loss": 0.0587,
      "step": 181
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.5284162759780884,
      "learning_rate": 0.0001670909090909091,
      "loss": 0.058,
      "step": 182
    },
    {
      "epoch": 0.16636363636363635,
      "grad_norm": 0.5512385964393616,
      "learning_rate": 0.00016690909090909093,
      "loss": 0.0651,
      "step": 183
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.5641694664955139,
      "learning_rate": 0.00016672727272727274,
      "loss": 0.061,
      "step": 184
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.6112959980964661,
      "learning_rate": 0.00016654545454545456,
      "loss": 0.0715,
      "step": 185
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.3413119316101074,
      "learning_rate": 0.00016636363636363637,
      "loss": 0.0669,
      "step": 186
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.4940033555030823,
      "learning_rate": 0.0001661818181818182,
      "loss": 0.0579,
      "step": 187
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.3718279302120209,
      "learning_rate": 0.000166,
      "loss": 0.0619,
      "step": 188
    },
    {
      "epoch": 0.17181818181818181,
      "grad_norm": 0.3103405237197876,
      "learning_rate": 0.00016581818181818182,
      "loss": 0.0601,
      "step": 189
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.5264508724212646,
      "learning_rate": 0.00016563636363636363,
      "loss": 0.069,
      "step": 190
    },
    {
      "epoch": 0.17363636363636364,
      "grad_norm": 0.5928992629051208,
      "learning_rate": 0.00016545454545454545,
      "loss": 0.0597,
      "step": 191
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.5013671517372131,
      "learning_rate": 0.00016527272727272728,
      "loss": 0.0655,
      "step": 192
    },
    {
      "epoch": 0.17545454545454545,
      "grad_norm": 0.5260915160179138,
      "learning_rate": 0.00016509090909090908,
      "loss": 0.0556,
      "step": 193
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.8590536713600159,
      "learning_rate": 0.0001649090909090909,
      "loss": 0.0579,
      "step": 194
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.7763898968696594,
      "learning_rate": 0.00016472727272727274,
      "loss": 0.1705,
      "step": 195
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.49014782905578613,
      "learning_rate": 0.00016454545454545457,
      "loss": 0.0545,
      "step": 196
    },
    {
      "epoch": 0.17909090909090908,
      "grad_norm": 1.134045958518982,
      "learning_rate": 0.00016436363636363637,
      "loss": 0.1,
      "step": 197
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.3864533305168152,
      "learning_rate": 0.0001641818181818182,
      "loss": 0.0729,
      "step": 198
    },
    {
      "epoch": 0.1809090909090909,
      "grad_norm": 0.47251808643341064,
      "learning_rate": 0.000164,
      "loss": 0.1011,
      "step": 199
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.2928670048713684,
      "learning_rate": 0.00016381818181818183,
      "loss": 0.0707,
      "step": 200
    },
    {
      "epoch": 0.18272727272727274,
      "grad_norm": 0.32182908058166504,
      "learning_rate": 0.00016363636363636366,
      "loss": 0.0645,
      "step": 201
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.2868672311306,
      "learning_rate": 0.00016345454545454546,
      "loss": 0.0638,
      "step": 202
    },
    {
      "epoch": 0.18454545454545454,
      "grad_norm": 0.30634209513664246,
      "learning_rate": 0.0001632727272727273,
      "loss": 0.0539,
      "step": 203
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.23804010450839996,
      "learning_rate": 0.0001630909090909091,
      "loss": 0.0433,
      "step": 204
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.5254029631614685,
      "learning_rate": 0.00016290909090909092,
      "loss": 0.0728,
      "step": 205
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.31728261709213257,
      "learning_rate": 0.00016272727272727272,
      "loss": 0.0677,
      "step": 206
    },
    {
      "epoch": 0.18818181818181817,
      "grad_norm": 0.3902622163295746,
      "learning_rate": 0.00016254545454545455,
      "loss": 0.0645,
      "step": 207
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.3304527997970581,
      "learning_rate": 0.00016236363636363635,
      "loss": 0.071,
      "step": 208
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.3221791386604309,
      "learning_rate": 0.00016218181818181818,
      "loss": 0.0812,
      "step": 209
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.3171650767326355,
      "learning_rate": 0.000162,
      "loss": 0.0439,
      "step": 210
    },
    {
      "epoch": 0.1918181818181818,
      "grad_norm": 0.36366862058639526,
      "learning_rate": 0.00016181818181818184,
      "loss": 0.0568,
      "step": 211
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.5135633945465088,
      "learning_rate": 0.00016163636363636366,
      "loss": 0.1564,
      "step": 212
    },
    {
      "epoch": 0.19363636363636363,
      "grad_norm": 0.3538890779018402,
      "learning_rate": 0.00016145454545454547,
      "loss": 0.0609,
      "step": 213
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.30900272727012634,
      "learning_rate": 0.0001612727272727273,
      "loss": 0.0591,
      "step": 214
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.5542465448379517,
      "learning_rate": 0.0001610909090909091,
      "loss": 0.0821,
      "step": 215
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.5326674580574036,
      "learning_rate": 0.00016090909090909092,
      "loss": 0.0538,
      "step": 216
    },
    {
      "epoch": 0.19727272727272727,
      "grad_norm": 0.38854193687438965,
      "learning_rate": 0.00016072727272727273,
      "loss": 0.0571,
      "step": 217
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.6171688437461853,
      "learning_rate": 0.00016054545454545455,
      "loss": 0.0735,
      "step": 218
    },
    {
      "epoch": 0.1990909090909091,
      "grad_norm": 0.38648495078086853,
      "learning_rate": 0.00016036363636363636,
      "loss": 0.0575,
      "step": 219
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2904341518878937,
      "learning_rate": 0.00016018181818181818,
      "loss": 0.0543,
      "step": 220
    },
    {
      "epoch": 0.2009090909090909,
      "grad_norm": 0.9121249914169312,
      "learning_rate": 0.00016,
      "loss": 0.0855,
      "step": 221
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.4189826548099518,
      "learning_rate": 0.00015981818181818181,
      "loss": 0.0406,
      "step": 222
    },
    {
      "epoch": 0.20272727272727273,
      "grad_norm": 0.9988802671432495,
      "learning_rate": 0.00015963636363636364,
      "loss": 0.076,
      "step": 223
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.24384698271751404,
      "learning_rate": 0.00015945454545454544,
      "loss": 0.0394,
      "step": 224
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.31544461846351624,
      "learning_rate": 0.00015927272727272727,
      "loss": 0.0376,
      "step": 225
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.2656472325325012,
      "learning_rate": 0.0001590909090909091,
      "loss": 0.037,
      "step": 226
    },
    {
      "epoch": 0.20636363636363636,
      "grad_norm": 0.38777923583984375,
      "learning_rate": 0.00015890909090909093,
      "loss": 0.0691,
      "step": 227
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.38484683632850647,
      "learning_rate": 0.00015872727272727273,
      "loss": 0.0728,
      "step": 228
    },
    {
      "epoch": 0.2081818181818182,
      "grad_norm": 0.24986888468265533,
      "learning_rate": 0.00015854545454545456,
      "loss": 0.0636,
      "step": 229
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.27716463804244995,
      "learning_rate": 0.0001583636363636364,
      "loss": 0.0361,
      "step": 230
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.20006966590881348,
      "learning_rate": 0.0001581818181818182,
      "loss": 0.0524,
      "step": 231
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.19200970232486725,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.0521,
      "step": 232
    },
    {
      "epoch": 0.21181818181818182,
      "grad_norm": 0.35533496737480164,
      "learning_rate": 0.00015781818181818182,
      "loss": 0.0402,
      "step": 233
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.32355543971061707,
      "learning_rate": 0.00015763636363636365,
      "loss": 0.0688,
      "step": 234
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 0.2987849712371826,
      "learning_rate": 0.00015745454545454545,
      "loss": 0.0533,
      "step": 235
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.5060073137283325,
      "learning_rate": 0.00015727272727272728,
      "loss": 0.09,
      "step": 236
    },
    {
      "epoch": 0.21545454545454545,
      "grad_norm": 0.3186173737049103,
      "learning_rate": 0.00015709090909090908,
      "loss": 0.0699,
      "step": 237
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.2837028503417969,
      "learning_rate": 0.0001569090909090909,
      "loss": 0.0614,
      "step": 238
    },
    {
      "epoch": 0.21727272727272728,
      "grad_norm": 0.25656455755233765,
      "learning_rate": 0.00015672727272727274,
      "loss": 0.0574,
      "step": 239
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.21518629789352417,
      "learning_rate": 0.00015654545454545454,
      "loss": 0.0553,
      "step": 240
    },
    {
      "epoch": 0.2190909090909091,
      "grad_norm": 0.3251064121723175,
      "learning_rate": 0.00015636363636363637,
      "loss": 0.0536,
      "step": 241
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.7501956224441528,
      "learning_rate": 0.0001561818181818182,
      "loss": 0.1271,
      "step": 242
    },
    {
      "epoch": 0.22090909090909092,
      "grad_norm": 0.4309731125831604,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0617,
      "step": 243
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.36694276332855225,
      "learning_rate": 0.00015581818181818183,
      "loss": 0.0628,
      "step": 244
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.667202353477478,
      "learning_rate": 0.00015563636363636365,
      "loss": 0.0612,
      "step": 245
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.4676710069179535,
      "learning_rate": 0.00015545454545454546,
      "loss": 0.0681,
      "step": 246
    },
    {
      "epoch": 0.22454545454545455,
      "grad_norm": 0.370150089263916,
      "learning_rate": 0.00015527272727272728,
      "loss": 0.037,
      "step": 247
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.36096635460853577,
      "learning_rate": 0.0001550909090909091,
      "loss": 0.0532,
      "step": 248
    },
    {
      "epoch": 0.22636363636363635,
      "grad_norm": 0.3217812180519104,
      "learning_rate": 0.00015490909090909091,
      "loss": 0.054,
      "step": 249
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.003282904624939,
      "learning_rate": 0.00015472727272727274,
      "loss": 0.0792,
      "step": 250
    },
    {
      "epoch": 0.22818181818181818,
      "grad_norm": 0.31878796219825745,
      "learning_rate": 0.00015454545454545454,
      "loss": 0.0524,
      "step": 251
    },
    {
      "epoch": 0.2290909090909091,
      "grad_norm": 1.2523918151855469,
      "learning_rate": 0.00015436363636363637,
      "loss": 0.1417,
      "step": 252
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.5895376205444336,
      "learning_rate": 0.00015418181818181817,
      "loss": 0.0669,
      "step": 253
    },
    {
      "epoch": 0.2309090909090909,
      "grad_norm": 0.5155538320541382,
      "learning_rate": 0.000154,
      "loss": 0.0423,
      "step": 254
    },
    {
      "epoch": 0.2318181818181818,
      "grad_norm": 0.362943172454834,
      "learning_rate": 0.0001538181818181818,
      "loss": 0.0321,
      "step": 255
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.7565818428993225,
      "learning_rate": 0.00015363636363636363,
      "loss": 0.0632,
      "step": 256
    },
    {
      "epoch": 0.23363636363636364,
      "grad_norm": 0.23409514129161835,
      "learning_rate": 0.00015345454545454546,
      "loss": 0.0458,
      "step": 257
    },
    {
      "epoch": 0.23454545454545456,
      "grad_norm": 0.39415600895881653,
      "learning_rate": 0.0001532727272727273,
      "loss": 0.0626,
      "step": 258
    },
    {
      "epoch": 0.23545454545454544,
      "grad_norm": 0.3454281985759735,
      "learning_rate": 0.00015309090909090912,
      "loss": 0.0553,
      "step": 259
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.4396125078201294,
      "learning_rate": 0.00015290909090909092,
      "loss": 0.0552,
      "step": 260
    },
    {
      "epoch": 0.23727272727272727,
      "grad_norm": 0.8332716226577759,
      "learning_rate": 0.00015272727272727275,
      "loss": 0.0902,
      "step": 261
    },
    {
      "epoch": 0.2381818181818182,
      "grad_norm": 0.2526211440563202,
      "learning_rate": 0.00015254545454545455,
      "loss": 0.0613,
      "step": 262
    },
    {
      "epoch": 0.2390909090909091,
      "grad_norm": 0.21954607963562012,
      "learning_rate": 0.00015236363636363638,
      "loss": 0.0512,
      "step": 263
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5366191864013672,
      "learning_rate": 0.00015218181818181818,
      "loss": 0.0805,
      "step": 264
    },
    {
      "epoch": 0.2409090909090909,
      "grad_norm": 0.4288274347782135,
      "learning_rate": 0.000152,
      "loss": 0.0644,
      "step": 265
    },
    {
      "epoch": 0.24181818181818182,
      "grad_norm": 0.411034494638443,
      "learning_rate": 0.0001518181818181818,
      "loss": 0.0757,
      "step": 266
    },
    {
      "epoch": 0.24272727272727274,
      "grad_norm": 0.7253739237785339,
      "learning_rate": 0.00015163636363636364,
      "loss": 0.0768,
      "step": 267
    },
    {
      "epoch": 0.24363636363636362,
      "grad_norm": 0.3263986110687256,
      "learning_rate": 0.00015145454545454547,
      "loss": 0.0612,
      "step": 268
    },
    {
      "epoch": 0.24454545454545454,
      "grad_norm": 0.32040727138519287,
      "learning_rate": 0.00015127272727272727,
      "loss": 0.0644,
      "step": 269
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.4026724398136139,
      "learning_rate": 0.0001510909090909091,
      "loss": 0.0706,
      "step": 270
    },
    {
      "epoch": 0.24636363636363637,
      "grad_norm": 0.3680315315723419,
      "learning_rate": 0.0001509090909090909,
      "loss": 0.0371,
      "step": 271
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.5714128017425537,
      "learning_rate": 0.00015072727272727273,
      "loss": 0.0702,
      "step": 272
    },
    {
      "epoch": 0.24818181818181817,
      "grad_norm": 0.3420800566673279,
      "learning_rate": 0.00015054545454545456,
      "loss": 0.0365,
      "step": 273
    },
    {
      "epoch": 0.24909090909090909,
      "grad_norm": 0.324450820684433,
      "learning_rate": 0.00015036363636363638,
      "loss": 0.0374,
      "step": 274
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.3791443109512329,
      "learning_rate": 0.00015018181818181819,
      "loss": 0.0582,
      "step": 275
    },
    {
      "epoch": 0.2509090909090909,
      "grad_norm": 0.8275885581970215,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.1111,
      "step": 276
    },
    {
      "epoch": 0.25181818181818183,
      "grad_norm": 0.5430127382278442,
      "learning_rate": 0.00014981818181818184,
      "loss": 0.0616,
      "step": 277
    },
    {
      "epoch": 0.25272727272727274,
      "grad_norm": 0.49130359292030334,
      "learning_rate": 0.00014963636363636364,
      "loss": 0.0597,
      "step": 278
    },
    {
      "epoch": 0.25363636363636366,
      "grad_norm": 0.38569533824920654,
      "learning_rate": 0.00014945454545454547,
      "loss": 0.1004,
      "step": 279
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.5788567662239075,
      "learning_rate": 0.00014927272727272727,
      "loss": 0.0564,
      "step": 280
    },
    {
      "epoch": 0.25545454545454543,
      "grad_norm": 0.36395788192749023,
      "learning_rate": 0.0001490909090909091,
      "loss": 0.0504,
      "step": 281
    },
    {
      "epoch": 0.25636363636363635,
      "grad_norm": 0.7720467448234558,
      "learning_rate": 0.0001489090909090909,
      "loss": 0.0611,
      "step": 282
    },
    {
      "epoch": 0.25727272727272726,
      "grad_norm": 0.2992991805076599,
      "learning_rate": 0.00014872727272727273,
      "loss": 0.0556,
      "step": 283
    },
    {
      "epoch": 0.2581818181818182,
      "grad_norm": 0.8498635292053223,
      "learning_rate": 0.00014854545454545453,
      "loss": 0.0646,
      "step": 284
    },
    {
      "epoch": 0.2590909090909091,
      "grad_norm": 0.44984978437423706,
      "learning_rate": 0.00014836363636363636,
      "loss": 0.0581,
      "step": 285
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.6472774744033813,
      "learning_rate": 0.0001481818181818182,
      "loss": 0.0642,
      "step": 286
    },
    {
      "epoch": 0.2609090909090909,
      "grad_norm": 0.35423243045806885,
      "learning_rate": 0.000148,
      "loss": 0.0633,
      "step": 287
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.9802057147026062,
      "learning_rate": 0.00014781818181818182,
      "loss": 0.0992,
      "step": 288
    },
    {
      "epoch": 0.26272727272727275,
      "grad_norm": 0.4295702576637268,
      "learning_rate": 0.00014763636363636365,
      "loss": 0.0231,
      "step": 289
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.478525310754776,
      "learning_rate": 0.00014745454545454548,
      "loss": 0.0664,
      "step": 290
    },
    {
      "epoch": 0.26454545454545453,
      "grad_norm": 0.6046764254570007,
      "learning_rate": 0.00014727272727272728,
      "loss": 0.059,
      "step": 291
    },
    {
      "epoch": 0.26545454545454544,
      "grad_norm": 0.2504611015319824,
      "learning_rate": 0.0001470909090909091,
      "loss": 0.0553,
      "step": 292
    },
    {
      "epoch": 0.26636363636363636,
      "grad_norm": 0.38243281841278076,
      "learning_rate": 0.0001469090909090909,
      "loss": 0.0597,
      "step": 293
    },
    {
      "epoch": 0.2672727272727273,
      "grad_norm": 0.47746536135673523,
      "learning_rate": 0.00014672727272727274,
      "loss": 0.0643,
      "step": 294
    },
    {
      "epoch": 0.2681818181818182,
      "grad_norm": 0.2534700036048889,
      "learning_rate": 0.00014654545454545457,
      "loss": 0.0543,
      "step": 295
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.36276915669441223,
      "learning_rate": 0.00014636363636363637,
      "loss": 0.0592,
      "step": 296
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.21277742087841034,
      "learning_rate": 0.0001461818181818182,
      "loss": 0.0478,
      "step": 297
    },
    {
      "epoch": 0.27090909090909093,
      "grad_norm": 0.326185017824173,
      "learning_rate": 0.000146,
      "loss": 0.0361,
      "step": 298
    },
    {
      "epoch": 0.2718181818181818,
      "grad_norm": 0.4173944890499115,
      "learning_rate": 0.00014581818181818183,
      "loss": 0.0467,
      "step": 299
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.3159448206424713,
      "learning_rate": 0.00014563636363636363,
      "loss": 0.0632,
      "step": 300
    },
    {
      "epoch": 0.2736363636363636,
      "grad_norm": 0.4428044557571411,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.0631,
      "step": 301
    },
    {
      "epoch": 0.27454545454545454,
      "grad_norm": 1.0232837200164795,
      "learning_rate": 0.00014527272727272726,
      "loss": 0.0746,
      "step": 302
    },
    {
      "epoch": 0.27545454545454545,
      "grad_norm": 0.3626788854598999,
      "learning_rate": 0.0001450909090909091,
      "loss": 0.0629,
      "step": 303
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.28564703464508057,
      "learning_rate": 0.00014490909090909092,
      "loss": 0.054,
      "step": 304
    },
    {
      "epoch": 0.2772727272727273,
      "grad_norm": 0.3720630705356598,
      "learning_rate": 0.00014472727272727274,
      "loss": 0.0633,
      "step": 305
    },
    {
      "epoch": 0.2781818181818182,
      "grad_norm": 0.3413792848587036,
      "learning_rate": 0.00014454545454545457,
      "loss": 0.0587,
      "step": 306
    },
    {
      "epoch": 0.2790909090909091,
      "grad_norm": 0.2425125241279602,
      "learning_rate": 0.00014436363636363637,
      "loss": 0.0541,
      "step": 307
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.3470965325832367,
      "learning_rate": 0.0001441818181818182,
      "loss": 0.054,
      "step": 308
    },
    {
      "epoch": 0.2809090909090909,
      "grad_norm": 0.7136321663856506,
      "learning_rate": 0.000144,
      "loss": 0.0633,
      "step": 309
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.4237690269947052,
      "learning_rate": 0.00014381818181818183,
      "loss": 0.0611,
      "step": 310
    },
    {
      "epoch": 0.2827272727272727,
      "grad_norm": 0.29500633478164673,
      "learning_rate": 0.00014363636363636363,
      "loss": 0.0362,
      "step": 311
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.34068381786346436,
      "learning_rate": 0.00014345454545454546,
      "loss": 0.0588,
      "step": 312
    },
    {
      "epoch": 0.28454545454545455,
      "grad_norm": 0.390487939119339,
      "learning_rate": 0.00014327272727272726,
      "loss": 0.0622,
      "step": 313
    },
    {
      "epoch": 0.28545454545454546,
      "grad_norm": 0.32867369055747986,
      "learning_rate": 0.0001430909090909091,
      "loss": 0.0547,
      "step": 314
    },
    {
      "epoch": 0.2863636363636364,
      "grad_norm": 0.16123417019844055,
      "learning_rate": 0.00014290909090909092,
      "loss": 0.0462,
      "step": 315
    },
    {
      "epoch": 0.2872727272727273,
      "grad_norm": 0.3896438777446747,
      "learning_rate": 0.00014272727272727272,
      "loss": 0.0524,
      "step": 316
    },
    {
      "epoch": 0.2881818181818182,
      "grad_norm": 0.3266948163509369,
      "learning_rate": 0.00014254545454545455,
      "loss": 0.0505,
      "step": 317
    },
    {
      "epoch": 0.28909090909090907,
      "grad_norm": 0.42548662424087524,
      "learning_rate": 0.00014236363636363635,
      "loss": 0.0557,
      "step": 318
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.9930770993232727,
      "learning_rate": 0.00014218181818181818,
      "loss": 0.0693,
      "step": 319
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.591011106967926,
      "learning_rate": 0.000142,
      "loss": 0.0672,
      "step": 320
    },
    {
      "epoch": 0.2918181818181818,
      "grad_norm": 0.3899790942668915,
      "learning_rate": 0.00014181818181818184,
      "loss": 0.0368,
      "step": 321
    },
    {
      "epoch": 0.2927272727272727,
      "grad_norm": 0.5281303524971008,
      "learning_rate": 0.00014163636363636364,
      "loss": 0.0602,
      "step": 322
    },
    {
      "epoch": 0.29363636363636364,
      "grad_norm": 0.7160094976425171,
      "learning_rate": 0.00014145454545454547,
      "loss": 0.1151,
      "step": 323
    },
    {
      "epoch": 0.29454545454545455,
      "grad_norm": 2.35193133354187,
      "learning_rate": 0.0001412727272727273,
      "loss": 0.0611,
      "step": 324
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.2589096426963806,
      "learning_rate": 0.0001410909090909091,
      "loss": 0.0516,
      "step": 325
    },
    {
      "epoch": 0.2963636363636364,
      "grad_norm": 0.2849162817001343,
      "learning_rate": 0.00014090909090909093,
      "loss": 0.0554,
      "step": 326
    },
    {
      "epoch": 0.2972727272727273,
      "grad_norm": 0.35161280632019043,
      "learning_rate": 0.00014072727272727273,
      "loss": 0.0613,
      "step": 327
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.2744251489639282,
      "learning_rate": 0.00014054545454545456,
      "loss": 0.0326,
      "step": 328
    },
    {
      "epoch": 0.2990909090909091,
      "grad_norm": 0.23284465074539185,
      "learning_rate": 0.00014036363636363636,
      "loss": 0.0351,
      "step": 329
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.4614412784576416,
      "learning_rate": 0.00014018181818181819,
      "loss": 0.0906,
      "step": 330
    },
    {
      "epoch": 0.3009090909090909,
      "grad_norm": 0.2323506623506546,
      "learning_rate": 0.00014,
      "loss": 0.0517,
      "step": 331
    },
    {
      "epoch": 0.3018181818181818,
      "grad_norm": 0.29957860708236694,
      "learning_rate": 0.00013981818181818182,
      "loss": 0.0631,
      "step": 332
    },
    {
      "epoch": 0.30272727272727273,
      "grad_norm": 0.2597053349018097,
      "learning_rate": 0.00013963636363636364,
      "loss": 0.0305,
      "step": 333
    },
    {
      "epoch": 0.30363636363636365,
      "grad_norm": 0.26088130474090576,
      "learning_rate": 0.00013945454545454547,
      "loss": 0.0618,
      "step": 334
    },
    {
      "epoch": 0.30454545454545456,
      "grad_norm": 0.22032298147678375,
      "learning_rate": 0.00013927272727272727,
      "loss": 0.0281,
      "step": 335
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.2698574364185333,
      "learning_rate": 0.0001390909090909091,
      "loss": 0.0613,
      "step": 336
    },
    {
      "epoch": 0.30636363636363634,
      "grad_norm": 0.2555588185787201,
      "learning_rate": 0.00013890909090909093,
      "loss": 0.0285,
      "step": 337
    },
    {
      "epoch": 0.30727272727272725,
      "grad_norm": 0.31534555554389954,
      "learning_rate": 0.00013872727272727273,
      "loss": 0.0647,
      "step": 338
    },
    {
      "epoch": 0.30818181818181817,
      "grad_norm": 0.3008165657520294,
      "learning_rate": 0.00013854545454545456,
      "loss": 0.0612,
      "step": 339
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.3910299241542816,
      "learning_rate": 0.00013836363636363636,
      "loss": 0.0734,
      "step": 340
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.3355688452720642,
      "learning_rate": 0.0001381818181818182,
      "loss": 0.0659,
      "step": 341
    },
    {
      "epoch": 0.3109090909090909,
      "grad_norm": 0.4719529151916504,
      "learning_rate": 0.000138,
      "loss": 0.0366,
      "step": 342
    },
    {
      "epoch": 0.3118181818181818,
      "grad_norm": 0.5611101388931274,
      "learning_rate": 0.00013781818181818182,
      "loss": 0.063,
      "step": 343
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.5206930041313171,
      "learning_rate": 0.00013763636363636365,
      "loss": 0.0684,
      "step": 344
    },
    {
      "epoch": 0.31363636363636366,
      "grad_norm": 0.49372151494026184,
      "learning_rate": 0.00013745454545454545,
      "loss": 0.0255,
      "step": 345
    },
    {
      "epoch": 0.3145454545454546,
      "grad_norm": 0.2593480050563812,
      "learning_rate": 0.00013727272727272728,
      "loss": 0.0611,
      "step": 346
    },
    {
      "epoch": 0.31545454545454543,
      "grad_norm": 0.2827622592449188,
      "learning_rate": 0.00013709090909090908,
      "loss": 0.027,
      "step": 347
    },
    {
      "epoch": 0.31636363636363635,
      "grad_norm": 0.25574079155921936,
      "learning_rate": 0.0001369090909090909,
      "loss": 0.0543,
      "step": 348
    },
    {
      "epoch": 0.31727272727272726,
      "grad_norm": 0.18482570350170135,
      "learning_rate": 0.00013672727272727274,
      "loss": 0.025,
      "step": 349
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.6622452139854431,
      "learning_rate": 0.00013654545454545457,
      "loss": 0.0676,
      "step": 350
    },
    {
      "epoch": 0.3190909090909091,
      "grad_norm": 0.28356245160102844,
      "learning_rate": 0.00013636363636363637,
      "loss": 0.0539,
      "step": 351
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.30557161569595337,
      "learning_rate": 0.0001361818181818182,
      "loss": 0.0622,
      "step": 352
    },
    {
      "epoch": 0.3209090909090909,
      "grad_norm": 0.5155278444290161,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.0365,
      "step": 353
    },
    {
      "epoch": 0.32181818181818184,
      "grad_norm": 0.36040395498275757,
      "learning_rate": 0.00013581818181818183,
      "loss": 0.0572,
      "step": 354
    },
    {
      "epoch": 0.32272727272727275,
      "grad_norm": 0.28687548637390137,
      "learning_rate": 0.00013563636363636366,
      "loss": 0.0307,
      "step": 355
    },
    {
      "epoch": 0.3236363636363636,
      "grad_norm": 0.2725818455219269,
      "learning_rate": 0.00013545454545454546,
      "loss": 0.0265,
      "step": 356
    },
    {
      "epoch": 0.3245454545454545,
      "grad_norm": 0.4249287247657776,
      "learning_rate": 0.00013527272727272729,
      "loss": 0.0701,
      "step": 357
    },
    {
      "epoch": 0.32545454545454544,
      "grad_norm": 0.3546792268753052,
      "learning_rate": 0.0001350909090909091,
      "loss": 0.0631,
      "step": 358
    },
    {
      "epoch": 0.32636363636363636,
      "grad_norm": 0.42497730255126953,
      "learning_rate": 0.00013490909090909092,
      "loss": 0.0651,
      "step": 359
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.5472553372383118,
      "learning_rate": 0.00013472727272727272,
      "loss": 0.0664,
      "step": 360
    },
    {
      "epoch": 0.3281818181818182,
      "grad_norm": 0.2693046033382416,
      "learning_rate": 0.00013454545454545455,
      "loss": 0.0567,
      "step": 361
    },
    {
      "epoch": 0.3290909090909091,
      "grad_norm": 0.2582634687423706,
      "learning_rate": 0.00013436363636363637,
      "loss": 0.0527,
      "step": 362
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.4905495345592499,
      "learning_rate": 0.00013418181818181818,
      "loss": 0.0675,
      "step": 363
    },
    {
      "epoch": 0.33090909090909093,
      "grad_norm": 0.7708890438079834,
      "learning_rate": 0.000134,
      "loss": 0.0885,
      "step": 364
    },
    {
      "epoch": 0.33181818181818185,
      "grad_norm": 0.41693297028541565,
      "learning_rate": 0.00013381818181818183,
      "loss": 0.0651,
      "step": 365
    },
    {
      "epoch": 0.3327272727272727,
      "grad_norm": 0.30240678787231445,
      "learning_rate": 0.00013363636363636366,
      "loss": 0.029,
      "step": 366
    },
    {
      "epoch": 0.3336363636363636,
      "grad_norm": 0.24862992763519287,
      "learning_rate": 0.00013345454545454546,
      "loss": 0.0509,
      "step": 367
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 1.469576358795166,
      "learning_rate": 0.0001332727272727273,
      "loss": 0.0625,
      "step": 368
    },
    {
      "epoch": 0.33545454545454545,
      "grad_norm": 0.2018454372882843,
      "learning_rate": 0.0001330909090909091,
      "loss": 0.0263,
      "step": 369
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.18855734169483185,
      "learning_rate": 0.00013290909090909092,
      "loss": 0.0519,
      "step": 370
    },
    {
      "epoch": 0.3372727272727273,
      "grad_norm": 0.22574466466903687,
      "learning_rate": 0.00013272727272727275,
      "loss": 0.0556,
      "step": 371
    },
    {
      "epoch": 0.3381818181818182,
      "grad_norm": 1.1331071853637695,
      "learning_rate": 0.00013254545454545455,
      "loss": 0.0727,
      "step": 372
    },
    {
      "epoch": 0.3390909090909091,
      "grad_norm": 0.26539671421051025,
      "learning_rate": 0.00013236363636363638,
      "loss": 0.0559,
      "step": 373
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.32153990864753723,
      "learning_rate": 0.00013218181818181818,
      "loss": 0.0603,
      "step": 374
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.367838978767395,
      "learning_rate": 0.000132,
      "loss": 0.0553,
      "step": 375
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 0.2657327651977539,
      "learning_rate": 0.0001318181818181818,
      "loss": 0.0505,
      "step": 376
    },
    {
      "epoch": 0.3427272727272727,
      "grad_norm": 0.3247745931148529,
      "learning_rate": 0.00013163636363636364,
      "loss": 0.0644,
      "step": 377
    },
    {
      "epoch": 0.34363636363636363,
      "grad_norm": 0.27165257930755615,
      "learning_rate": 0.00013145454545454544,
      "loss": 0.0549,
      "step": 378
    },
    {
      "epoch": 0.34454545454545454,
      "grad_norm": 0.22945518791675568,
      "learning_rate": 0.00013127272727272727,
      "loss": 0.026,
      "step": 379
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.1963033378124237,
      "learning_rate": 0.0001310909090909091,
      "loss": 0.0244,
      "step": 380
    },
    {
      "epoch": 0.3463636363636364,
      "grad_norm": 0.19104614853858948,
      "learning_rate": 0.00013090909090909093,
      "loss": 0.0472,
      "step": 381
    },
    {
      "epoch": 0.3472727272727273,
      "grad_norm": 0.20110571384429932,
      "learning_rate": 0.00013072727272727276,
      "loss": 0.0496,
      "step": 382
    },
    {
      "epoch": 0.3481818181818182,
      "grad_norm": 0.30689695477485657,
      "learning_rate": 0.00013054545454545456,
      "loss": 0.054,
      "step": 383
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.4878307282924652,
      "learning_rate": 0.00013036363636363639,
      "loss": 0.065,
      "step": 384
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.3614552915096283,
      "learning_rate": 0.0001301818181818182,
      "loss": 0.0516,
      "step": 385
    },
    {
      "epoch": 0.3509090909090909,
      "grad_norm": 0.46248936653137207,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.0706,
      "step": 386
    },
    {
      "epoch": 0.3518181818181818,
      "grad_norm": 0.4122622311115265,
      "learning_rate": 0.00012981818181818182,
      "loss": 0.0536,
      "step": 387
    },
    {
      "epoch": 0.3527272727272727,
      "grad_norm": 0.3548372685909271,
      "learning_rate": 0.00012963636363636365,
      "loss": 0.0592,
      "step": 388
    },
    {
      "epoch": 0.35363636363636364,
      "grad_norm": 0.44997847080230713,
      "learning_rate": 0.00012945454545454545,
      "loss": 0.0685,
      "step": 389
    },
    {
      "epoch": 0.35454545454545455,
      "grad_norm": 0.6316152215003967,
      "learning_rate": 0.00012927272727272728,
      "loss": 0.0818,
      "step": 390
    },
    {
      "epoch": 0.35545454545454547,
      "grad_norm": 0.40339505672454834,
      "learning_rate": 0.0001290909090909091,
      "loss": 0.0591,
      "step": 391
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 0.25368157029151917,
      "learning_rate": 0.0001289090909090909,
      "loss": 0.0553,
      "step": 392
    },
    {
      "epoch": 0.3572727272727273,
      "grad_norm": 0.5349712371826172,
      "learning_rate": 0.00012872727272727273,
      "loss": 0.0739,
      "step": 393
    },
    {
      "epoch": 0.35818181818181816,
      "grad_norm": 0.6988741159439087,
      "learning_rate": 0.00012854545454545454,
      "loss": 0.0698,
      "step": 394
    },
    {
      "epoch": 0.35909090909090907,
      "grad_norm": 0.3270755410194397,
      "learning_rate": 0.00012836363636363636,
      "loss": 0.065,
      "step": 395
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.24317795038223267,
      "learning_rate": 0.0001281818181818182,
      "loss": 0.0319,
      "step": 396
    },
    {
      "epoch": 0.3609090909090909,
      "grad_norm": 0.5461304783821106,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.0516,
      "step": 397
    },
    {
      "epoch": 0.3618181818181818,
      "grad_norm": 0.31694385409355164,
      "learning_rate": 0.00012781818181818182,
      "loss": 0.0567,
      "step": 398
    },
    {
      "epoch": 0.36272727272727273,
      "grad_norm": 0.4410610795021057,
      "learning_rate": 0.00012763636363636365,
      "loss": 0.0532,
      "step": 399
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.27854281663894653,
      "learning_rate": 0.00012745454545454548,
      "loss": 0.0546,
      "step": 400
    },
    {
      "epoch": 0.36454545454545456,
      "grad_norm": 0.3846808075904846,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.0377,
      "step": 401
    },
    {
      "epoch": 0.3654545454545455,
      "grad_norm": 0.3058278560638428,
      "learning_rate": 0.0001270909090909091,
      "loss": 0.0561,
      "step": 402
    },
    {
      "epoch": 0.3663636363636364,
      "grad_norm": 0.2762816548347473,
      "learning_rate": 0.0001269090909090909,
      "loss": 0.0593,
      "step": 403
    },
    {
      "epoch": 0.36727272727272725,
      "grad_norm": 0.27248355746269226,
      "learning_rate": 0.00012672727272727274,
      "loss": 0.0527,
      "step": 404
    },
    {
      "epoch": 0.36818181818181817,
      "grad_norm": 0.2731420397758484,
      "learning_rate": 0.00012654545454545454,
      "loss": 0.0497,
      "step": 405
    },
    {
      "epoch": 0.3690909090909091,
      "grad_norm": 0.8762916326522827,
      "learning_rate": 0.00012636363636363637,
      "loss": 0.0598,
      "step": 406
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.2220631241798401,
      "learning_rate": 0.00012618181818181817,
      "loss": 0.0488,
      "step": 407
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.5820425152778625,
      "learning_rate": 0.000126,
      "loss": 0.0706,
      "step": 408
    },
    {
      "epoch": 0.3718181818181818,
      "grad_norm": 0.18932843208312988,
      "learning_rate": 0.00012581818181818183,
      "loss": 0.0491,
      "step": 409
    },
    {
      "epoch": 0.37272727272727274,
      "grad_norm": 0.36126768589019775,
      "learning_rate": 0.00012563636363636363,
      "loss": 0.0566,
      "step": 410
    },
    {
      "epoch": 0.37363636363636366,
      "grad_norm": 0.3173307776451111,
      "learning_rate": 0.00012545454545454546,
      "loss": 0.0559,
      "step": 411
    },
    {
      "epoch": 0.37454545454545457,
      "grad_norm": 0.2856764793395996,
      "learning_rate": 0.0001252727272727273,
      "loss": 0.0302,
      "step": 412
    },
    {
      "epoch": 0.37545454545454543,
      "grad_norm": 0.20929314196109772,
      "learning_rate": 0.00012509090909090912,
      "loss": 0.024,
      "step": 413
    },
    {
      "epoch": 0.37636363636363634,
      "grad_norm": 0.3080812394618988,
      "learning_rate": 0.00012490909090909092,
      "loss": 0.0513,
      "step": 414
    },
    {
      "epoch": 0.37727272727272726,
      "grad_norm": 0.3197500705718994,
      "learning_rate": 0.00012472727272727275,
      "loss": 0.0574,
      "step": 415
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 0.3359318673610687,
      "learning_rate": 0.00012454545454545455,
      "loss": 0.0611,
      "step": 416
    },
    {
      "epoch": 0.3790909090909091,
      "grad_norm": 0.2077966183423996,
      "learning_rate": 0.00012436363636363638,
      "loss": 0.0493,
      "step": 417
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.9626249074935913,
      "learning_rate": 0.00012418181818181818,
      "loss": 0.0648,
      "step": 418
    },
    {
      "epoch": 0.3809090909090909,
      "grad_norm": 0.1909794956445694,
      "learning_rate": 0.000124,
      "loss": 0.0506,
      "step": 419
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.419283926486969,
      "learning_rate": 0.00012381818181818183,
      "loss": 0.0587,
      "step": 420
    },
    {
      "epoch": 0.38272727272727275,
      "grad_norm": 0.3696501553058624,
      "learning_rate": 0.00012363636363636364,
      "loss": 0.0566,
      "step": 421
    },
    {
      "epoch": 0.3836363636363636,
      "grad_norm": 0.4293128550052643,
      "learning_rate": 0.00012345454545454546,
      "loss": 0.0606,
      "step": 422
    },
    {
      "epoch": 0.3845454545454545,
      "grad_norm": 0.3174975514411926,
      "learning_rate": 0.00012327272727272727,
      "loss": 0.054,
      "step": 423
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.15423813462257385,
      "learning_rate": 0.0001230909090909091,
      "loss": 0.0494,
      "step": 424
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.7049425840377808,
      "learning_rate": 0.0001229090909090909,
      "loss": 0.0673,
      "step": 425
    },
    {
      "epoch": 0.38727272727272727,
      "grad_norm": 0.3653467297554016,
      "learning_rate": 0.00012272727272727272,
      "loss": 0.0625,
      "step": 426
    },
    {
      "epoch": 0.3881818181818182,
      "grad_norm": 0.36775901913642883,
      "learning_rate": 0.00012254545454545455,
      "loss": 0.0653,
      "step": 427
    },
    {
      "epoch": 0.3890909090909091,
      "grad_norm": 0.2128051221370697,
      "learning_rate": 0.00012236363636363638,
      "loss": 0.0232,
      "step": 428
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.2659347355365753,
      "learning_rate": 0.0001221818181818182,
      "loss": 0.0604,
      "step": 429
    },
    {
      "epoch": 0.39090909090909093,
      "grad_norm": 0.7692505121231079,
      "learning_rate": 0.000122,
      "loss": 0.0659,
      "step": 430
    },
    {
      "epoch": 0.39181818181818184,
      "grad_norm": 0.23321229219436646,
      "learning_rate": 0.00012181818181818183,
      "loss": 0.0575,
      "step": 431
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.2529878318309784,
      "learning_rate": 0.00012163636363636364,
      "loss": 0.0609,
      "step": 432
    },
    {
      "epoch": 0.3936363636363636,
      "grad_norm": 0.3087160885334015,
      "learning_rate": 0.00012145454545454547,
      "loss": 0.0605,
      "step": 433
    },
    {
      "epoch": 0.39454545454545453,
      "grad_norm": 0.24036739766597748,
      "learning_rate": 0.00012127272727272727,
      "loss": 0.0494,
      "step": 434
    },
    {
      "epoch": 0.39545454545454545,
      "grad_norm": 0.3179612159729004,
      "learning_rate": 0.0001210909090909091,
      "loss": 0.0562,
      "step": 435
    },
    {
      "epoch": 0.39636363636363636,
      "grad_norm": 0.3302544355392456,
      "learning_rate": 0.0001209090909090909,
      "loss": 0.0577,
      "step": 436
    },
    {
      "epoch": 0.3972727272727273,
      "grad_norm": 0.5449963808059692,
      "learning_rate": 0.00012072727272727273,
      "loss": 0.051,
      "step": 437
    },
    {
      "epoch": 0.3981818181818182,
      "grad_norm": 0.31894201040267944,
      "learning_rate": 0.00012054545454545456,
      "loss": 0.0554,
      "step": 438
    },
    {
      "epoch": 0.3990909090909091,
      "grad_norm": 0.4858469069004059,
      "learning_rate": 0.00012036363636363637,
      "loss": 0.0542,
      "step": 439
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2753209173679352,
      "learning_rate": 0.0001201818181818182,
      "loss": 0.0557,
      "step": 440
    },
    {
      "epoch": 0.4009090909090909,
      "grad_norm": 0.2481006234884262,
      "learning_rate": 0.00012,
      "loss": 0.0501,
      "step": 441
    },
    {
      "epoch": 0.4018181818181818,
      "grad_norm": 0.42723211646080017,
      "learning_rate": 0.00011981818181818183,
      "loss": 0.064,
      "step": 442
    },
    {
      "epoch": 0.4027272727272727,
      "grad_norm": 0.28928765654563904,
      "learning_rate": 0.00011963636363636363,
      "loss": 0.0333,
      "step": 443
    },
    {
      "epoch": 0.4036363636363636,
      "grad_norm": 0.5320556163787842,
      "learning_rate": 0.00011945454545454546,
      "loss": 0.0606,
      "step": 444
    },
    {
      "epoch": 0.40454545454545454,
      "grad_norm": 0.19839932024478912,
      "learning_rate": 0.00011927272727272726,
      "loss": 0.0478,
      "step": 445
    },
    {
      "epoch": 0.40545454545454546,
      "grad_norm": 0.28675422072410583,
      "learning_rate": 0.00011909090909090909,
      "loss": 0.057,
      "step": 446
    },
    {
      "epoch": 0.40636363636363637,
      "grad_norm": 0.2628745138645172,
      "learning_rate": 0.00011890909090909092,
      "loss": 0.0498,
      "step": 447
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.32006412744522095,
      "learning_rate": 0.00011872727272727274,
      "loss": 0.0538,
      "step": 448
    },
    {
      "epoch": 0.4081818181818182,
      "grad_norm": 0.3207314610481262,
      "learning_rate": 0.00011854545454545456,
      "loss": 0.0597,
      "step": 449
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.43756407499313354,
      "learning_rate": 0.00011836363636363637,
      "loss": 0.0462,
      "step": 450
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.45412197709083557,
      "learning_rate": 0.0001181818181818182,
      "loss": 0.0605,
      "step": 451
    },
    {
      "epoch": 0.4109090909090909,
      "grad_norm": 0.40128597617149353,
      "learning_rate": 0.000118,
      "loss": 0.0369,
      "step": 452
    },
    {
      "epoch": 0.4118181818181818,
      "grad_norm": 0.35860830545425415,
      "learning_rate": 0.00011781818181818182,
      "loss": 0.0584,
      "step": 453
    },
    {
      "epoch": 0.4127272727272727,
      "grad_norm": 0.5303643941879272,
      "learning_rate": 0.00011763636363636364,
      "loss": 0.0637,
      "step": 454
    },
    {
      "epoch": 0.41363636363636364,
      "grad_norm": 0.32328000664711,
      "learning_rate": 0.00011745454545454547,
      "loss": 0.0397,
      "step": 455
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 0.3799273669719696,
      "learning_rate": 0.00011727272727272727,
      "loss": 0.0594,
      "step": 456
    },
    {
      "epoch": 0.41545454545454547,
      "grad_norm": 0.3432691693305969,
      "learning_rate": 0.0001170909090909091,
      "loss": 0.0518,
      "step": 457
    },
    {
      "epoch": 0.4163636363636364,
      "grad_norm": 0.39309075474739075,
      "learning_rate": 0.00011690909090909093,
      "loss": 0.0563,
      "step": 458
    },
    {
      "epoch": 0.4172727272727273,
      "grad_norm": 0.6364865899085999,
      "learning_rate": 0.00011672727272727273,
      "loss": 0.0461,
      "step": 459
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.4076211154460907,
      "learning_rate": 0.00011654545454545456,
      "loss": 0.0576,
      "step": 460
    },
    {
      "epoch": 0.41909090909090907,
      "grad_norm": 0.21667179465293884,
      "learning_rate": 0.00011636363636363636,
      "loss": 0.0486,
      "step": 461
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.19847649335861206,
      "learning_rate": 0.00011618181818181819,
      "loss": 0.0254,
      "step": 462
    },
    {
      "epoch": 0.4209090909090909,
      "grad_norm": 0.24339571595191956,
      "learning_rate": 0.000116,
      "loss": 0.0546,
      "step": 463
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.4462047815322876,
      "learning_rate": 0.00011581818181818183,
      "loss": 0.0623,
      "step": 464
    },
    {
      "epoch": 0.42272727272727273,
      "grad_norm": 0.3588307797908783,
      "learning_rate": 0.00011563636363636363,
      "loss": 0.0559,
      "step": 465
    },
    {
      "epoch": 0.42363636363636364,
      "grad_norm": 0.2207401841878891,
      "learning_rate": 0.00011545454545454546,
      "loss": 0.0307,
      "step": 466
    },
    {
      "epoch": 0.42454545454545456,
      "grad_norm": 0.20566317439079285,
      "learning_rate": 0.00011527272727272729,
      "loss": 0.0507,
      "step": 467
    },
    {
      "epoch": 0.4254545454545455,
      "grad_norm": 0.32436105608940125,
      "learning_rate": 0.00011509090909090909,
      "loss": 0.0575,
      "step": 468
    },
    {
      "epoch": 0.4263636363636364,
      "grad_norm": 0.2315075844526291,
      "learning_rate": 0.00011490909090909092,
      "loss": 0.0563,
      "step": 469
    },
    {
      "epoch": 0.42727272727272725,
      "grad_norm": 0.32775425910949707,
      "learning_rate": 0.00011472727272727273,
      "loss": 0.0351,
      "step": 470
    },
    {
      "epoch": 0.42818181818181816,
      "grad_norm": 0.2501753568649292,
      "learning_rate": 0.00011454545454545456,
      "loss": 0.0542,
      "step": 471
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.24422655999660492,
      "learning_rate": 0.00011436363636363636,
      "loss": 0.0589,
      "step": 472
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.3677924573421478,
      "learning_rate": 0.00011418181818181819,
      "loss": 0.073,
      "step": 473
    },
    {
      "epoch": 0.4309090909090909,
      "grad_norm": 0.1973247081041336,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.0285,
      "step": 474
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.28572189807891846,
      "learning_rate": 0.00011381818181818182,
      "loss": 0.0512,
      "step": 475
    },
    {
      "epoch": 0.43272727272727274,
      "grad_norm": 0.20683862268924713,
      "learning_rate": 0.00011363636363636365,
      "loss": 0.0547,
      "step": 476
    },
    {
      "epoch": 0.43363636363636365,
      "grad_norm": 0.3235137164592743,
      "learning_rate": 0.00011345454545454545,
      "loss": 0.0664,
      "step": 477
    },
    {
      "epoch": 0.43454545454545457,
      "grad_norm": 0.19530446827411652,
      "learning_rate": 0.00011327272727272728,
      "loss": 0.0505,
      "step": 478
    },
    {
      "epoch": 0.4354545454545454,
      "grad_norm": 0.43417641520500183,
      "learning_rate": 0.0001130909090909091,
      "loss": 0.0661,
      "step": 479
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.22940130531787872,
      "learning_rate": 0.00011290909090909092,
      "loss": 0.0482,
      "step": 480
    },
    {
      "epoch": 0.43727272727272726,
      "grad_norm": 0.21476376056671143,
      "learning_rate": 0.00011272727272727272,
      "loss": 0.0502,
      "step": 481
    },
    {
      "epoch": 0.4381818181818182,
      "grad_norm": 0.29843124747276306,
      "learning_rate": 0.00011254545454545455,
      "loss": 0.0629,
      "step": 482
    },
    {
      "epoch": 0.4390909090909091,
      "grad_norm": 0.32513347268104553,
      "learning_rate": 0.00011236363636363635,
      "loss": 0.0561,
      "step": 483
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5387204885482788,
      "learning_rate": 0.00011218181818181818,
      "loss": 0.0735,
      "step": 484
    },
    {
      "epoch": 0.4409090909090909,
      "grad_norm": 0.3741561472415924,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.0525,
      "step": 485
    },
    {
      "epoch": 0.44181818181818183,
      "grad_norm": 0.1921900510787964,
      "learning_rate": 0.00011181818181818183,
      "loss": 0.0249,
      "step": 486
    },
    {
      "epoch": 0.44272727272727275,
      "grad_norm": 0.40658530592918396,
      "learning_rate": 0.00011163636363636366,
      "loss": 0.0645,
      "step": 487
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.270207017660141,
      "learning_rate": 0.00011145454545454546,
      "loss": 0.03,
      "step": 488
    },
    {
      "epoch": 0.4445454545454545,
      "grad_norm": 0.5816388726234436,
      "learning_rate": 0.00011127272727272729,
      "loss": 0.0329,
      "step": 489
    },
    {
      "epoch": 0.44545454545454544,
      "grad_norm": 0.31535592675209045,
      "learning_rate": 0.00011109090909090909,
      "loss": 0.0605,
      "step": 490
    },
    {
      "epoch": 0.44636363636363635,
      "grad_norm": 0.2709725797176361,
      "learning_rate": 0.00011090909090909092,
      "loss": 0.0593,
      "step": 491
    },
    {
      "epoch": 0.44727272727272727,
      "grad_norm": 0.3843725621700287,
      "learning_rate": 0.00011072727272727273,
      "loss": 0.0639,
      "step": 492
    },
    {
      "epoch": 0.4481818181818182,
      "grad_norm": 0.2121669054031372,
      "learning_rate": 0.00011054545454545455,
      "loss": 0.0677,
      "step": 493
    },
    {
      "epoch": 0.4490909090909091,
      "grad_norm": 0.23169821500778198,
      "learning_rate": 0.00011036363636363636,
      "loss": 0.0265,
      "step": 494
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.24839231371879578,
      "learning_rate": 0.00011018181818181819,
      "loss": 0.0532,
      "step": 495
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 0.3437248766422272,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.0555,
      "step": 496
    },
    {
      "epoch": 0.45181818181818184,
      "grad_norm": 0.17999367415905,
      "learning_rate": 0.00010981818181818182,
      "loss": 0.0505,
      "step": 497
    },
    {
      "epoch": 0.4527272727272727,
      "grad_norm": 0.3637174963951111,
      "learning_rate": 0.00010963636363636365,
      "loss": 0.0642,
      "step": 498
    },
    {
      "epoch": 0.4536363636363636,
      "grad_norm": 0.33274614810943604,
      "learning_rate": 0.00010945454545454545,
      "loss": 0.0593,
      "step": 499
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.2053908407688141,
      "learning_rate": 0.00010927272727272728,
      "loss": 0.055,
      "step": 500
    },
    {
      "epoch": 0.45545454545454545,
      "grad_norm": 0.22375769913196564,
      "learning_rate": 0.00010909090909090909,
      "loss": 0.051,
      "step": 501
    },
    {
      "epoch": 0.45636363636363636,
      "grad_norm": 0.39631375670433044,
      "learning_rate": 0.00010890909090909092,
      "loss": 0.0554,
      "step": 502
    },
    {
      "epoch": 0.4572727272727273,
      "grad_norm": 0.25629112124443054,
      "learning_rate": 0.00010872727272727272,
      "loss": 0.03,
      "step": 503
    },
    {
      "epoch": 0.4581818181818182,
      "grad_norm": 0.27033543586730957,
      "learning_rate": 0.00010854545454545455,
      "loss": 0.0519,
      "step": 504
    },
    {
      "epoch": 0.4590909090909091,
      "grad_norm": 0.1614951193332672,
      "learning_rate": 0.00010836363636363638,
      "loss": 0.0486,
      "step": 505
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.6497014164924622,
      "learning_rate": 0.00010818181818181818,
      "loss": 0.0584,
      "step": 506
    },
    {
      "epoch": 0.46090909090909093,
      "grad_norm": 0.24217981100082397,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.0293,
      "step": 507
    },
    {
      "epoch": 0.4618181818181818,
      "grad_norm": 0.17951662838459015,
      "learning_rate": 0.00010781818181818182,
      "loss": 0.0506,
      "step": 508
    },
    {
      "epoch": 0.4627272727272727,
      "grad_norm": 0.42849549651145935,
      "learning_rate": 0.00010763636363636365,
      "loss": 0.0624,
      "step": 509
    },
    {
      "epoch": 0.4636363636363636,
      "grad_norm": 0.23206761479377747,
      "learning_rate": 0.00010745454545454545,
      "loss": 0.0527,
      "step": 510
    },
    {
      "epoch": 0.46454545454545454,
      "grad_norm": 0.1990525871515274,
      "learning_rate": 0.00010727272727272728,
      "loss": 0.0522,
      "step": 511
    },
    {
      "epoch": 0.46545454545454545,
      "grad_norm": 0.18369486927986145,
      "learning_rate": 0.00010709090909090908,
      "loss": 0.0494,
      "step": 512
    },
    {
      "epoch": 0.46636363636363637,
      "grad_norm": 0.477679044008255,
      "learning_rate": 0.00010690909090909091,
      "loss": 0.0601,
      "step": 513
    },
    {
      "epoch": 0.4672727272727273,
      "grad_norm": 0.4091612696647644,
      "learning_rate": 0.00010672727272727274,
      "loss": 0.0508,
      "step": 514
    },
    {
      "epoch": 0.4681818181818182,
      "grad_norm": 0.6028718948364258,
      "learning_rate": 0.00010654545454545454,
      "loss": 0.0552,
      "step": 515
    },
    {
      "epoch": 0.4690909090909091,
      "grad_norm": 0.3996671140193939,
      "learning_rate": 0.00010636363636363637,
      "loss": 0.0531,
      "step": 516
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.9130843281745911,
      "learning_rate": 0.00010618181818181819,
      "loss": 0.0747,
      "step": 517
    },
    {
      "epoch": 0.4709090909090909,
      "grad_norm": 0.24107146263122559,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.0481,
      "step": 518
    },
    {
      "epoch": 0.4718181818181818,
      "grad_norm": 0.21851450204849243,
      "learning_rate": 0.00010581818181818182,
      "loss": 0.0248,
      "step": 519
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.6280899047851562,
      "learning_rate": 0.00010563636363636365,
      "loss": 0.0677,
      "step": 520
    },
    {
      "epoch": 0.47363636363636363,
      "grad_norm": 0.5525327920913696,
      "learning_rate": 0.00010545454545454545,
      "loss": 0.0779,
      "step": 521
    },
    {
      "epoch": 0.47454545454545455,
      "grad_norm": 0.5060248374938965,
      "learning_rate": 0.00010527272727272728,
      "loss": 0.0694,
      "step": 522
    },
    {
      "epoch": 0.47545454545454546,
      "grad_norm": 0.17773807048797607,
      "learning_rate": 0.0001050909090909091,
      "loss": 0.0512,
      "step": 523
    },
    {
      "epoch": 0.4763636363636364,
      "grad_norm": 0.3432261049747467,
      "learning_rate": 0.00010490909090909092,
      "loss": 0.0635,
      "step": 524
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.1775764673948288,
      "learning_rate": 0.00010472727272727275,
      "loss": 0.048,
      "step": 525
    },
    {
      "epoch": 0.4781818181818182,
      "grad_norm": 0.3203364312648773,
      "learning_rate": 0.00010454545454545455,
      "loss": 0.0529,
      "step": 526
    },
    {
      "epoch": 0.47909090909090907,
      "grad_norm": 0.3629850447177887,
      "learning_rate": 0.00010436363636363638,
      "loss": 0.0623,
      "step": 527
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.40679553151130676,
      "learning_rate": 0.00010418181818181818,
      "loss": 0.0608,
      "step": 528
    },
    {
      "epoch": 0.4809090909090909,
      "grad_norm": 0.5572177767753601,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.0654,
      "step": 529
    },
    {
      "epoch": 0.4818181818181818,
      "grad_norm": 0.4046052396297455,
      "learning_rate": 0.00010381818181818181,
      "loss": 0.0591,
      "step": 530
    },
    {
      "epoch": 0.4827272727272727,
      "grad_norm": 0.2802076041698456,
      "learning_rate": 0.00010363636363636364,
      "loss": 0.0602,
      "step": 531
    },
    {
      "epoch": 0.48363636363636364,
      "grad_norm": 0.3007996678352356,
      "learning_rate": 0.00010345454545454545,
      "loss": 0.052,
      "step": 532
    },
    {
      "epoch": 0.48454545454545456,
      "grad_norm": 0.3221473693847656,
      "learning_rate": 0.00010327272727272728,
      "loss": 0.0574,
      "step": 533
    },
    {
      "epoch": 0.48545454545454547,
      "grad_norm": 0.2065458595752716,
      "learning_rate": 0.00010309090909090911,
      "loss": 0.052,
      "step": 534
    },
    {
      "epoch": 0.4863636363636364,
      "grad_norm": 0.2506801187992096,
      "learning_rate": 0.00010290909090909091,
      "loss": 0.0355,
      "step": 535
    },
    {
      "epoch": 0.48727272727272725,
      "grad_norm": 0.21791008114814758,
      "learning_rate": 0.00010272727272727274,
      "loss": 0.0524,
      "step": 536
    },
    {
      "epoch": 0.48818181818181816,
      "grad_norm": 0.20444165170192719,
      "learning_rate": 0.00010254545454545454,
      "loss": 0.0539,
      "step": 537
    },
    {
      "epoch": 0.4890909090909091,
      "grad_norm": 0.2986676096916199,
      "learning_rate": 0.00010236363636363637,
      "loss": 0.0613,
      "step": 538
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.20688915252685547,
      "learning_rate": 0.00010218181818181818,
      "loss": 0.0517,
      "step": 539
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.17583806812763214,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.0506,
      "step": 540
    },
    {
      "epoch": 0.4918181818181818,
      "grad_norm": 0.23491422832012177,
      "learning_rate": 0.00010181818181818181,
      "loss": 0.0534,
      "step": 541
    },
    {
      "epoch": 0.49272727272727274,
      "grad_norm": 0.3146779239177704,
      "learning_rate": 0.00010163636363636364,
      "loss": 0.0649,
      "step": 542
    },
    {
      "epoch": 0.49363636363636365,
      "grad_norm": 0.221796452999115,
      "learning_rate": 0.00010145454545454547,
      "loss": 0.0536,
      "step": 543
    },
    {
      "epoch": 0.49454545454545457,
      "grad_norm": 0.17671918869018555,
      "learning_rate": 0.00010127272727272727,
      "loss": 0.0481,
      "step": 544
    },
    {
      "epoch": 0.4954545454545455,
      "grad_norm": 0.35537031292915344,
      "learning_rate": 0.0001010909090909091,
      "loss": 0.0536,
      "step": 545
    },
    {
      "epoch": 0.49636363636363634,
      "grad_norm": 0.30638015270233154,
      "learning_rate": 0.0001009090909090909,
      "loss": 0.0311,
      "step": 546
    },
    {
      "epoch": 0.49727272727272726,
      "grad_norm": 0.2704637944698334,
      "learning_rate": 0.00010072727272727273,
      "loss": 0.0559,
      "step": 547
    },
    {
      "epoch": 0.49818181818181817,
      "grad_norm": 0.5492804050445557,
      "learning_rate": 0.00010054545454545455,
      "loss": 0.0659,
      "step": 548
    },
    {
      "epoch": 0.4990909090909091,
      "grad_norm": 0.39420944452285767,
      "learning_rate": 0.00010036363636363637,
      "loss": 0.0355,
      "step": 549
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.2734184265136719,
      "learning_rate": 0.00010018181818181818,
      "loss": 0.0552,
      "step": 550
    },
    {
      "epoch": 0.5009090909090909,
      "grad_norm": 0.33644789457321167,
      "learning_rate": 0.0001,
      "loss": 0.0645,
      "step": 551
    },
    {
      "epoch": 0.5018181818181818,
      "grad_norm": 0.37657105922698975,
      "learning_rate": 9.981818181818182e-05,
      "loss": 0.0259,
      "step": 552
    },
    {
      "epoch": 0.5027272727272727,
      "grad_norm": 0.2575015127658844,
      "learning_rate": 9.963636363636363e-05,
      "loss": 0.0551,
      "step": 553
    },
    {
      "epoch": 0.5036363636363637,
      "grad_norm": 0.2713155150413513,
      "learning_rate": 9.945454545454545e-05,
      "loss": 0.0512,
      "step": 554
    },
    {
      "epoch": 0.5045454545454545,
      "grad_norm": 0.21896064281463623,
      "learning_rate": 9.927272727272728e-05,
      "loss": 0.0547,
      "step": 555
    },
    {
      "epoch": 0.5054545454545455,
      "grad_norm": 0.5544456243515015,
      "learning_rate": 9.909090909090911e-05,
      "loss": 0.0599,
      "step": 556
    },
    {
      "epoch": 0.5063636363636363,
      "grad_norm": 0.17388445138931274,
      "learning_rate": 9.890909090909092e-05,
      "loss": 0.0511,
      "step": 557
    },
    {
      "epoch": 0.5072727272727273,
      "grad_norm": 0.21619458496570587,
      "learning_rate": 9.872727272727274e-05,
      "loss": 0.0527,
      "step": 558
    },
    {
      "epoch": 0.5081818181818182,
      "grad_norm": 0.1923649162054062,
      "learning_rate": 9.854545454545455e-05,
      "loss": 0.0234,
      "step": 559
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.2556163966655731,
      "learning_rate": 9.836363636363637e-05,
      "loss": 0.0611,
      "step": 560
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.20232699811458588,
      "learning_rate": 9.818181818181818e-05,
      "loss": 0.0471,
      "step": 561
    },
    {
      "epoch": 0.5109090909090909,
      "grad_norm": 0.23433466255664825,
      "learning_rate": 9.8e-05,
      "loss": 0.026,
      "step": 562
    },
    {
      "epoch": 0.5118181818181818,
      "grad_norm": 0.3913628160953522,
      "learning_rate": 9.781818181818183e-05,
      "loss": 0.0594,
      "step": 563
    },
    {
      "epoch": 0.5127272727272727,
      "grad_norm": 0.20349691808223724,
      "learning_rate": 9.763636363636364e-05,
      "loss": 0.0518,
      "step": 564
    },
    {
      "epoch": 0.5136363636363637,
      "grad_norm": 0.2813946008682251,
      "learning_rate": 9.745454545454546e-05,
      "loss": 0.0607,
      "step": 565
    },
    {
      "epoch": 0.5145454545454545,
      "grad_norm": 0.2199898660182953,
      "learning_rate": 9.727272727272728e-05,
      "loss": 0.0519,
      "step": 566
    },
    {
      "epoch": 0.5154545454545455,
      "grad_norm": 0.19827060401439667,
      "learning_rate": 9.70909090909091e-05,
      "loss": 0.0509,
      "step": 567
    },
    {
      "epoch": 0.5163636363636364,
      "grad_norm": 0.2195836305618286,
      "learning_rate": 9.690909090909091e-05,
      "loss": 0.0511,
      "step": 568
    },
    {
      "epoch": 0.5172727272727272,
      "grad_norm": 0.2675393223762512,
      "learning_rate": 9.672727272727273e-05,
      "loss": 0.0507,
      "step": 569
    },
    {
      "epoch": 0.5181818181818182,
      "grad_norm": 0.4491267800331116,
      "learning_rate": 9.654545454545454e-05,
      "loss": 0.0564,
      "step": 570
    },
    {
      "epoch": 0.519090909090909,
      "grad_norm": 0.29248979687690735,
      "learning_rate": 9.636363636363637e-05,
      "loss": 0.053,
      "step": 571
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.3670671582221985,
      "learning_rate": 9.618181818181819e-05,
      "loss": 0.0624,
      "step": 572
    },
    {
      "epoch": 0.5209090909090909,
      "grad_norm": 0.3837312161922455,
      "learning_rate": 9.6e-05,
      "loss": 0.0544,
      "step": 573
    },
    {
      "epoch": 0.5218181818181818,
      "grad_norm": 0.24663186073303223,
      "learning_rate": 9.581818181818182e-05,
      "loss": 0.0311,
      "step": 574
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.26039770245552063,
      "learning_rate": 9.563636363636365e-05,
      "loss": 0.0504,
      "step": 575
    },
    {
      "epoch": 0.5236363636363637,
      "grad_norm": 0.3055863082408905,
      "learning_rate": 9.545454545454546e-05,
      "loss": 0.0516,
      "step": 576
    },
    {
      "epoch": 0.5245454545454545,
      "grad_norm": 0.4289829134941101,
      "learning_rate": 9.527272727272728e-05,
      "loss": 0.0578,
      "step": 577
    },
    {
      "epoch": 0.5254545454545455,
      "grad_norm": 0.22532016038894653,
      "learning_rate": 9.509090909090909e-05,
      "loss": 0.0494,
      "step": 578
    },
    {
      "epoch": 0.5263636363636364,
      "grad_norm": 0.20613084733486176,
      "learning_rate": 9.490909090909092e-05,
      "loss": 0.0569,
      "step": 579
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.15582934021949768,
      "learning_rate": 9.472727272727273e-05,
      "loss": 0.0516,
      "step": 580
    },
    {
      "epoch": 0.5281818181818182,
      "grad_norm": 0.40954044461250305,
      "learning_rate": 9.454545454545455e-05,
      "loss": 0.0588,
      "step": 581
    },
    {
      "epoch": 0.5290909090909091,
      "grad_norm": 0.4165618121623993,
      "learning_rate": 9.436363636363636e-05,
      "loss": 0.0578,
      "step": 582
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.26187562942504883,
      "learning_rate": 9.418181818181818e-05,
      "loss": 0.0529,
      "step": 583
    },
    {
      "epoch": 0.5309090909090909,
      "grad_norm": 0.29175838828086853,
      "learning_rate": 9.4e-05,
      "loss": 0.0634,
      "step": 584
    },
    {
      "epoch": 0.5318181818181819,
      "grad_norm": 0.22212354838848114,
      "learning_rate": 9.381818181818182e-05,
      "loss": 0.0482,
      "step": 585
    },
    {
      "epoch": 0.5327272727272727,
      "grad_norm": 0.13813795149326324,
      "learning_rate": 9.363636363636364e-05,
      "loss": 0.0507,
      "step": 586
    },
    {
      "epoch": 0.5336363636363637,
      "grad_norm": 0.1382954865694046,
      "learning_rate": 9.345454545454547e-05,
      "loss": 0.0536,
      "step": 587
    },
    {
      "epoch": 0.5345454545454545,
      "grad_norm": 0.1439981758594513,
      "learning_rate": 9.327272727272728e-05,
      "loss": 0.0473,
      "step": 588
    },
    {
      "epoch": 0.5354545454545454,
      "grad_norm": 0.32316845655441284,
      "learning_rate": 9.30909090909091e-05,
      "loss": 0.0576,
      "step": 589
    },
    {
      "epoch": 0.5363636363636364,
      "grad_norm": 0.38007885217666626,
      "learning_rate": 9.290909090909091e-05,
      "loss": 0.0597,
      "step": 590
    },
    {
      "epoch": 0.5372727272727272,
      "grad_norm": 0.2592889666557312,
      "learning_rate": 9.272727272727273e-05,
      "loss": 0.0302,
      "step": 591
    },
    {
      "epoch": 0.5381818181818182,
      "grad_norm": 0.1694120615720749,
      "learning_rate": 9.254545454545454e-05,
      "loss": 0.0514,
      "step": 592
    },
    {
      "epoch": 0.5390909090909091,
      "grad_norm": 0.24470573663711548,
      "learning_rate": 9.236363636363636e-05,
      "loss": 0.0584,
      "step": 593
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.29835399985313416,
      "learning_rate": 9.218181818181819e-05,
      "loss": 0.0308,
      "step": 594
    },
    {
      "epoch": 0.5409090909090909,
      "grad_norm": 0.1911226511001587,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.056,
      "step": 595
    },
    {
      "epoch": 0.5418181818181819,
      "grad_norm": 0.20756886899471283,
      "learning_rate": 9.181818181818183e-05,
      "loss": 0.0279,
      "step": 596
    },
    {
      "epoch": 0.5427272727272727,
      "grad_norm": 0.2448321133852005,
      "learning_rate": 9.163636363636364e-05,
      "loss": 0.0544,
      "step": 597
    },
    {
      "epoch": 0.5436363636363636,
      "grad_norm": 0.35943782329559326,
      "learning_rate": 9.145454545454546e-05,
      "loss": 0.0596,
      "step": 598
    },
    {
      "epoch": 0.5445454545454546,
      "grad_norm": 0.21320244669914246,
      "learning_rate": 9.127272727272727e-05,
      "loss": 0.0301,
      "step": 599
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.15652425587177277,
      "learning_rate": 9.109090909090909e-05,
      "loss": 0.0501,
      "step": 600
    },
    {
      "epoch": 0.5463636363636364,
      "grad_norm": 0.18184709548950195,
      "learning_rate": 9.090909090909092e-05,
      "loss": 0.0506,
      "step": 601
    },
    {
      "epoch": 0.5472727272727272,
      "grad_norm": 0.26983019709587097,
      "learning_rate": 9.072727272727273e-05,
      "loss": 0.0573,
      "step": 602
    },
    {
      "epoch": 0.5481818181818182,
      "grad_norm": 0.20358288288116455,
      "learning_rate": 9.054545454545455e-05,
      "loss": 0.0507,
      "step": 603
    },
    {
      "epoch": 0.5490909090909091,
      "grad_norm": 0.3356699049472809,
      "learning_rate": 9.036363636363638e-05,
      "loss": 0.0537,
      "step": 604
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.24962574243545532,
      "learning_rate": 9.018181818181819e-05,
      "loss": 0.0276,
      "step": 605
    },
    {
      "epoch": 0.5509090909090909,
      "grad_norm": 0.29949861764907837,
      "learning_rate": 9e-05,
      "loss": 0.0532,
      "step": 606
    },
    {
      "epoch": 0.5518181818181818,
      "grad_norm": 0.24113792181015015,
      "learning_rate": 8.981818181818182e-05,
      "loss": 0.0262,
      "step": 607
    },
    {
      "epoch": 0.5527272727272727,
      "grad_norm": 0.261392742395401,
      "learning_rate": 8.963636363636364e-05,
      "loss": 0.0526,
      "step": 608
    },
    {
      "epoch": 0.5536363636363636,
      "grad_norm": 0.2229243516921997,
      "learning_rate": 8.945454545454546e-05,
      "loss": 0.025,
      "step": 609
    },
    {
      "epoch": 0.5545454545454546,
      "grad_norm": 0.28851523995399475,
      "learning_rate": 8.927272727272728e-05,
      "loss": 0.0591,
      "step": 610
    },
    {
      "epoch": 0.5554545454545454,
      "grad_norm": 0.1767849177122116,
      "learning_rate": 8.90909090909091e-05,
      "loss": 0.0559,
      "step": 611
    },
    {
      "epoch": 0.5563636363636364,
      "grad_norm": 0.19229300320148468,
      "learning_rate": 8.890909090909091e-05,
      "loss": 0.0238,
      "step": 612
    },
    {
      "epoch": 0.5572727272727273,
      "grad_norm": 0.17314012348651886,
      "learning_rate": 8.872727272727274e-05,
      "loss": 0.0203,
      "step": 613
    },
    {
      "epoch": 0.5581818181818182,
      "grad_norm": 0.24816490709781647,
      "learning_rate": 8.854545454545455e-05,
      "loss": 0.0561,
      "step": 614
    },
    {
      "epoch": 0.5590909090909091,
      "grad_norm": 0.24629783630371094,
      "learning_rate": 8.836363636363637e-05,
      "loss": 0.0552,
      "step": 615
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.375703364610672,
      "learning_rate": 8.818181818181818e-05,
      "loss": 0.0579,
      "step": 616
    },
    {
      "epoch": 0.5609090909090909,
      "grad_norm": 0.4345281422138214,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.0635,
      "step": 617
    },
    {
      "epoch": 0.5618181818181818,
      "grad_norm": 0.176976278424263,
      "learning_rate": 8.781818181818183e-05,
      "loss": 0.0222,
      "step": 618
    },
    {
      "epoch": 0.5627272727272727,
      "grad_norm": 0.38322192430496216,
      "learning_rate": 8.763636363636364e-05,
      "loss": 0.022,
      "step": 619
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.23432716727256775,
      "learning_rate": 8.745454545454546e-05,
      "loss": 0.054,
      "step": 620
    },
    {
      "epoch": 0.5645454545454546,
      "grad_norm": 1.0195285081863403,
      "learning_rate": 8.727272727272727e-05,
      "loss": 0.0665,
      "step": 621
    },
    {
      "epoch": 0.5654545454545454,
      "grad_norm": 0.28149253129959106,
      "learning_rate": 8.709090909090909e-05,
      "loss": 0.0547,
      "step": 622
    },
    {
      "epoch": 0.5663636363636364,
      "grad_norm": 0.2520698308944702,
      "learning_rate": 8.690909090909091e-05,
      "loss": 0.0573,
      "step": 623
    },
    {
      "epoch": 0.5672727272727273,
      "grad_norm": 0.3118336498737335,
      "learning_rate": 8.672727272727273e-05,
      "loss": 0.0597,
      "step": 624
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.31604868173599243,
      "learning_rate": 8.654545454545456e-05,
      "loss": 0.0606,
      "step": 625
    },
    {
      "epoch": 0.5690909090909091,
      "grad_norm": 0.1658223420381546,
      "learning_rate": 8.636363636363637e-05,
      "loss": 0.0501,
      "step": 626
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.2280496209859848,
      "learning_rate": 8.618181818181819e-05,
      "loss": 0.0537,
      "step": 627
    },
    {
      "epoch": 0.5709090909090909,
      "grad_norm": 0.23079736530780792,
      "learning_rate": 8.6e-05,
      "loss": 0.054,
      "step": 628
    },
    {
      "epoch": 0.5718181818181818,
      "grad_norm": 0.36986443400382996,
      "learning_rate": 8.581818181818182e-05,
      "loss": 0.0551,
      "step": 629
    },
    {
      "epoch": 0.5727272727272728,
      "grad_norm": 0.23646774888038635,
      "learning_rate": 8.563636363636363e-05,
      "loss": 0.0536,
      "step": 630
    },
    {
      "epoch": 0.5736363636363636,
      "grad_norm": 0.21335236728191376,
      "learning_rate": 8.545454545454545e-05,
      "loss": 0.0534,
      "step": 631
    },
    {
      "epoch": 0.5745454545454546,
      "grad_norm": 0.24113145470619202,
      "learning_rate": 8.527272727272728e-05,
      "loss": 0.0545,
      "step": 632
    },
    {
      "epoch": 0.5754545454545454,
      "grad_norm": 0.1956726461648941,
      "learning_rate": 8.50909090909091e-05,
      "loss": 0.0283,
      "step": 633
    },
    {
      "epoch": 0.5763636363636364,
      "grad_norm": 0.24163918197155,
      "learning_rate": 8.490909090909092e-05,
      "loss": 0.0566,
      "step": 634
    },
    {
      "epoch": 0.5772727272727273,
      "grad_norm": 0.2637614607810974,
      "learning_rate": 8.472727272727274e-05,
      "loss": 0.0626,
      "step": 635
    },
    {
      "epoch": 0.5781818181818181,
      "grad_norm": 0.21133796870708466,
      "learning_rate": 8.454545454545455e-05,
      "loss": 0.0545,
      "step": 636
    },
    {
      "epoch": 0.5790909090909091,
      "grad_norm": 0.21040740609169006,
      "learning_rate": 8.436363636363637e-05,
      "loss": 0.0241,
      "step": 637
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.2778123915195465,
      "learning_rate": 8.418181818181818e-05,
      "loss": 0.0622,
      "step": 638
    },
    {
      "epoch": 0.5809090909090909,
      "grad_norm": 0.2818715274333954,
      "learning_rate": 8.4e-05,
      "loss": 0.0571,
      "step": 639
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.25664034485816956,
      "learning_rate": 8.381818181818182e-05,
      "loss": 0.0545,
      "step": 640
    },
    {
      "epoch": 0.5827272727272728,
      "grad_norm": 0.22256138920783997,
      "learning_rate": 8.363636363636364e-05,
      "loss": 0.0527,
      "step": 641
    },
    {
      "epoch": 0.5836363636363636,
      "grad_norm": 0.18125148117542267,
      "learning_rate": 8.345454545454547e-05,
      "loss": 0.0535,
      "step": 642
    },
    {
      "epoch": 0.5845454545454546,
      "grad_norm": 0.21113070845603943,
      "learning_rate": 8.327272727272728e-05,
      "loss": 0.0276,
      "step": 643
    },
    {
      "epoch": 0.5854545454545454,
      "grad_norm": 0.20576825737953186,
      "learning_rate": 8.30909090909091e-05,
      "loss": 0.0318,
      "step": 644
    },
    {
      "epoch": 0.5863636363636363,
      "grad_norm": 0.21679536998271942,
      "learning_rate": 8.290909090909091e-05,
      "loss": 0.0239,
      "step": 645
    },
    {
      "epoch": 0.5872727272727273,
      "grad_norm": 0.3254431486129761,
      "learning_rate": 8.272727272727273e-05,
      "loss": 0.0572,
      "step": 646
    },
    {
      "epoch": 0.5881818181818181,
      "grad_norm": 0.1871674656867981,
      "learning_rate": 8.254545454545454e-05,
      "loss": 0.0229,
      "step": 647
    },
    {
      "epoch": 0.5890909090909091,
      "grad_norm": 0.34882429242134094,
      "learning_rate": 8.236363636363637e-05,
      "loss": 0.057,
      "step": 648
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.1583695113658905,
      "learning_rate": 8.218181818181819e-05,
      "loss": 0.0512,
      "step": 649
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.21729589998722076,
      "learning_rate": 8.2e-05,
      "loss": 0.0291,
      "step": 650
    },
    {
      "epoch": 0.5918181818181818,
      "grad_norm": 0.1793445497751236,
      "learning_rate": 8.181818181818183e-05,
      "loss": 0.0509,
      "step": 651
    },
    {
      "epoch": 0.5927272727272728,
      "grad_norm": 0.17288917303085327,
      "learning_rate": 8.163636363636364e-05,
      "loss": 0.054,
      "step": 652
    },
    {
      "epoch": 0.5936363636363636,
      "grad_norm": 0.2709241509437561,
      "learning_rate": 8.145454545454546e-05,
      "loss": 0.0585,
      "step": 653
    },
    {
      "epoch": 0.5945454545454546,
      "grad_norm": 0.2337709218263626,
      "learning_rate": 8.127272727272727e-05,
      "loss": 0.0501,
      "step": 654
    },
    {
      "epoch": 0.5954545454545455,
      "grad_norm": 0.3138410747051239,
      "learning_rate": 8.109090909090909e-05,
      "loss": 0.0598,
      "step": 655
    },
    {
      "epoch": 0.5963636363636363,
      "grad_norm": 0.18824517726898193,
      "learning_rate": 8.090909090909092e-05,
      "loss": 0.05,
      "step": 656
    },
    {
      "epoch": 0.5972727272727273,
      "grad_norm": 0.23923112452030182,
      "learning_rate": 8.072727272727273e-05,
      "loss": 0.0535,
      "step": 657
    },
    {
      "epoch": 0.5981818181818181,
      "grad_norm": 0.2897174060344696,
      "learning_rate": 8.054545454545455e-05,
      "loss": 0.0532,
      "step": 658
    },
    {
      "epoch": 0.5990909090909091,
      "grad_norm": 0.3253220319747925,
      "learning_rate": 8.036363636363636e-05,
      "loss": 0.0539,
      "step": 659
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.19064414501190186,
      "learning_rate": 8.018181818181818e-05,
      "loss": 0.021,
      "step": 660
    },
    {
      "epoch": 0.600909090909091,
      "grad_norm": 0.438553124666214,
      "learning_rate": 8e-05,
      "loss": 0.0633,
      "step": 661
    },
    {
      "epoch": 0.6018181818181818,
      "grad_norm": 0.3146952688694,
      "learning_rate": 7.981818181818182e-05,
      "loss": 0.0581,
      "step": 662
    },
    {
      "epoch": 0.6027272727272728,
      "grad_norm": 0.3736204504966736,
      "learning_rate": 7.963636363636364e-05,
      "loss": 0.0585,
      "step": 663
    },
    {
      "epoch": 0.6036363636363636,
      "grad_norm": 0.3156498372554779,
      "learning_rate": 7.945454545454547e-05,
      "loss": 0.0354,
      "step": 664
    },
    {
      "epoch": 0.6045454545454545,
      "grad_norm": 0.23896406590938568,
      "learning_rate": 7.927272727272728e-05,
      "loss": 0.0554,
      "step": 665
    },
    {
      "epoch": 0.6054545454545455,
      "grad_norm": 0.2295774519443512,
      "learning_rate": 7.90909090909091e-05,
      "loss": 0.0521,
      "step": 666
    },
    {
      "epoch": 0.6063636363636363,
      "grad_norm": 0.42662614583969116,
      "learning_rate": 7.890909090909091e-05,
      "loss": 0.0557,
      "step": 667
    },
    {
      "epoch": 0.6072727272727273,
      "grad_norm": 0.4793553650379181,
      "learning_rate": 7.872727272727273e-05,
      "loss": 0.0629,
      "step": 668
    },
    {
      "epoch": 0.6081818181818182,
      "grad_norm": 0.21643291413784027,
      "learning_rate": 7.854545454545454e-05,
      "loss": 0.0265,
      "step": 669
    },
    {
      "epoch": 0.6090909090909091,
      "grad_norm": 0.20526684820652008,
      "learning_rate": 7.836363636363637e-05,
      "loss": 0.049,
      "step": 670
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.31191369891166687,
      "learning_rate": 7.818181818181818e-05,
      "loss": 0.0305,
      "step": 671
    },
    {
      "epoch": 0.610909090909091,
      "grad_norm": 0.2613331973552704,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.0557,
      "step": 672
    },
    {
      "epoch": 0.6118181818181818,
      "grad_norm": 0.18971025943756104,
      "learning_rate": 7.781818181818183e-05,
      "loss": 0.0267,
      "step": 673
    },
    {
      "epoch": 0.6127272727272727,
      "grad_norm": 0.191529780626297,
      "learning_rate": 7.763636363636364e-05,
      "loss": 0.0215,
      "step": 674
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.32144638895988464,
      "learning_rate": 7.745454545454546e-05,
      "loss": 0.0571,
      "step": 675
    },
    {
      "epoch": 0.6145454545454545,
      "grad_norm": 0.4777231812477112,
      "learning_rate": 7.727272727272727e-05,
      "loss": 0.0563,
      "step": 676
    },
    {
      "epoch": 0.6154545454545455,
      "grad_norm": 0.15580782294273376,
      "learning_rate": 7.709090909090909e-05,
      "loss": 0.025,
      "step": 677
    },
    {
      "epoch": 0.6163636363636363,
      "grad_norm": 0.24987369775772095,
      "learning_rate": 7.69090909090909e-05,
      "loss": 0.0566,
      "step": 678
    },
    {
      "epoch": 0.6172727272727273,
      "grad_norm": 0.2721593976020813,
      "learning_rate": 7.672727272727273e-05,
      "loss": 0.0513,
      "step": 679
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.2728869616985321,
      "learning_rate": 7.654545454545456e-05,
      "loss": 0.0546,
      "step": 680
    },
    {
      "epoch": 0.6190909090909091,
      "grad_norm": 0.3918989300727844,
      "learning_rate": 7.636363636363637e-05,
      "loss": 0.056,
      "step": 681
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.37663769721984863,
      "learning_rate": 7.618181818181819e-05,
      "loss": 0.0603,
      "step": 682
    },
    {
      "epoch": 0.6209090909090909,
      "grad_norm": 0.21651802957057953,
      "learning_rate": 7.6e-05,
      "loss": 0.0511,
      "step": 683
    },
    {
      "epoch": 0.6218181818181818,
      "grad_norm": 0.2528011202812195,
      "learning_rate": 7.581818181818182e-05,
      "loss": 0.0548,
      "step": 684
    },
    {
      "epoch": 0.6227272727272727,
      "grad_norm": 0.2098301500082016,
      "learning_rate": 7.563636363636363e-05,
      "loss": 0.0521,
      "step": 685
    },
    {
      "epoch": 0.6236363636363637,
      "grad_norm": 0.17646223306655884,
      "learning_rate": 7.545454545454545e-05,
      "loss": 0.0516,
      "step": 686
    },
    {
      "epoch": 0.6245454545454545,
      "grad_norm": 0.19127042591571808,
      "learning_rate": 7.527272727272728e-05,
      "loss": 0.0241,
      "step": 687
    },
    {
      "epoch": 0.6254545454545455,
      "grad_norm": 0.3106508255004883,
      "learning_rate": 7.509090909090909e-05,
      "loss": 0.0557,
      "step": 688
    },
    {
      "epoch": 0.6263636363636363,
      "grad_norm": 0.27945172786712646,
      "learning_rate": 7.490909090909092e-05,
      "loss": 0.0541,
      "step": 689
    },
    {
      "epoch": 0.6272727272727273,
      "grad_norm": 0.38767126202583313,
      "learning_rate": 7.472727272727274e-05,
      "loss": 0.0618,
      "step": 690
    },
    {
      "epoch": 0.6281818181818182,
      "grad_norm": 0.21975067257881165,
      "learning_rate": 7.454545454545455e-05,
      "loss": 0.0565,
      "step": 691
    },
    {
      "epoch": 0.6290909090909091,
      "grad_norm": 0.379781037569046,
      "learning_rate": 7.436363636363637e-05,
      "loss": 0.0647,
      "step": 692
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.8826600313186646,
      "learning_rate": 7.418181818181818e-05,
      "loss": 0.0609,
      "step": 693
    },
    {
      "epoch": 0.6309090909090909,
      "grad_norm": 0.2273537665605545,
      "learning_rate": 7.4e-05,
      "loss": 0.056,
      "step": 694
    },
    {
      "epoch": 0.6318181818181818,
      "grad_norm": 0.24815046787261963,
      "learning_rate": 7.381818181818182e-05,
      "loss": 0.0565,
      "step": 695
    },
    {
      "epoch": 0.6327272727272727,
      "grad_norm": 0.32102110981941223,
      "learning_rate": 7.363636363636364e-05,
      "loss": 0.0549,
      "step": 696
    },
    {
      "epoch": 0.6336363636363637,
      "grad_norm": 0.1523551344871521,
      "learning_rate": 7.345454545454545e-05,
      "loss": 0.0536,
      "step": 697
    },
    {
      "epoch": 0.6345454545454545,
      "grad_norm": 0.21096262335777283,
      "learning_rate": 7.327272727272728e-05,
      "loss": 0.0267,
      "step": 698
    },
    {
      "epoch": 0.6354545454545455,
      "grad_norm": 0.1996089071035385,
      "learning_rate": 7.30909090909091e-05,
      "loss": 0.0286,
      "step": 699
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.3270150423049927,
      "learning_rate": 7.290909090909091e-05,
      "loss": 0.0536,
      "step": 700
    },
    {
      "epoch": 0.6372727272727273,
      "grad_norm": 0.30030128359794617,
      "learning_rate": 7.272727272727273e-05,
      "loss": 0.0577,
      "step": 701
    },
    {
      "epoch": 0.6381818181818182,
      "grad_norm": 0.1600712090730667,
      "learning_rate": 7.254545454545454e-05,
      "loss": 0.0539,
      "step": 702
    },
    {
      "epoch": 0.639090909090909,
      "grad_norm": 0.19135747849941254,
      "learning_rate": 7.236363636363637e-05,
      "loss": 0.0527,
      "step": 703
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.1637732982635498,
      "learning_rate": 7.218181818181819e-05,
      "loss": 0.0254,
      "step": 704
    },
    {
      "epoch": 0.6409090909090909,
      "grad_norm": 0.26794886589050293,
      "learning_rate": 7.2e-05,
      "loss": 0.0522,
      "step": 705
    },
    {
      "epoch": 0.6418181818181818,
      "grad_norm": 0.19702671468257904,
      "learning_rate": 7.181818181818182e-05,
      "loss": 0.0512,
      "step": 706
    },
    {
      "epoch": 0.6427272727272727,
      "grad_norm": 0.25415462255477905,
      "learning_rate": 7.163636363636363e-05,
      "loss": 0.0565,
      "step": 707
    },
    {
      "epoch": 0.6436363636363637,
      "grad_norm": 0.28868961334228516,
      "learning_rate": 7.145454545454546e-05,
      "loss": 0.0543,
      "step": 708
    },
    {
      "epoch": 0.6445454545454545,
      "grad_norm": 0.21170523762702942,
      "learning_rate": 7.127272727272728e-05,
      "loss": 0.0535,
      "step": 709
    },
    {
      "epoch": 0.6454545454545455,
      "grad_norm": 0.1892554610967636,
      "learning_rate": 7.109090909090909e-05,
      "loss": 0.0264,
      "step": 710
    },
    {
      "epoch": 0.6463636363636364,
      "grad_norm": 0.19539786875247955,
      "learning_rate": 7.090909090909092e-05,
      "loss": 0.054,
      "step": 711
    },
    {
      "epoch": 0.6472727272727272,
      "grad_norm": 0.19173575937747955,
      "learning_rate": 7.072727272727273e-05,
      "loss": 0.0492,
      "step": 712
    },
    {
      "epoch": 0.6481818181818182,
      "grad_norm": 0.2276279330253601,
      "learning_rate": 7.054545454545455e-05,
      "loss": 0.0518,
      "step": 713
    },
    {
      "epoch": 0.649090909090909,
      "grad_norm": 0.48696374893188477,
      "learning_rate": 7.036363636363636e-05,
      "loss": 0.0627,
      "step": 714
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.16062800586223602,
      "learning_rate": 7.018181818181818e-05,
      "loss": 0.0231,
      "step": 715
    },
    {
      "epoch": 0.6509090909090909,
      "grad_norm": 0.3506931662559509,
      "learning_rate": 7e-05,
      "loss": 0.053,
      "step": 716
    },
    {
      "epoch": 0.6518181818181819,
      "grad_norm": 0.1956942230463028,
      "learning_rate": 6.981818181818182e-05,
      "loss": 0.0266,
      "step": 717
    },
    {
      "epoch": 0.6527272727272727,
      "grad_norm": 0.19146914780139923,
      "learning_rate": 6.963636363636364e-05,
      "loss": 0.0269,
      "step": 718
    },
    {
      "epoch": 0.6536363636363637,
      "grad_norm": 0.2267921268939972,
      "learning_rate": 6.945454545454547e-05,
      "loss": 0.0512,
      "step": 719
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.1903991550207138,
      "learning_rate": 6.927272727272728e-05,
      "loss": 0.0257,
      "step": 720
    },
    {
      "epoch": 0.6554545454545454,
      "grad_norm": 0.1942572444677353,
      "learning_rate": 6.90909090909091e-05,
      "loss": 0.0544,
      "step": 721
    },
    {
      "epoch": 0.6563636363636364,
      "grad_norm": 0.4346921741962433,
      "learning_rate": 6.890909090909091e-05,
      "loss": 0.0611,
      "step": 722
    },
    {
      "epoch": 0.6572727272727272,
      "grad_norm": 0.167611226439476,
      "learning_rate": 6.872727272727273e-05,
      "loss": 0.0503,
      "step": 723
    },
    {
      "epoch": 0.6581818181818182,
      "grad_norm": 0.20030826330184937,
      "learning_rate": 6.854545454545454e-05,
      "loss": 0.0453,
      "step": 724
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.21364450454711914,
      "learning_rate": 6.836363636363637e-05,
      "loss": 0.051,
      "step": 725
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.19160819053649902,
      "learning_rate": 6.818181818181818e-05,
      "loss": 0.0515,
      "step": 726
    },
    {
      "epoch": 0.6609090909090909,
      "grad_norm": 0.2459808886051178,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0516,
      "step": 727
    },
    {
      "epoch": 0.6618181818181819,
      "grad_norm": 0.21444852650165558,
      "learning_rate": 6.781818181818183e-05,
      "loss": 0.0523,
      "step": 728
    },
    {
      "epoch": 0.6627272727272727,
      "grad_norm": 0.20143428444862366,
      "learning_rate": 6.763636363636364e-05,
      "loss": 0.0532,
      "step": 729
    },
    {
      "epoch": 0.6636363636363637,
      "grad_norm": 0.35557809472084045,
      "learning_rate": 6.745454545454546e-05,
      "loss": 0.0633,
      "step": 730
    },
    {
      "epoch": 0.6645454545454546,
      "grad_norm": 0.17388084530830383,
      "learning_rate": 6.727272727272727e-05,
      "loss": 0.026,
      "step": 731
    },
    {
      "epoch": 0.6654545454545454,
      "grad_norm": 0.20059086382389069,
      "learning_rate": 6.709090909090909e-05,
      "loss": 0.0501,
      "step": 732
    },
    {
      "epoch": 0.6663636363636364,
      "grad_norm": 0.2669875919818878,
      "learning_rate": 6.690909090909092e-05,
      "loss": 0.0471,
      "step": 733
    },
    {
      "epoch": 0.6672727272727272,
      "grad_norm": 0.2172190397977829,
      "learning_rate": 6.672727272727273e-05,
      "loss": 0.0557,
      "step": 734
    },
    {
      "epoch": 0.6681818181818182,
      "grad_norm": 0.2411191612482071,
      "learning_rate": 6.654545454545455e-05,
      "loss": 0.0548,
      "step": 735
    },
    {
      "epoch": 0.6690909090909091,
      "grad_norm": 0.24367569386959076,
      "learning_rate": 6.636363636363638e-05,
      "loss": 0.0476,
      "step": 736
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.2705601751804352,
      "learning_rate": 6.618181818181819e-05,
      "loss": 0.0517,
      "step": 737
    },
    {
      "epoch": 0.6709090909090909,
      "grad_norm": 0.6416834592819214,
      "learning_rate": 6.6e-05,
      "loss": 0.0651,
      "step": 738
    },
    {
      "epoch": 0.6718181818181819,
      "grad_norm": 0.2923550009727478,
      "learning_rate": 6.581818181818182e-05,
      "loss": 0.0518,
      "step": 739
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.22735074162483215,
      "learning_rate": 6.563636363636364e-05,
      "loss": 0.0544,
      "step": 740
    },
    {
      "epoch": 0.6736363636363636,
      "grad_norm": 0.28176790475845337,
      "learning_rate": 6.545454545454546e-05,
      "loss": 0.0514,
      "step": 741
    },
    {
      "epoch": 0.6745454545454546,
      "grad_norm": 0.23222088813781738,
      "learning_rate": 6.527272727272728e-05,
      "loss": 0.0559,
      "step": 742
    },
    {
      "epoch": 0.6754545454545454,
      "grad_norm": 0.23884382843971252,
      "learning_rate": 6.50909090909091e-05,
      "loss": 0.0529,
      "step": 743
    },
    {
      "epoch": 0.6763636363636364,
      "grad_norm": 0.25008416175842285,
      "learning_rate": 6.490909090909091e-05,
      "loss": 0.0582,
      "step": 744
    },
    {
      "epoch": 0.6772727272727272,
      "grad_norm": 0.23036274313926697,
      "learning_rate": 6.472727272727272e-05,
      "loss": 0.0536,
      "step": 745
    },
    {
      "epoch": 0.6781818181818182,
      "grad_norm": 0.5604188442230225,
      "learning_rate": 6.454545454545455e-05,
      "loss": 0.0389,
      "step": 746
    },
    {
      "epoch": 0.6790909090909091,
      "grad_norm": 0.3320333957672119,
      "learning_rate": 6.436363636363637e-05,
      "loss": 0.0514,
      "step": 747
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.3300572335720062,
      "learning_rate": 6.418181818181818e-05,
      "loss": 0.0589,
      "step": 748
    },
    {
      "epoch": 0.6809090909090909,
      "grad_norm": 0.18553423881530762,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.0246,
      "step": 749
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.2498057335615158,
      "learning_rate": 6.381818181818183e-05,
      "loss": 0.0277,
      "step": 750
    },
    {
      "epoch": 0.6827272727272727,
      "grad_norm": 0.23848365247249603,
      "learning_rate": 6.363636363636364e-05,
      "loss": 0.0305,
      "step": 751
    },
    {
      "epoch": 0.6836363636363636,
      "grad_norm": 0.25399771332740784,
      "learning_rate": 6.345454545454546e-05,
      "loss": 0.0283,
      "step": 752
    },
    {
      "epoch": 0.6845454545454546,
      "grad_norm": 0.164177805185318,
      "learning_rate": 6.327272727272727e-05,
      "loss": 0.0502,
      "step": 753
    },
    {
      "epoch": 0.6854545454545454,
      "grad_norm": 0.16230171918869019,
      "learning_rate": 6.309090909090909e-05,
      "loss": 0.0497,
      "step": 754
    },
    {
      "epoch": 0.6863636363636364,
      "grad_norm": 0.4684385359287262,
      "learning_rate": 6.290909090909091e-05,
      "loss": 0.0309,
      "step": 755
    },
    {
      "epoch": 0.6872727272727273,
      "grad_norm": 0.20561036467552185,
      "learning_rate": 6.272727272727273e-05,
      "loss": 0.0227,
      "step": 756
    },
    {
      "epoch": 0.6881818181818182,
      "grad_norm": 0.3136565089225769,
      "learning_rate": 6.254545454545456e-05,
      "loss": 0.0557,
      "step": 757
    },
    {
      "epoch": 0.6890909090909091,
      "grad_norm": 0.18209218978881836,
      "learning_rate": 6.236363636363637e-05,
      "loss": 0.0506,
      "step": 758
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.3036195635795593,
      "learning_rate": 6.218181818181819e-05,
      "loss": 0.0557,
      "step": 759
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.2780900299549103,
      "learning_rate": 6.2e-05,
      "loss": 0.0572,
      "step": 760
    },
    {
      "epoch": 0.6918181818181818,
      "grad_norm": 0.19758568704128265,
      "learning_rate": 6.181818181818182e-05,
      "loss": 0.0495,
      "step": 761
    },
    {
      "epoch": 0.6927272727272727,
      "grad_norm": 0.37232694029808044,
      "learning_rate": 6.163636363636363e-05,
      "loss": 0.0617,
      "step": 762
    },
    {
      "epoch": 0.6936363636363636,
      "grad_norm": 0.1674736589193344,
      "learning_rate": 6.145454545454545e-05,
      "loss": 0.0504,
      "step": 763
    },
    {
      "epoch": 0.6945454545454546,
      "grad_norm": 0.5867564678192139,
      "learning_rate": 6.127272727272728e-05,
      "loss": 0.0704,
      "step": 764
    },
    {
      "epoch": 0.6954545454545454,
      "grad_norm": 0.27695244550704956,
      "learning_rate": 6.10909090909091e-05,
      "loss": 0.0545,
      "step": 765
    },
    {
      "epoch": 0.6963636363636364,
      "grad_norm": 0.36048364639282227,
      "learning_rate": 6.090909090909091e-05,
      "loss": 0.0494,
      "step": 766
    },
    {
      "epoch": 0.6972727272727273,
      "grad_norm": 0.19672003388404846,
      "learning_rate": 6.0727272727272735e-05,
      "loss": 0.0563,
      "step": 767
    },
    {
      "epoch": 0.6981818181818182,
      "grad_norm": 0.19878512620925903,
      "learning_rate": 6.054545454545455e-05,
      "loss": 0.0258,
      "step": 768
    },
    {
      "epoch": 0.6990909090909091,
      "grad_norm": 0.40811845660209656,
      "learning_rate": 6.0363636363636365e-05,
      "loss": 0.0596,
      "step": 769
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.21318040788173676,
      "learning_rate": 6.0181818181818187e-05,
      "loss": 0.0261,
      "step": 770
    },
    {
      "epoch": 0.7009090909090909,
      "grad_norm": 0.3693384528160095,
      "learning_rate": 6e-05,
      "loss": 0.0577,
      "step": 771
    },
    {
      "epoch": 0.7018181818181818,
      "grad_norm": 0.22008420526981354,
      "learning_rate": 5.9818181818181817e-05,
      "loss": 0.0549,
      "step": 772
    },
    {
      "epoch": 0.7027272727272728,
      "grad_norm": 0.16120924055576324,
      "learning_rate": 5.963636363636363e-05,
      "loss": 0.0475,
      "step": 773
    },
    {
      "epoch": 0.7036363636363636,
      "grad_norm": 0.2856186330318451,
      "learning_rate": 5.945454545454546e-05,
      "loss": 0.0595,
      "step": 774
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.319208025932312,
      "learning_rate": 5.927272727272728e-05,
      "loss": 0.0551,
      "step": 775
    },
    {
      "epoch": 0.7054545454545454,
      "grad_norm": 0.30237671732902527,
      "learning_rate": 5.90909090909091e-05,
      "loss": 0.0587,
      "step": 776
    },
    {
      "epoch": 0.7063636363636364,
      "grad_norm": 0.29178088903427124,
      "learning_rate": 5.890909090909091e-05,
      "loss": 0.0573,
      "step": 777
    },
    {
      "epoch": 0.7072727272727273,
      "grad_norm": 0.3710848391056061,
      "learning_rate": 5.8727272727272734e-05,
      "loss": 0.0568,
      "step": 778
    },
    {
      "epoch": 0.7081818181818181,
      "grad_norm": 0.3436744213104248,
      "learning_rate": 5.854545454545455e-05,
      "loss": 0.0598,
      "step": 779
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.21319754421710968,
      "learning_rate": 5.8363636363636364e-05,
      "loss": 0.0545,
      "step": 780
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.2197578251361847,
      "learning_rate": 5.818181818181818e-05,
      "loss": 0.0555,
      "step": 781
    },
    {
      "epoch": 0.7109090909090909,
      "grad_norm": 0.4621308743953705,
      "learning_rate": 5.8e-05,
      "loss": 0.0617,
      "step": 782
    },
    {
      "epoch": 0.7118181818181818,
      "grad_norm": 0.2079159915447235,
      "learning_rate": 5.7818181818181815e-05,
      "loss": 0.0513,
      "step": 783
    },
    {
      "epoch": 0.7127272727272728,
      "grad_norm": 0.18425965309143066,
      "learning_rate": 5.7636363636363644e-05,
      "loss": 0.0261,
      "step": 784
    },
    {
      "epoch": 0.7136363636363636,
      "grad_norm": 0.33626440167427063,
      "learning_rate": 5.745454545454546e-05,
      "loss": 0.0656,
      "step": 785
    },
    {
      "epoch": 0.7145454545454546,
      "grad_norm": 0.13812033832073212,
      "learning_rate": 5.727272727272728e-05,
      "loss": 0.0493,
      "step": 786
    },
    {
      "epoch": 0.7154545454545455,
      "grad_norm": 0.19914193451404572,
      "learning_rate": 5.7090909090909096e-05,
      "loss": 0.0272,
      "step": 787
    },
    {
      "epoch": 0.7163636363636363,
      "grad_norm": 0.3375912308692932,
      "learning_rate": 5.690909090909091e-05,
      "loss": 0.0322,
      "step": 788
    },
    {
      "epoch": 0.7172727272727273,
      "grad_norm": 0.2143637090921402,
      "learning_rate": 5.6727272727272726e-05,
      "loss": 0.0522,
      "step": 789
    },
    {
      "epoch": 0.7181818181818181,
      "grad_norm": 0.23272906243801117,
      "learning_rate": 5.654545454545455e-05,
      "loss": 0.0542,
      "step": 790
    },
    {
      "epoch": 0.7190909090909091,
      "grad_norm": 0.18752096593379974,
      "learning_rate": 5.636363636363636e-05,
      "loss": 0.0558,
      "step": 791
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.271390825510025,
      "learning_rate": 5.618181818181818e-05,
      "loss": 0.0568,
      "step": 792
    },
    {
      "epoch": 0.7209090909090909,
      "grad_norm": 0.17564107477664948,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.0518,
      "step": 793
    },
    {
      "epoch": 0.7218181818181818,
      "grad_norm": 0.2581776976585388,
      "learning_rate": 5.581818181818183e-05,
      "loss": 0.058,
      "step": 794
    },
    {
      "epoch": 0.7227272727272728,
      "grad_norm": 0.3142525851726532,
      "learning_rate": 5.563636363636364e-05,
      "loss": 0.0628,
      "step": 795
    },
    {
      "epoch": 0.7236363636363636,
      "grad_norm": 0.18017636239528656,
      "learning_rate": 5.545454545454546e-05,
      "loss": 0.0244,
      "step": 796
    },
    {
      "epoch": 0.7245454545454545,
      "grad_norm": 0.177339106798172,
      "learning_rate": 5.527272727272727e-05,
      "loss": 0.052,
      "step": 797
    },
    {
      "epoch": 0.7254545454545455,
      "grad_norm": 0.21182014048099518,
      "learning_rate": 5.5090909090909094e-05,
      "loss": 0.057,
      "step": 798
    },
    {
      "epoch": 0.7263636363636363,
      "grad_norm": 0.18776442110538483,
      "learning_rate": 5.490909090909091e-05,
      "loss": 0.0576,
      "step": 799
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.4030503034591675,
      "learning_rate": 5.4727272727272724e-05,
      "loss": 0.0611,
      "step": 800
    },
    {
      "epoch": 0.7281818181818182,
      "grad_norm": 0.2918105125427246,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 0.0575,
      "step": 801
    },
    {
      "epoch": 0.7290909090909091,
      "grad_norm": 0.2544324994087219,
      "learning_rate": 5.436363636363636e-05,
      "loss": 0.0609,
      "step": 802
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.33331429958343506,
      "learning_rate": 5.418181818181819e-05,
      "loss": 0.0595,
      "step": 803
    },
    {
      "epoch": 0.730909090909091,
      "grad_norm": 0.24255116283893585,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.0583,
      "step": 804
    },
    {
      "epoch": 0.7318181818181818,
      "grad_norm": 0.2878064811229706,
      "learning_rate": 5.3818181818181827e-05,
      "loss": 0.0584,
      "step": 805
    },
    {
      "epoch": 0.7327272727272728,
      "grad_norm": 0.19273972511291504,
      "learning_rate": 5.363636363636364e-05,
      "loss": 0.0513,
      "step": 806
    },
    {
      "epoch": 0.7336363636363636,
      "grad_norm": 0.18282894790172577,
      "learning_rate": 5.3454545454545457e-05,
      "loss": 0.0272,
      "step": 807
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.1977590173482895,
      "learning_rate": 5.327272727272727e-05,
      "loss": 0.0284,
      "step": 808
    },
    {
      "epoch": 0.7354545454545455,
      "grad_norm": 0.21339353919029236,
      "learning_rate": 5.309090909090909e-05,
      "loss": 0.028,
      "step": 809
    },
    {
      "epoch": 0.7363636363636363,
      "grad_norm": 0.19426341354846954,
      "learning_rate": 5.290909090909091e-05,
      "loss": 0.0559,
      "step": 810
    },
    {
      "epoch": 0.7372727272727273,
      "grad_norm": 0.21363268792629242,
      "learning_rate": 5.272727272727272e-05,
      "loss": 0.0532,
      "step": 811
    },
    {
      "epoch": 0.7381818181818182,
      "grad_norm": 0.3022354543209076,
      "learning_rate": 5.254545454545455e-05,
      "loss": 0.0585,
      "step": 812
    },
    {
      "epoch": 0.7390909090909091,
      "grad_norm": 0.20264680683612823,
      "learning_rate": 5.2363636363636374e-05,
      "loss": 0.0565,
      "step": 813
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.20465877652168274,
      "learning_rate": 5.218181818181819e-05,
      "loss": 0.0236,
      "step": 814
    },
    {
      "epoch": 0.740909090909091,
      "grad_norm": 0.17491677403450012,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.0483,
      "step": 815
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.22342225909233093,
      "learning_rate": 5.181818181818182e-05,
      "loss": 0.0551,
      "step": 816
    },
    {
      "epoch": 0.7427272727272727,
      "grad_norm": 0.2182849943637848,
      "learning_rate": 5.163636363636364e-05,
      "loss": 0.0268,
      "step": 817
    },
    {
      "epoch": 0.7436363636363637,
      "grad_norm": 0.29392582178115845,
      "learning_rate": 5.1454545454545455e-05,
      "loss": 0.0646,
      "step": 818
    },
    {
      "epoch": 0.7445454545454545,
      "grad_norm": 0.20234203338623047,
      "learning_rate": 5.127272727272727e-05,
      "loss": 0.0555,
      "step": 819
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.15878839790821075,
      "learning_rate": 5.109090909090909e-05,
      "loss": 0.0537,
      "step": 820
    },
    {
      "epoch": 0.7463636363636363,
      "grad_norm": 0.21841642260551453,
      "learning_rate": 5.090909090909091e-05,
      "loss": 0.0584,
      "step": 821
    },
    {
      "epoch": 0.7472727272727273,
      "grad_norm": 0.3508666455745697,
      "learning_rate": 5.0727272727272736e-05,
      "loss": 0.0617,
      "step": 822
    },
    {
      "epoch": 0.7481818181818182,
      "grad_norm": 0.239065021276474,
      "learning_rate": 5.054545454545455e-05,
      "loss": 0.0514,
      "step": 823
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.4957171082496643,
      "learning_rate": 5.0363636363636366e-05,
      "loss": 0.0641,
      "step": 824
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.25954899191856384,
      "learning_rate": 5.018181818181819e-05,
      "loss": 0.0549,
      "step": 825
    },
    {
      "epoch": 0.7509090909090909,
      "grad_norm": 0.18260277807712555,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 826
    },
    {
      "epoch": 0.7518181818181818,
      "grad_norm": 0.1976996213197708,
      "learning_rate": 4.981818181818182e-05,
      "loss": 0.0271,
      "step": 827
    },
    {
      "epoch": 0.7527272727272727,
      "grad_norm": 0.19034764170646667,
      "learning_rate": 4.963636363636364e-05,
      "loss": 0.0568,
      "step": 828
    },
    {
      "epoch": 0.7536363636363637,
      "grad_norm": 0.19358934462070465,
      "learning_rate": 4.945454545454546e-05,
      "loss": 0.0563,
      "step": 829
    },
    {
      "epoch": 0.7545454545454545,
      "grad_norm": 0.20338845252990723,
      "learning_rate": 4.9272727272727276e-05,
      "loss": 0.056,
      "step": 830
    },
    {
      "epoch": 0.7554545454545455,
      "grad_norm": 0.3853823244571686,
      "learning_rate": 4.909090909090909e-05,
      "loss": 0.0575,
      "step": 831
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.23172950744628906,
      "learning_rate": 4.890909090909091e-05,
      "loss": 0.0514,
      "step": 832
    },
    {
      "epoch": 0.7572727272727273,
      "grad_norm": 0.22398783266544342,
      "learning_rate": 4.872727272727273e-05,
      "loss": 0.0529,
      "step": 833
    },
    {
      "epoch": 0.7581818181818182,
      "grad_norm": 0.39339277148246765,
      "learning_rate": 4.854545454545455e-05,
      "loss": 0.0588,
      "step": 834
    },
    {
      "epoch": 0.759090909090909,
      "grad_norm": 0.1924564242362976,
      "learning_rate": 4.8363636363636364e-05,
      "loss": 0.0488,
      "step": 835
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.2007904201745987,
      "learning_rate": 4.8181818181818186e-05,
      "loss": 0.0564,
      "step": 836
    },
    {
      "epoch": 0.7609090909090909,
      "grad_norm": 0.7672240734100342,
      "learning_rate": 4.8e-05,
      "loss": 0.0668,
      "step": 837
    },
    {
      "epoch": 0.7618181818181818,
      "grad_norm": 0.5996595621109009,
      "learning_rate": 4.781818181818182e-05,
      "loss": 0.0609,
      "step": 838
    },
    {
      "epoch": 0.7627272727272727,
      "grad_norm": 0.3917081356048584,
      "learning_rate": 4.763636363636364e-05,
      "loss": 0.0551,
      "step": 839
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.2118200957775116,
      "learning_rate": 4.745454545454546e-05,
      "loss": 0.0564,
      "step": 840
    },
    {
      "epoch": 0.7645454545454545,
      "grad_norm": 0.36450883746147156,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 0.0537,
      "step": 841
    },
    {
      "epoch": 0.7654545454545455,
      "grad_norm": 0.2939676344394684,
      "learning_rate": 4.709090909090909e-05,
      "loss": 0.0529,
      "step": 842
    },
    {
      "epoch": 0.7663636363636364,
      "grad_norm": 0.20683516561985016,
      "learning_rate": 4.690909090909091e-05,
      "loss": 0.0582,
      "step": 843
    },
    {
      "epoch": 0.7672727272727272,
      "grad_norm": 0.21829847991466522,
      "learning_rate": 4.672727272727273e-05,
      "loss": 0.0533,
      "step": 844
    },
    {
      "epoch": 0.7681818181818182,
      "grad_norm": 0.2405492067337036,
      "learning_rate": 4.654545454545455e-05,
      "loss": 0.0569,
      "step": 845
    },
    {
      "epoch": 0.769090909090909,
      "grad_norm": 0.13638463616371155,
      "learning_rate": 4.636363636363636e-05,
      "loss": 0.0517,
      "step": 846
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.5324906706809998,
      "learning_rate": 4.618181818181818e-05,
      "loss": 0.0659,
      "step": 847
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.17954635620117188,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.055,
      "step": 848
    },
    {
      "epoch": 0.7718181818181818,
      "grad_norm": 0.2826773226261139,
      "learning_rate": 4.581818181818182e-05,
      "loss": 0.0536,
      "step": 849
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.1592562347650528,
      "learning_rate": 4.563636363636364e-05,
      "loss": 0.0504,
      "step": 850
    },
    {
      "epoch": 0.7736363636363637,
      "grad_norm": 0.3376138210296631,
      "learning_rate": 4.545454545454546e-05,
      "loss": 0.0322,
      "step": 851
    },
    {
      "epoch": 0.7745454545454545,
      "grad_norm": 0.2037758231163025,
      "learning_rate": 4.5272727272727274e-05,
      "loss": 0.0522,
      "step": 852
    },
    {
      "epoch": 0.7754545454545455,
      "grad_norm": 0.2778702676296234,
      "learning_rate": 4.5090909090909095e-05,
      "loss": 0.0533,
      "step": 853
    },
    {
      "epoch": 0.7763636363636364,
      "grad_norm": 0.18752025067806244,
      "learning_rate": 4.490909090909091e-05,
      "loss": 0.0515,
      "step": 854
    },
    {
      "epoch": 0.7772727272727272,
      "grad_norm": 0.19659611582756042,
      "learning_rate": 4.472727272727273e-05,
      "loss": 0.0499,
      "step": 855
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.1836577206850052,
      "learning_rate": 4.454545454545455e-05,
      "loss": 0.0497,
      "step": 856
    },
    {
      "epoch": 0.7790909090909091,
      "grad_norm": 0.22353897988796234,
      "learning_rate": 4.436363636363637e-05,
      "loss": 0.053,
      "step": 857
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.23655036091804504,
      "learning_rate": 4.4181818181818184e-05,
      "loss": 0.0527,
      "step": 858
    },
    {
      "epoch": 0.7809090909090909,
      "grad_norm": 0.22720327973365784,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0525,
      "step": 859
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.30396586656570435,
      "learning_rate": 4.381818181818182e-05,
      "loss": 0.0578,
      "step": 860
    },
    {
      "epoch": 0.7827272727272727,
      "grad_norm": 0.13091084361076355,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 0.0518,
      "step": 861
    },
    {
      "epoch": 0.7836363636363637,
      "grad_norm": 0.1399194896221161,
      "learning_rate": 4.345454545454546e-05,
      "loss": 0.0516,
      "step": 862
    },
    {
      "epoch": 0.7845454545454545,
      "grad_norm": 0.24022409319877625,
      "learning_rate": 4.327272727272728e-05,
      "loss": 0.0538,
      "step": 863
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.2527396082878113,
      "learning_rate": 4.3090909090909094e-05,
      "loss": 0.0517,
      "step": 864
    },
    {
      "epoch": 0.7863636363636364,
      "grad_norm": 0.17194685339927673,
      "learning_rate": 4.290909090909091e-05,
      "loss": 0.0513,
      "step": 865
    },
    {
      "epoch": 0.7872727272727272,
      "grad_norm": 0.2525333762168884,
      "learning_rate": 4.2727272727272724e-05,
      "loss": 0.0499,
      "step": 866
    },
    {
      "epoch": 0.7881818181818182,
      "grad_norm": 0.17250163853168488,
      "learning_rate": 4.254545454545455e-05,
      "loss": 0.0507,
      "step": 867
    },
    {
      "epoch": 0.7890909090909091,
      "grad_norm": 0.253019779920578,
      "learning_rate": 4.236363636363637e-05,
      "loss": 0.054,
      "step": 868
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.17052102088928223,
      "learning_rate": 4.218181818181818e-05,
      "loss": 0.0504,
      "step": 869
    },
    {
      "epoch": 0.7909090909090909,
      "grad_norm": 0.22149880230426788,
      "learning_rate": 4.2e-05,
      "loss": 0.0289,
      "step": 870
    },
    {
      "epoch": 0.7918181818181819,
      "grad_norm": 0.24777282774448395,
      "learning_rate": 4.181818181818182e-05,
      "loss": 0.0566,
      "step": 871
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 0.2814708352088928,
      "learning_rate": 4.163636363636364e-05,
      "loss": 0.0573,
      "step": 872
    },
    {
      "epoch": 0.7936363636363636,
      "grad_norm": 0.14071694016456604,
      "learning_rate": 4.1454545454545456e-05,
      "loss": 0.0485,
      "step": 873
    },
    {
      "epoch": 0.7945454545454546,
      "grad_norm": 0.20323622226715088,
      "learning_rate": 4.127272727272727e-05,
      "loss": 0.029,
      "step": 874
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 0.23609231412410736,
      "learning_rate": 4.109090909090909e-05,
      "loss": 0.051,
      "step": 875
    },
    {
      "epoch": 0.7963636363636364,
      "grad_norm": 0.4082074463367462,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 0.0526,
      "step": 876
    },
    {
      "epoch": 0.7972727272727272,
      "grad_norm": 0.29646074771881104,
      "learning_rate": 4.072727272727273e-05,
      "loss": 0.0544,
      "step": 877
    },
    {
      "epoch": 0.7981818181818182,
      "grad_norm": 0.183143749833107,
      "learning_rate": 4.0545454545454545e-05,
      "loss": 0.0501,
      "step": 878
    },
    {
      "epoch": 0.7990909090909091,
      "grad_norm": 0.37613925337791443,
      "learning_rate": 4.0363636363636367e-05,
      "loss": 0.031,
      "step": 879
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1912785917520523,
      "learning_rate": 4.018181818181818e-05,
      "loss": 0.0528,
      "step": 880
    },
    {
      "epoch": 0.8009090909090909,
      "grad_norm": 0.20103612542152405,
      "learning_rate": 4e-05,
      "loss": 0.0572,
      "step": 881
    },
    {
      "epoch": 0.8018181818181818,
      "grad_norm": 0.31200066208839417,
      "learning_rate": 3.981818181818182e-05,
      "loss": 0.0554,
      "step": 882
    },
    {
      "epoch": 0.8027272727272727,
      "grad_norm": 0.17478154599666595,
      "learning_rate": 3.963636363636364e-05,
      "loss": 0.0512,
      "step": 883
    },
    {
      "epoch": 0.8036363636363636,
      "grad_norm": 0.25717633962631226,
      "learning_rate": 3.9454545454545455e-05,
      "loss": 0.0576,
      "step": 884
    },
    {
      "epoch": 0.8045454545454546,
      "grad_norm": 0.2606467306613922,
      "learning_rate": 3.927272727272727e-05,
      "loss": 0.0504,
      "step": 885
    },
    {
      "epoch": 0.8054545454545454,
      "grad_norm": 0.183901846408844,
      "learning_rate": 3.909090909090909e-05,
      "loss": 0.0516,
      "step": 886
    },
    {
      "epoch": 0.8063636363636364,
      "grad_norm": 0.16310161352157593,
      "learning_rate": 3.8909090909090914e-05,
      "loss": 0.0478,
      "step": 887
    },
    {
      "epoch": 0.8072727272727273,
      "grad_norm": 0.31287145614624023,
      "learning_rate": 3.872727272727273e-05,
      "loss": 0.0537,
      "step": 888
    },
    {
      "epoch": 0.8081818181818182,
      "grad_norm": 0.23238462209701538,
      "learning_rate": 3.8545454545454544e-05,
      "loss": 0.0491,
      "step": 889
    },
    {
      "epoch": 0.8090909090909091,
      "grad_norm": 0.2544257640838623,
      "learning_rate": 3.8363636363636365e-05,
      "loss": 0.0518,
      "step": 890
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.32298514246940613,
      "learning_rate": 3.818181818181819e-05,
      "loss": 0.0513,
      "step": 891
    },
    {
      "epoch": 0.8109090909090909,
      "grad_norm": 0.19690342247486115,
      "learning_rate": 3.8e-05,
      "loss": 0.0513,
      "step": 892
    },
    {
      "epoch": 0.8118181818181818,
      "grad_norm": 0.26743417978286743,
      "learning_rate": 3.781818181818182e-05,
      "loss": 0.0513,
      "step": 893
    },
    {
      "epoch": 0.8127272727272727,
      "grad_norm": 0.21972469985485077,
      "learning_rate": 3.763636363636364e-05,
      "loss": 0.0262,
      "step": 894
    },
    {
      "epoch": 0.8136363636363636,
      "grad_norm": 0.24183852970600128,
      "learning_rate": 3.745454545454546e-05,
      "loss": 0.0574,
      "step": 895
    },
    {
      "epoch": 0.8145454545454546,
      "grad_norm": 0.22782433032989502,
      "learning_rate": 3.7272727272727276e-05,
      "loss": 0.0558,
      "step": 896
    },
    {
      "epoch": 0.8154545454545454,
      "grad_norm": 0.18753144145011902,
      "learning_rate": 3.709090909090909e-05,
      "loss": 0.0257,
      "step": 897
    },
    {
      "epoch": 0.8163636363636364,
      "grad_norm": 0.23323144018650055,
      "learning_rate": 3.690909090909091e-05,
      "loss": 0.0487,
      "step": 898
    },
    {
      "epoch": 0.8172727272727273,
      "grad_norm": 0.19062738120555878,
      "learning_rate": 3.672727272727273e-05,
      "loss": 0.05,
      "step": 899
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.21274535357952118,
      "learning_rate": 3.654545454545455e-05,
      "loss": 0.0479,
      "step": 900
    },
    {
      "epoch": 0.8190909090909091,
      "grad_norm": 0.22002772986888885,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 0.0583,
      "step": 901
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.20660530030727386,
      "learning_rate": 3.6181818181818186e-05,
      "loss": 0.0531,
      "step": 902
    },
    {
      "epoch": 0.8209090909090909,
      "grad_norm": 0.28378283977508545,
      "learning_rate": 3.6e-05,
      "loss": 0.053,
      "step": 903
    },
    {
      "epoch": 0.8218181818181818,
      "grad_norm": 0.3243793845176697,
      "learning_rate": 3.5818181818181816e-05,
      "loss": 0.0589,
      "step": 904
    },
    {
      "epoch": 0.8227272727272728,
      "grad_norm": 0.1581100970506668,
      "learning_rate": 3.563636363636364e-05,
      "loss": 0.0528,
      "step": 905
    },
    {
      "epoch": 0.8236363636363636,
      "grad_norm": 0.192199245095253,
      "learning_rate": 3.545454545454546e-05,
      "loss": 0.0529,
      "step": 906
    },
    {
      "epoch": 0.8245454545454546,
      "grad_norm": 0.22999007999897003,
      "learning_rate": 3.5272727272727274e-05,
      "loss": 0.0496,
      "step": 907
    },
    {
      "epoch": 0.8254545454545454,
      "grad_norm": 0.24687916040420532,
      "learning_rate": 3.509090909090909e-05,
      "loss": 0.057,
      "step": 908
    },
    {
      "epoch": 0.8263636363636364,
      "grad_norm": 0.3835703432559967,
      "learning_rate": 3.490909090909091e-05,
      "loss": 0.027,
      "step": 909
    },
    {
      "epoch": 0.8272727272727273,
      "grad_norm": 0.2466076761484146,
      "learning_rate": 3.472727272727273e-05,
      "loss": 0.0272,
      "step": 910
    },
    {
      "epoch": 0.8281818181818181,
      "grad_norm": 0.24990713596343994,
      "learning_rate": 3.454545454545455e-05,
      "loss": 0.0522,
      "step": 911
    },
    {
      "epoch": 0.8290909090909091,
      "grad_norm": 0.19417287409305573,
      "learning_rate": 3.436363636363636e-05,
      "loss": 0.0233,
      "step": 912
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.20305973291397095,
      "learning_rate": 3.4181818181818185e-05,
      "loss": 0.0534,
      "step": 913
    },
    {
      "epoch": 0.8309090909090909,
      "grad_norm": 0.5151549577713013,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0588,
      "step": 914
    },
    {
      "epoch": 0.8318181818181818,
      "grad_norm": 0.24046826362609863,
      "learning_rate": 3.381818181818182e-05,
      "loss": 0.0292,
      "step": 915
    },
    {
      "epoch": 0.8327272727272728,
      "grad_norm": 0.28566688299179077,
      "learning_rate": 3.3636363636363636e-05,
      "loss": 0.0583,
      "step": 916
    },
    {
      "epoch": 0.8336363636363636,
      "grad_norm": 0.2256142795085907,
      "learning_rate": 3.345454545454546e-05,
      "loss": 0.0471,
      "step": 917
    },
    {
      "epoch": 0.8345454545454546,
      "grad_norm": 0.2681719958782196,
      "learning_rate": 3.327272727272727e-05,
      "loss": 0.0528,
      "step": 918
    },
    {
      "epoch": 0.8354545454545454,
      "grad_norm": 0.273573100566864,
      "learning_rate": 3.3090909090909095e-05,
      "loss": 0.0545,
      "step": 919
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.20480535924434662,
      "learning_rate": 3.290909090909091e-05,
      "loss": 0.054,
      "step": 920
    },
    {
      "epoch": 0.8372727272727273,
      "grad_norm": 0.23774656653404236,
      "learning_rate": 3.272727272727273e-05,
      "loss": 0.0528,
      "step": 921
    },
    {
      "epoch": 0.8381818181818181,
      "grad_norm": 0.21228137612342834,
      "learning_rate": 3.254545454545455e-05,
      "loss": 0.0502,
      "step": 922
    },
    {
      "epoch": 0.8390909090909091,
      "grad_norm": 0.20400843024253845,
      "learning_rate": 3.236363636363636e-05,
      "loss": 0.0511,
      "step": 923
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.20905102789402008,
      "learning_rate": 3.2181818181818184e-05,
      "loss": 0.0259,
      "step": 924
    },
    {
      "epoch": 0.8409090909090909,
      "grad_norm": 0.14713837206363678,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0494,
      "step": 925
    },
    {
      "epoch": 0.8418181818181818,
      "grad_norm": 0.7657853960990906,
      "learning_rate": 3.181818181818182e-05,
      "loss": 0.0703,
      "step": 926
    },
    {
      "epoch": 0.8427272727272728,
      "grad_norm": 0.19992227852344513,
      "learning_rate": 3.1636363636363635e-05,
      "loss": 0.0293,
      "step": 927
    },
    {
      "epoch": 0.8436363636363636,
      "grad_norm": 0.21008636057376862,
      "learning_rate": 3.145454545454546e-05,
      "loss": 0.0528,
      "step": 928
    },
    {
      "epoch": 0.8445454545454546,
      "grad_norm": 0.32681283354759216,
      "learning_rate": 3.127272727272728e-05,
      "loss": 0.0536,
      "step": 929
    },
    {
      "epoch": 0.8454545454545455,
      "grad_norm": 0.3659972846508026,
      "learning_rate": 3.1090909090909094e-05,
      "loss": 0.0644,
      "step": 930
    },
    {
      "epoch": 0.8463636363636363,
      "grad_norm": 0.2684040665626526,
      "learning_rate": 3.090909090909091e-05,
      "loss": 0.0522,
      "step": 931
    },
    {
      "epoch": 0.8472727272727273,
      "grad_norm": 0.19467924535274506,
      "learning_rate": 3.0727272727272724e-05,
      "loss": 0.0251,
      "step": 932
    },
    {
      "epoch": 0.8481818181818181,
      "grad_norm": 0.32111087441444397,
      "learning_rate": 3.054545454545455e-05,
      "loss": 0.0559,
      "step": 933
    },
    {
      "epoch": 0.8490909090909091,
      "grad_norm": 0.16485761106014252,
      "learning_rate": 3.0363636363636367e-05,
      "loss": 0.0518,
      "step": 934
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.18769431114196777,
      "learning_rate": 3.0181818181818182e-05,
      "loss": 0.0265,
      "step": 935
    },
    {
      "epoch": 0.850909090909091,
      "grad_norm": 0.4025070071220398,
      "learning_rate": 3e-05,
      "loss": 0.0285,
      "step": 936
    },
    {
      "epoch": 0.8518181818181818,
      "grad_norm": 0.20720961689949036,
      "learning_rate": 2.9818181818181816e-05,
      "loss": 0.0579,
      "step": 937
    },
    {
      "epoch": 0.8527272727272728,
      "grad_norm": 0.2768906056880951,
      "learning_rate": 2.963636363636364e-05,
      "loss": 0.0548,
      "step": 938
    },
    {
      "epoch": 0.8536363636363636,
      "grad_norm": 0.2890460193157196,
      "learning_rate": 2.9454545454545456e-05,
      "loss": 0.0511,
      "step": 939
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.2002430260181427,
      "learning_rate": 2.9272727272727274e-05,
      "loss": 0.0262,
      "step": 940
    },
    {
      "epoch": 0.8554545454545455,
      "grad_norm": 0.29466965794563293,
      "learning_rate": 2.909090909090909e-05,
      "loss": 0.0526,
      "step": 941
    },
    {
      "epoch": 0.8563636363636363,
      "grad_norm": 0.28994283080101013,
      "learning_rate": 2.8909090909090908e-05,
      "loss": 0.0531,
      "step": 942
    },
    {
      "epoch": 0.8572727272727273,
      "grad_norm": 0.3162596821784973,
      "learning_rate": 2.872727272727273e-05,
      "loss": 0.0519,
      "step": 943
    },
    {
      "epoch": 0.8581818181818182,
      "grad_norm": 0.17520646750926971,
      "learning_rate": 2.8545454545454548e-05,
      "loss": 0.0526,
      "step": 944
    },
    {
      "epoch": 0.8590909090909091,
      "grad_norm": 0.2646542489528656,
      "learning_rate": 2.8363636363636363e-05,
      "loss": 0.0513,
      "step": 945
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.45931118726730347,
      "learning_rate": 2.818181818181818e-05,
      "loss": 0.0582,
      "step": 946
    },
    {
      "epoch": 0.860909090909091,
      "grad_norm": 0.21052822470664978,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0496,
      "step": 947
    },
    {
      "epoch": 0.8618181818181818,
      "grad_norm": 0.2722320258617401,
      "learning_rate": 2.781818181818182e-05,
      "loss": 0.0516,
      "step": 948
    },
    {
      "epoch": 0.8627272727272727,
      "grad_norm": 0.2777880132198334,
      "learning_rate": 2.7636363636363636e-05,
      "loss": 0.0519,
      "step": 949
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.20379483699798584,
      "learning_rate": 2.7454545454545455e-05,
      "loss": 0.0526,
      "step": 950
    },
    {
      "epoch": 0.8645454545454545,
      "grad_norm": 0.17199470102787018,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 0.0544,
      "step": 951
    },
    {
      "epoch": 0.8654545454545455,
      "grad_norm": 0.16459456086158752,
      "learning_rate": 2.7090909090909095e-05,
      "loss": 0.0511,
      "step": 952
    },
    {
      "epoch": 0.8663636363636363,
      "grad_norm": 0.6572884917259216,
      "learning_rate": 2.6909090909090913e-05,
      "loss": 0.0391,
      "step": 953
    },
    {
      "epoch": 0.8672727272727273,
      "grad_norm": 0.23437170684337616,
      "learning_rate": 2.6727272727272728e-05,
      "loss": 0.0554,
      "step": 954
    },
    {
      "epoch": 0.8681818181818182,
      "grad_norm": 0.19232112169265747,
      "learning_rate": 2.6545454545454547e-05,
      "loss": 0.054,
      "step": 955
    },
    {
      "epoch": 0.8690909090909091,
      "grad_norm": 0.21444456279277802,
      "learning_rate": 2.636363636363636e-05,
      "loss": 0.0505,
      "step": 956
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.21811115741729736,
      "learning_rate": 2.6181818181818187e-05,
      "loss": 0.0511,
      "step": 957
    },
    {
      "epoch": 0.8709090909090909,
      "grad_norm": 0.23448720574378967,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0505,
      "step": 958
    },
    {
      "epoch": 0.8718181818181818,
      "grad_norm": 0.22706986963748932,
      "learning_rate": 2.581818181818182e-05,
      "loss": 0.0509,
      "step": 959
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.1586838662624359,
      "learning_rate": 2.5636363636363635e-05,
      "loss": 0.0483,
      "step": 960
    },
    {
      "epoch": 0.8736363636363637,
      "grad_norm": 0.20564156770706177,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 0.0268,
      "step": 961
    },
    {
      "epoch": 0.8745454545454545,
      "grad_norm": 0.39575666189193726,
      "learning_rate": 2.5272727272727275e-05,
      "loss": 0.0295,
      "step": 962
    },
    {
      "epoch": 0.8754545454545455,
      "grad_norm": 0.22601863741874695,
      "learning_rate": 2.5090909090909094e-05,
      "loss": 0.0552,
      "step": 963
    },
    {
      "epoch": 0.8763636363636363,
      "grad_norm": 0.15949733555316925,
      "learning_rate": 2.490909090909091e-05,
      "loss": 0.0538,
      "step": 964
    },
    {
      "epoch": 0.8772727272727273,
      "grad_norm": 0.2985471487045288,
      "learning_rate": 2.472727272727273e-05,
      "loss": 0.0583,
      "step": 965
    },
    {
      "epoch": 0.8781818181818182,
      "grad_norm": 0.28172677755355835,
      "learning_rate": 2.4545454545454545e-05,
      "loss": 0.0531,
      "step": 966
    },
    {
      "epoch": 0.8790909090909091,
      "grad_norm": 0.2726418375968933,
      "learning_rate": 2.4363636363636364e-05,
      "loss": 0.0516,
      "step": 967
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.22147607803344727,
      "learning_rate": 2.4181818181818182e-05,
      "loss": 0.0501,
      "step": 968
    },
    {
      "epoch": 0.8809090909090909,
      "grad_norm": 0.2879340648651123,
      "learning_rate": 2.4e-05,
      "loss": 0.0544,
      "step": 969
    },
    {
      "epoch": 0.8818181818181818,
      "grad_norm": 0.23950542509555817,
      "learning_rate": 2.381818181818182e-05,
      "loss": 0.0554,
      "step": 970
    },
    {
      "epoch": 0.8827272727272727,
      "grad_norm": 0.1982128769159317,
      "learning_rate": 2.3636363636363637e-05,
      "loss": 0.0462,
      "step": 971
    },
    {
      "epoch": 0.8836363636363637,
      "grad_norm": 0.28304776549339294,
      "learning_rate": 2.3454545454545456e-05,
      "loss": 0.0544,
      "step": 972
    },
    {
      "epoch": 0.8845454545454545,
      "grad_norm": 0.24769903719425201,
      "learning_rate": 2.3272727272727274e-05,
      "loss": 0.0522,
      "step": 973
    },
    {
      "epoch": 0.8854545454545455,
      "grad_norm": 0.16805092990398407,
      "learning_rate": 2.309090909090909e-05,
      "loss": 0.0511,
      "step": 974
    },
    {
      "epoch": 0.8863636363636364,
      "grad_norm": 0.1973588913679123,
      "learning_rate": 2.290909090909091e-05,
      "loss": 0.0269,
      "step": 975
    },
    {
      "epoch": 0.8872727272727273,
      "grad_norm": 0.2958989441394806,
      "learning_rate": 2.272727272727273e-05,
      "loss": 0.0585,
      "step": 976
    },
    {
      "epoch": 0.8881818181818182,
      "grad_norm": 0.1923600435256958,
      "learning_rate": 2.2545454545454548e-05,
      "loss": 0.0542,
      "step": 977
    },
    {
      "epoch": 0.889090909090909,
      "grad_norm": 0.29327788949012756,
      "learning_rate": 2.2363636363636366e-05,
      "loss": 0.0536,
      "step": 978
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.23056389391422272,
      "learning_rate": 2.2181818181818184e-05,
      "loss": 0.0505,
      "step": 979
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.28085005283355713,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0565,
      "step": 980
    },
    {
      "epoch": 0.8918181818181818,
      "grad_norm": 0.21928595006465912,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 0.0509,
      "step": 981
    },
    {
      "epoch": 0.8927272727272727,
      "grad_norm": 0.2977064549922943,
      "learning_rate": 2.163636363636364e-05,
      "loss": 0.0607,
      "step": 982
    },
    {
      "epoch": 0.8936363636363637,
      "grad_norm": 0.5429867506027222,
      "learning_rate": 2.1454545454545455e-05,
      "loss": 0.0592,
      "step": 983
    },
    {
      "epoch": 0.8945454545454545,
      "grad_norm": 0.2061573714017868,
      "learning_rate": 2.1272727272727276e-05,
      "loss": 0.0486,
      "step": 984
    },
    {
      "epoch": 0.8954545454545455,
      "grad_norm": 0.2078258991241455,
      "learning_rate": 2.109090909090909e-05,
      "loss": 0.0551,
      "step": 985
    },
    {
      "epoch": 0.8963636363636364,
      "grad_norm": 0.1924189031124115,
      "learning_rate": 2.090909090909091e-05,
      "loss": 0.0465,
      "step": 986
    },
    {
      "epoch": 0.8972727272727272,
      "grad_norm": 0.24370193481445312,
      "learning_rate": 2.0727272727272728e-05,
      "loss": 0.0541,
      "step": 987
    },
    {
      "epoch": 0.8981818181818182,
      "grad_norm": 0.20916084945201874,
      "learning_rate": 2.0545454545454546e-05,
      "loss": 0.0561,
      "step": 988
    },
    {
      "epoch": 0.899090909090909,
      "grad_norm": 0.2531740367412567,
      "learning_rate": 2.0363636363636365e-05,
      "loss": 0.0514,
      "step": 989
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.18149642646312714,
      "learning_rate": 2.0181818181818183e-05,
      "loss": 0.0528,
      "step": 990
    },
    {
      "epoch": 0.9009090909090909,
      "grad_norm": 0.17994628846645355,
      "learning_rate": 2e-05,
      "loss": 0.0519,
      "step": 991
    },
    {
      "epoch": 0.9018181818181819,
      "grad_norm": 0.22486139833927155,
      "learning_rate": 1.981818181818182e-05,
      "loss": 0.0277,
      "step": 992
    },
    {
      "epoch": 0.9027272727272727,
      "grad_norm": 0.33224591612815857,
      "learning_rate": 1.9636363636363635e-05,
      "loss": 0.0534,
      "step": 993
    },
    {
      "epoch": 0.9036363636363637,
      "grad_norm": 0.2821165919303894,
      "learning_rate": 1.9454545454545457e-05,
      "loss": 0.0538,
      "step": 994
    },
    {
      "epoch": 0.9045454545454545,
      "grad_norm": 0.33031341433525085,
      "learning_rate": 1.9272727272727272e-05,
      "loss": 0.0352,
      "step": 995
    },
    {
      "epoch": 0.9054545454545454,
      "grad_norm": 0.21681036055088043,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 0.0533,
      "step": 996
    },
    {
      "epoch": 0.9063636363636364,
      "grad_norm": 0.24059665203094482,
      "learning_rate": 1.890909090909091e-05,
      "loss": 0.0494,
      "step": 997
    },
    {
      "epoch": 0.9072727272727272,
      "grad_norm": 0.16452209651470184,
      "learning_rate": 1.872727272727273e-05,
      "loss": 0.0506,
      "step": 998
    },
    {
      "epoch": 0.9081818181818182,
      "grad_norm": 0.29030993580818176,
      "learning_rate": 1.8545454545454545e-05,
      "loss": 0.0524,
      "step": 999
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.2611234486103058,
      "learning_rate": 1.8363636363636364e-05,
      "loss": 0.0481,
      "step": 1000
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.22997993230819702,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.0548,
      "step": 1001
    },
    {
      "epoch": 0.9109090909090909,
      "grad_norm": 0.21002881228923798,
      "learning_rate": 1.8e-05,
      "loss": 0.0487,
      "step": 1002
    },
    {
      "epoch": 0.9118181818181819,
      "grad_norm": 0.3946491777896881,
      "learning_rate": 1.781818181818182e-05,
      "loss": 0.0274,
      "step": 1003
    },
    {
      "epoch": 0.9127272727272727,
      "grad_norm": 0.2238774299621582,
      "learning_rate": 1.7636363636363637e-05,
      "loss": 0.0511,
      "step": 1004
    },
    {
      "epoch": 0.9136363636363637,
      "grad_norm": 0.34815090894699097,
      "learning_rate": 1.7454545454545456e-05,
      "loss": 0.053,
      "step": 1005
    },
    {
      "epoch": 0.9145454545454546,
      "grad_norm": 0.1183852031826973,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 0.0494,
      "step": 1006
    },
    {
      "epoch": 0.9154545454545454,
      "grad_norm": 0.35809335112571716,
      "learning_rate": 1.7090909090909092e-05,
      "loss": 0.0552,
      "step": 1007
    },
    {
      "epoch": 0.9163636363636364,
      "grad_norm": 0.19597330689430237,
      "learning_rate": 1.690909090909091e-05,
      "loss": 0.0269,
      "step": 1008
    },
    {
      "epoch": 0.9172727272727272,
      "grad_norm": 0.4011503756046295,
      "learning_rate": 1.672727272727273e-05,
      "loss": 0.0599,
      "step": 1009
    },
    {
      "epoch": 0.9181818181818182,
      "grad_norm": 0.17837685346603394,
      "learning_rate": 1.6545454545454548e-05,
      "loss": 0.0491,
      "step": 1010
    },
    {
      "epoch": 0.9190909090909091,
      "grad_norm": 0.27670806646347046,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 0.0529,
      "step": 1011
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.24150323867797852,
      "learning_rate": 1.618181818181818e-05,
      "loss": 0.0543,
      "step": 1012
    },
    {
      "epoch": 0.9209090909090909,
      "grad_norm": 0.24697767198085785,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0521,
      "step": 1013
    },
    {
      "epoch": 0.9218181818181819,
      "grad_norm": 0.16358298063278198,
      "learning_rate": 1.5818181818181818e-05,
      "loss": 0.0505,
      "step": 1014
    },
    {
      "epoch": 0.9227272727272727,
      "grad_norm": 0.18144655227661133,
      "learning_rate": 1.563636363636364e-05,
      "loss": 0.0493,
      "step": 1015
    },
    {
      "epoch": 0.9236363636363636,
      "grad_norm": 0.28276050090789795,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 0.029,
      "step": 1016
    },
    {
      "epoch": 0.9245454545454546,
      "grad_norm": 0.17960414290428162,
      "learning_rate": 1.5272727272727276e-05,
      "loss": 0.0489,
      "step": 1017
    },
    {
      "epoch": 0.9254545454545454,
      "grad_norm": 0.203772634267807,
      "learning_rate": 1.5090909090909091e-05,
      "loss": 0.0529,
      "step": 1018
    },
    {
      "epoch": 0.9263636363636364,
      "grad_norm": 0.35106056928634644,
      "learning_rate": 1.4909090909090908e-05,
      "loss": 0.0531,
      "step": 1019
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.3033183813095093,
      "learning_rate": 1.4727272727272728e-05,
      "loss": 0.0547,
      "step": 1020
    },
    {
      "epoch": 0.9281818181818182,
      "grad_norm": 0.20715618133544922,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 0.0541,
      "step": 1021
    },
    {
      "epoch": 0.9290909090909091,
      "grad_norm": 0.19135373830795288,
      "learning_rate": 1.4363636363636365e-05,
      "loss": 0.0506,
      "step": 1022
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.446902871131897,
      "learning_rate": 1.4181818181818181e-05,
      "loss": 0.0584,
      "step": 1023
    },
    {
      "epoch": 0.9309090909090909,
      "grad_norm": 0.43811243772506714,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0599,
      "step": 1024
    },
    {
      "epoch": 0.9318181818181818,
      "grad_norm": 0.2237681895494461,
      "learning_rate": 1.3818181818181818e-05,
      "loss": 0.0268,
      "step": 1025
    },
    {
      "epoch": 0.9327272727272727,
      "grad_norm": 0.3144195079803467,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 0.0536,
      "step": 1026
    },
    {
      "epoch": 0.9336363636363636,
      "grad_norm": 0.18709929287433624,
      "learning_rate": 1.3454545454545457e-05,
      "loss": 0.0496,
      "step": 1027
    },
    {
      "epoch": 0.9345454545454546,
      "grad_norm": 0.4175103008747101,
      "learning_rate": 1.3272727272727273e-05,
      "loss": 0.0587,
      "step": 1028
    },
    {
      "epoch": 0.9354545454545454,
      "grad_norm": 0.18762782216072083,
      "learning_rate": 1.3090909090909093e-05,
      "loss": 0.0541,
      "step": 1029
    },
    {
      "epoch": 0.9363636363636364,
      "grad_norm": 0.15318402647972107,
      "learning_rate": 1.290909090909091e-05,
      "loss": 0.0471,
      "step": 1030
    },
    {
      "epoch": 0.9372727272727273,
      "grad_norm": 0.16729620099067688,
      "learning_rate": 1.2727272727272727e-05,
      "loss": 0.0555,
      "step": 1031
    },
    {
      "epoch": 0.9381818181818182,
      "grad_norm": 0.1605006605386734,
      "learning_rate": 1.2545454545454547e-05,
      "loss": 0.0473,
      "step": 1032
    },
    {
      "epoch": 0.9390909090909091,
      "grad_norm": 0.2726943790912628,
      "learning_rate": 1.2363636363636365e-05,
      "loss": 0.0511,
      "step": 1033
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.2594622075557709,
      "learning_rate": 1.2181818181818182e-05,
      "loss": 0.0543,
      "step": 1034
    },
    {
      "epoch": 0.9409090909090909,
      "grad_norm": 0.2204505205154419,
      "learning_rate": 1.2e-05,
      "loss": 0.0263,
      "step": 1035
    },
    {
      "epoch": 0.9418181818181818,
      "grad_norm": 0.42237311601638794,
      "learning_rate": 1.1818181818181819e-05,
      "loss": 0.0534,
      "step": 1036
    },
    {
      "epoch": 0.9427272727272727,
      "grad_norm": 0.27421727776527405,
      "learning_rate": 1.1636363636363637e-05,
      "loss": 0.051,
      "step": 1037
    },
    {
      "epoch": 0.9436363636363636,
      "grad_norm": 0.22037847340106964,
      "learning_rate": 1.1454545454545455e-05,
      "loss": 0.0287,
      "step": 1038
    },
    {
      "epoch": 0.9445454545454546,
      "grad_norm": 0.22041265666484833,
      "learning_rate": 1.1272727272727274e-05,
      "loss": 0.0274,
      "step": 1039
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.1478969007730484,
      "learning_rate": 1.1090909090909092e-05,
      "loss": 0.0501,
      "step": 1040
    },
    {
      "epoch": 0.9463636363636364,
      "grad_norm": 0.30080553889274597,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.028,
      "step": 1041
    },
    {
      "epoch": 0.9472727272727273,
      "grad_norm": 0.32030779123306274,
      "learning_rate": 1.0727272727272727e-05,
      "loss": 0.0538,
      "step": 1042
    },
    {
      "epoch": 0.9481818181818182,
      "grad_norm": 0.3158712089061737,
      "learning_rate": 1.0545454545454546e-05,
      "loss": 0.0524,
      "step": 1043
    },
    {
      "epoch": 0.9490909090909091,
      "grad_norm": 0.2124832272529602,
      "learning_rate": 1.0363636363636364e-05,
      "loss": 0.027,
      "step": 1044
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.24427685141563416,
      "learning_rate": 1.0181818181818182e-05,
      "loss": 0.0555,
      "step": 1045
    },
    {
      "epoch": 0.9509090909090909,
      "grad_norm": 0.2826186418533325,
      "learning_rate": 1e-05,
      "loss": 0.0554,
      "step": 1046
    },
    {
      "epoch": 0.9518181818181818,
      "grad_norm": 0.2501842975616455,
      "learning_rate": 9.818181818181818e-06,
      "loss": 0.0497,
      "step": 1047
    },
    {
      "epoch": 0.9527272727272728,
      "grad_norm": 0.2917221784591675,
      "learning_rate": 9.636363636363636e-06,
      "loss": 0.0571,
      "step": 1048
    },
    {
      "epoch": 0.9536363636363636,
      "grad_norm": 0.3571226894855499,
      "learning_rate": 9.454545454545454e-06,
      "loss": 0.0536,
      "step": 1049
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.18935850262641907,
      "learning_rate": 9.272727272727273e-06,
      "loss": 0.0531,
      "step": 1050
    },
    {
      "epoch": 0.9554545454545454,
      "grad_norm": 0.19434136152267456,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.0494,
      "step": 1051
    },
    {
      "epoch": 0.9563636363636364,
      "grad_norm": 0.3302231729030609,
      "learning_rate": 8.90909090909091e-06,
      "loss": 0.0573,
      "step": 1052
    },
    {
      "epoch": 0.9572727272727273,
      "grad_norm": 0.2395145744085312,
      "learning_rate": 8.727272727272728e-06,
      "loss": 0.053,
      "step": 1053
    },
    {
      "epoch": 0.9581818181818181,
      "grad_norm": 0.16711586713790894,
      "learning_rate": 8.545454545454546e-06,
      "loss": 0.0468,
      "step": 1054
    },
    {
      "epoch": 0.9590909090909091,
      "grad_norm": 0.2501666843891144,
      "learning_rate": 8.363636363636365e-06,
      "loss": 0.0487,
      "step": 1055
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.2130855768918991,
      "learning_rate": 8.181818181818183e-06,
      "loss": 0.0251,
      "step": 1056
    },
    {
      "epoch": 0.9609090909090909,
      "grad_norm": 0.22768737375736237,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0502,
      "step": 1057
    },
    {
      "epoch": 0.9618181818181818,
      "grad_norm": 0.21765214204788208,
      "learning_rate": 7.81818181818182e-06,
      "loss": 0.0498,
      "step": 1058
    },
    {
      "epoch": 0.9627272727272728,
      "grad_norm": 0.1890149861574173,
      "learning_rate": 7.636363636363638e-06,
      "loss": 0.0521,
      "step": 1059
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.2611997425556183,
      "learning_rate": 7.454545454545454e-06,
      "loss": 0.0521,
      "step": 1060
    },
    {
      "epoch": 0.9645454545454546,
      "grad_norm": 0.2591358721256256,
      "learning_rate": 7.272727272727272e-06,
      "loss": 0.0539,
      "step": 1061
    },
    {
      "epoch": 0.9654545454545455,
      "grad_norm": 0.32703280448913574,
      "learning_rate": 7.090909090909091e-06,
      "loss": 0.0537,
      "step": 1062
    },
    {
      "epoch": 0.9663636363636363,
      "grad_norm": 0.24456164240837097,
      "learning_rate": 6.909090909090909e-06,
      "loss": 0.0525,
      "step": 1063
    },
    {
      "epoch": 0.9672727272727273,
      "grad_norm": 0.33360883593559265,
      "learning_rate": 6.727272727272728e-06,
      "loss": 0.0562,
      "step": 1064
    },
    {
      "epoch": 0.9681818181818181,
      "grad_norm": 0.266961008310318,
      "learning_rate": 6.545454545454547e-06,
      "loss": 0.0539,
      "step": 1065
    },
    {
      "epoch": 0.9690909090909091,
      "grad_norm": 0.19902688264846802,
      "learning_rate": 6.363636363636363e-06,
      "loss": 0.0487,
      "step": 1066
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.15767036378383636,
      "learning_rate": 6.181818181818183e-06,
      "loss": 0.0492,
      "step": 1067
    },
    {
      "epoch": 0.9709090909090909,
      "grad_norm": 0.19990701973438263,
      "learning_rate": 6e-06,
      "loss": 0.0275,
      "step": 1068
    },
    {
      "epoch": 0.9718181818181818,
      "grad_norm": 0.23082205653190613,
      "learning_rate": 5.8181818181818185e-06,
      "loss": 0.0518,
      "step": 1069
    },
    {
      "epoch": 0.9727272727272728,
      "grad_norm": 0.26190972328186035,
      "learning_rate": 5.636363636363637e-06,
      "loss": 0.052,
      "step": 1070
    },
    {
      "epoch": 0.9736363636363636,
      "grad_norm": 0.2963542938232422,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 0.0271,
      "step": 1071
    },
    {
      "epoch": 0.9745454545454545,
      "grad_norm": 0.20787563920021057,
      "learning_rate": 5.272727272727273e-06,
      "loss": 0.0518,
      "step": 1072
    },
    {
      "epoch": 0.9754545454545455,
      "grad_norm": 0.2326311469078064,
      "learning_rate": 5.090909090909091e-06,
      "loss": 0.05,
      "step": 1073
    },
    {
      "epoch": 0.9763636363636363,
      "grad_norm": 0.2624060809612274,
      "learning_rate": 4.909090909090909e-06,
      "loss": 0.0286,
      "step": 1074
    },
    {
      "epoch": 0.9772727272727273,
      "grad_norm": 0.22264491021633148,
      "learning_rate": 4.727272727272727e-06,
      "loss": 0.0537,
      "step": 1075
    },
    {
      "epoch": 0.9781818181818182,
      "grad_norm": 0.16017554700374603,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 0.0486,
      "step": 1076
    },
    {
      "epoch": 0.9790909090909091,
      "grad_norm": 0.29466482996940613,
      "learning_rate": 4.363636363636364e-06,
      "loss": 0.0548,
      "step": 1077
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.18153028190135956,
      "learning_rate": 4.181818181818182e-06,
      "loss": 0.0481,
      "step": 1078
    },
    {
      "epoch": 0.980909090909091,
      "grad_norm": 0.2663118839263916,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0527,
      "step": 1079
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.19669143855571747,
      "learning_rate": 3.818181818181819e-06,
      "loss": 0.0503,
      "step": 1080
    },
    {
      "epoch": 0.9827272727272728,
      "grad_norm": 0.33127275109291077,
      "learning_rate": 3.636363636363636e-06,
      "loss": 0.0258,
      "step": 1081
    },
    {
      "epoch": 0.9836363636363636,
      "grad_norm": 0.20195609331130981,
      "learning_rate": 3.4545454545454545e-06,
      "loss": 0.0505,
      "step": 1082
    },
    {
      "epoch": 0.9845454545454545,
      "grad_norm": 0.20285101234912872,
      "learning_rate": 3.2727272727272733e-06,
      "loss": 0.0263,
      "step": 1083
    },
    {
      "epoch": 0.9854545454545455,
      "grad_norm": 0.22401204705238342,
      "learning_rate": 3.0909090909090913e-06,
      "loss": 0.0494,
      "step": 1084
    },
    {
      "epoch": 0.9863636363636363,
      "grad_norm": 0.2566785514354706,
      "learning_rate": 2.9090909090909093e-06,
      "loss": 0.0538,
      "step": 1085
    },
    {
      "epoch": 0.9872727272727273,
      "grad_norm": 0.27231526374816895,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 0.0518,
      "step": 1086
    },
    {
      "epoch": 0.9881818181818182,
      "grad_norm": 0.1759960949420929,
      "learning_rate": 2.5454545454545456e-06,
      "loss": 0.0501,
      "step": 1087
    },
    {
      "epoch": 0.9890909090909091,
      "grad_norm": 0.18412935733795166,
      "learning_rate": 2.3636363636363636e-06,
      "loss": 0.0253,
      "step": 1088
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.1484965831041336,
      "learning_rate": 2.181818181818182e-06,
      "loss": 0.0475,
      "step": 1089
    },
    {
      "epoch": 0.990909090909091,
      "grad_norm": 0.26492810249328613,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0274,
      "step": 1090
    },
    {
      "epoch": 0.9918181818181818,
      "grad_norm": 0.24549831449985504,
      "learning_rate": 1.818181818181818e-06,
      "loss": 0.0493,
      "step": 1091
    },
    {
      "epoch": 0.9927272727272727,
      "grad_norm": 0.20895080268383026,
      "learning_rate": 1.6363636363636367e-06,
      "loss": 0.0273,
      "step": 1092
    },
    {
      "epoch": 0.9936363636363637,
      "grad_norm": 0.20555855333805084,
      "learning_rate": 1.4545454545454546e-06,
      "loss": 0.0271,
      "step": 1093
    },
    {
      "epoch": 0.9945454545454545,
      "grad_norm": 0.2884652614593506,
      "learning_rate": 1.2727272727272728e-06,
      "loss": 0.061,
      "step": 1094
    },
    {
      "epoch": 0.9954545454545455,
      "grad_norm": 0.2734461724758148,
      "learning_rate": 1.090909090909091e-06,
      "loss": 0.0507,
      "step": 1095
    },
    {
      "epoch": 0.9963636363636363,
      "grad_norm": 0.41290050745010376,
      "learning_rate": 9.09090909090909e-07,
      "loss": 0.0596,
      "step": 1096
    },
    {
      "epoch": 0.9972727272727273,
      "grad_norm": 0.3670406937599182,
      "learning_rate": 7.272727272727273e-07,
      "loss": 0.059,
      "step": 1097
    },
    {
      "epoch": 0.9981818181818182,
      "grad_norm": 0.20963174104690552,
      "learning_rate": 5.454545454545455e-07,
      "loss": 0.0237,
      "step": 1098
    },
    {
      "epoch": 0.9990909090909091,
      "grad_norm": 0.20135241746902466,
      "learning_rate": 3.6363636363636366e-07,
      "loss": 0.0534,
      "step": 1099
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.20823638141155243,
      "learning_rate": 1.8181818181818183e-07,
      "loss": 0.0527,
      "step": 1100
    }
  ],
  "logging_steps": 1,
  "max_steps": 1100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3028518409789440.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
