base_model: Qwen/Qwen2.5-7B-Instruct

load_in_4bit: true
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

learning_rate: 2e-4
epochs: 4
per_device_train_batch_size: 2
gradient_accumulation_steps: 8

max_seq_length: 4096
