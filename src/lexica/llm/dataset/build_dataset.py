"""
Lexica Orbit Dataset Builder
============================

Purpose:
- Build high-quality training data for NL --> IR
- Enforce IR v1 correctness at dataset creation time
- Merge multiple dataset sources (primitives/features/etc.)
- Generate BOTH positive and negative samples
- Guarantee: every positive sample passes validate_ir()

This file is the ONLY place where training data is merged + validated.
"""

from __future__ import annotations

import json
import random
from pathlib import Path
from typing import Dict, List, Iterable, Tuple

from lexica.pipeline.nl_to_ir.schema import (
    IRModel,
    PrimitiveOp,
    FeatureOp,
    BooleanOp,
    ExportOp,
    PrimitiveKind,
    FeatureKind,
    BooleanKind,
    TopologyIntent,
    TopologyTarget,
)
from lexica.pipeline.nl_to_ir.validate import validate_ir, IRValidationError
from lexica.llm.dataset.intent_semantic import all_semantic_intents


# -------------------------------------------------
# Paths
# -------------------------------------------------

BASE_DIR = Path(__file__).parent

# Inputs (generated by other scripts)
PRIMITIVES_IN = BASE_DIR / "primitives.jsonl"
FEATURES_IN = BASE_DIR / "features.jsonl"

# Outputs
TRAIN_OUT = BASE_DIR / "train.jsonl"
EVAL_OUT = BASE_DIR / "eval.jsonl"

SYSTEM_PROMPT = (
    Path(__file__).parents[1] / "prompts" / "system.txt"
).read_text().strip()


# -------------------------------------------------
# Message helpers
# -------------------------------------------------

def _msg(system: str, user: str, assistant: str | Dict) -> Dict:
    if not isinstance(assistant, str):
        assistant = json.dumps(assistant, separators=(",", ":"))

    return {
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
            {"role": "assistant", "content": assistant},
        ]
    }


# -------------------------------------------------
# IR serialization
# -------------------------------------------------

def _serialize_ir(ir: IRModel) -> str:
    """
    Serialize IRModel into deterministic JSON (canonical assistant output).

    Notes:
    - Uses op.kind.value to match schema enums
    - Exports: your kernel uses ExportOp(format="step"), so assistant must emit
      {"kind":"export","format":"step","params":{}}.
    """

    def serialize_op(op):
        d = {"kind": op.kind.value}

        if isinstance(op, PrimitiveOp):
            d["primitive_kind"] = op.primitive_kind.value

        if isinstance(op, FeatureOp):
            d["feature_kind"] = op.feature_kind.value
            if op.topology:
                d["topology"] = {
                    "target": op.topology.target.value,
                    "rule": op.topology.rule,
                }

        if isinstance(op, BooleanOp):
            d["boolean_kind"] = op.boolean_kind.value
            if op.operands:
                d["operands"] = op.operands

        if isinstance(op, ExportOp):
            d["format"] = op.format

        d["params"] = op.params
        return d

    return json.dumps(
        {"ops": [serialize_op(op) for op in ir.ops]},
        separators=(",", ":"),
    )


# -------------------------------------------------
# IR parsing (dict -> IRModel)
# -------------------------------------------------

def _parse_ir_dict(ir_dict: Dict) -> IRModel:
    """
    Convert an IR dict (from JSON dataset generators) into an IRModel instance
    so we can strictly validate it using validate_ir().
    """
    ops = []

    for raw_op in ir_dict.get("ops", []):
        kind = raw_op.get("kind")

        if kind == "primitive":
            ops.append(
                PrimitiveOp(
                    primitive_kind=PrimitiveKind(raw_op["primitive_kind"]),
                    params=raw_op.get("params", {}),
                )
            )

        elif kind == "feature":
            topology = None
            topo = raw_op.get("topology") or raw_op.get("topo")

            if topo:
                topology = TopologyIntent(
                    target=TopologyTarget(topo["target"]),
                    rule=topo.get("rule", "all"),
                )

            ops.append(
                FeatureOp(
                    feature_kind=FeatureKind(raw_op["feature_kind"]),
                    params=raw_op.get("params", {}),
                    topology=topology,
                )
            )

        elif kind == "boolean":
            ops.append(
                BooleanOp(
                    boolean_kind=BooleanKind(raw_op["boolean_kind"]),
                    params=raw_op.get("params", {}),
                    operands=raw_op.get("operands"),
                )
            )

        elif kind == "export":
            ops.append(
                ExportOp(
                    format=raw_op.get("format", "step"),
                    params=raw_op.get("params", {}),
                )
            )

        else:
            raise ValueError(f"Unknown op kind in dataset IR: {kind}")

    return IRModel(ops=ops)


# -------------------------------------------------
# Dataset loading utilities
# -------------------------------------------------

def _read_jsonl(path: Path) -> List[Dict]:
    """
    Load jsonl dataset written as {"messages": [...]} style.
    """
    if not path.exists():
        return []

    out = []
    with path.open("r") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            out.append(json.loads(line))
    return out


def _extract_user_and_ir(sample: Dict) -> Tuple[str, Dict]:
    """
    Extract (user_text, ir_dict) from generator jsonl samples.

    Expected format:
        {
          "messages": [
            {"role":"system","content":...},
            {"role":"user","content":...},
            {"role":"assistant","content":"{...json...}"}
          ]
        }
    """
    msgs = sample["messages"]
    user_text = None
    assistant_text = None

    for m in msgs:
        if m.get("role") == "user":
            user_text = m.get("content")
        elif m.get("role") == "assistant":
            assistant_text = m.get("content")

    if user_text is None or assistant_text is None:
        raise ValueError("Invalid dataset sample: missing user/assistant message")

    ir_dict = json.loads(assistant_text)
    return user_text, ir_dict


# -------------------------------------------------
# Positive samples
# -------------------------------------------------

def _semantic_intent_samples() -> List[Dict]:
    """
    Convert semantic intent definitions into validated training samples.
    """
    samples = []

    for item in all_semantic_intents():
        user_text = item["input"]
        ir_dict = item["ir"]

        ir_model = _parse_ir_dict(ir_dict)
        validate_ir(ir_model)

        samples.append(
            _msg(
                SYSTEM_PROMPT,
                user_text,
                _serialize_ir(ir_model),
            )
        )

    return samples


def _load_generated_samples() -> List[Dict]:
    """
    Load generated jsonl sources (primitives/features) and strictly validate them.
    Any invalid sample is discarded to preserve the dataset guarantee.
    """
    sources = [
        ("primitives", PRIMITIVES_IN),
        ("features", FEATURES_IN),
    ]

    merged: List[Dict] = []
    dropped: List[Tuple[str, str]] = []

    for name, path in sources:
        raw = _read_jsonl(path)
        if not raw:
            continue

        for s in raw:
            try:
                user_text, ir_dict = _extract_user_and_ir(s)

                ir_model = _parse_ir_dict(ir_dict)
                validate_ir(ir_model)

                merged.append(
                    _msg(
                        SYSTEM_PROMPT,
                        user_text,
                        _serialize_ir(ir_model),
                    )
                )

            except Exception as e:
                dropped.append((name, str(e)))

    if dropped:
        print(f"[dataset] dropped invalid samples: {len(dropped)}")
        # Print first few reasons for visibility
        for i, (src, err) in enumerate(dropped[:10]):
            print(f"  - {src}: {err}")

    return merged


def positive_samples() -> List[Dict]:
    """
    Build positive samples.
    """
    samples: List[Dict] = []

    # Canonical hand-written samples (few, high quality)
    ir = IRModel(
        ops=[
            PrimitiveOp(
                primitive_kind=PrimitiveKind.BOX,
                params={"x": 30, "y": 30, "z": 30},
            ),
            ExportOp(format="step"),
        ]
    )
    validate_ir(ir)
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Create a 30 by 30 by 30 box",
            _serialize_ir(ir),
        )
    )

    ir = IRModel(
        ops=[
            PrimitiveOp(
                primitive_kind=PrimitiveKind.BOX,
                params={"x": 20, "y": 20, "z": 20},
            ),
            FeatureOp(
                feature_kind=FeatureKind.FILLET,
                params={"radius": 2},
                topology=TopologyIntent(
                    target=TopologyTarget.EDGE,
                    rule="all",
                ),
            ),
            ExportOp(format="step"),
        ]
    )
    validate_ir(ir)
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Create a 20mm cube and fillet all edges by 2mm",
            _serialize_ir(ir),
        )
    )

    # Generated datasets (strictly validated)
    samples.extend(_load_generated_samples())

    # Semantic intent dataset (also strictly validated)
    samples.extend(_semantic_intent_samples())

    return samples


# -------------------------------------------------
# Negative samples (compiler rejection)
# -------------------------------------------------

def negative_samples() -> List[Dict]:
    """
    Negative samples are curated. We do not attempt to auto-generate invalid IR
    because it often produces confusing/noisy supervision.
    """
    samples: List[Dict] = []

    # Fillet without a base solid
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Fillet all edges by 2mm",
            {"error": "Feature requires an existing solid"},
        )
    )

    # Unknown operation
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Smooth the box",
            {"error": "Unknown operation: smooth"},
        )
    )

    # Invalid primitive params
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Create a box of width 10 height 10 depth 10",
            {"error": "Primitive BOX uses invalid params. Use x, y, z."},
        )
    )

    # Torus invalid radii example
    samples.append(
        _msg(
            SYSTEM_PROMPT,
            "Make a torus with major radius 10 and minor radius 20",
            {"error": "Invalid torus: must satisfy r < R"},
        )
    )

    return samples


# -------------------------------------------------
# Build dataset
# -------------------------------------------------

def build(seed: int = 1337, train_ratio: float = 0.9) -> None:
    random.seed(seed)

    all_samples: List[Dict] = []
    all_samples.extend(positive_samples())
    all_samples.extend(negative_samples())

    random.shuffle(all_samples)

    split = int(len(all_samples) * train_ratio)
    train = all_samples[:split]
    eval_ = all_samples[split:]

    with TRAIN_OUT.open("w") as f:
        for s in train:
            f.write(json.dumps(s) + "\n")

    with EVAL_OUT.open("w") as f:
        for s in eval_:
            f.write(json.dumps(s) + "\n")

    print(f"[dataset] total samples: {len(all_samples)}")
    print(f"[dataset] train samples: {len(train)}")
    print(f"[dataset] eval samples:  {len(eval_)}")
    print(f"[dataset] output:")
    print(f"  - train: {TRAIN_OUT}")
    print(f"  - eval:  {EVAL_OUT}")


if __name__ == "__main__":
    build()
